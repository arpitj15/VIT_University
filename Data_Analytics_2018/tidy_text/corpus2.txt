Data Analytics using Open-Source Tools
By
Jeffrey S. Strickland
 Humalytica Analytics
Colorado Springs, CO
Copyright 2016 by Jeffrey S. Strickland.  All rights Reserved
All rights reserved. This book or any portion thereof may not be reproduced or used in any manner whatsoever without the express written permission of the author except for the use of brief quotations in a book review.
ISBN 978-1-365-27041-3
Printed in the United States of America
First Printing, 2015
Published by Lulu, Inc.  All Rights Reserved
Acknowledgements
The author would like to thank colleague; Adam Wright for his constant encouragement; Paul Dalen for his challenging questions; and Cameron Warren for his desire to learn. Working with them over the past several years has validated the concepts presented herein.
A special thanks to Dr. Bob Simmonds, who mentored me as a senior operations research analyst.
Preface
This book is a sequel to Predictive Analytics using R. It is about data analytics, which differs significantly from descriptive analytics and partly from prescriptive analytics. One might think of this a survey of predictive models, both statistical and machine learning. We define a predictive model as a statistical model or machine learning model used to predict future behavior based on past behavior.
If I include the work done for the prequel, this was a four-year project that started just before I ventured away from DoD modeling and simulation. Hoping to transition to private industry, I began to look at way in which my modeling experience would be a good fit. I had taught statistical modeling and machine learning (e.g., neural networks) for years, but I had not applied these on the scale of “Big Data”. I have now done so, many times over—often dealing with data sets containing 2000+ variables and 20 million observations (records).
In order to use this book, one should have a basic understanding of mathematical statistics (statistical inference, models, tests, etc.)—this is an advanced book. Some theoretical foundations are laid out (perhaps subtlety) but not proven, but references are provided for additional coverage. Every chapter culminates in an example using one or more open source tools. The tools include R, Python, and KNIME. Though not open-source, I have included a few examples with SAS Studio, University Edition.
R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS. To download R, please choose your preferred CRAN mirror at http://www.r-project.org/. An introduction to R is also available at http://cran.r-project.org/doc/manuals/r-release/R-intro.html. I use R-Studio as the graphical user interface (GUI) for R.
Python comes in several packages. I use Anaconda, which contains the Python programming language and several auxiliary tools and GUIs. My GUI of choice is iPython Notebook. Anaconda can be freely downloaded at https://www.continuum.io/download. 
The KNIME Analytics Platform incorporates hundreds of processing nodes for data I/O, preprocessing and cleansing, modeling, analysis and data mining as well as various interactive views, such as scatter plots, parallel coordinates and others. It integrates all of the analysis modules of the well-known Weka data mining environment and additional plugins allow R-scripts to be run, offering access to a vast library of statistical routines. KNIME can be freely downloaded at https://www.knime.org/downloads/overview. 
When you use the SAS University Edition, you are using SAS Studio to access SAS. Many people program in SAS by using an application on their PC desktop or SAS server. SAS Studio is different because it is a tool that you can use to write and run SAS code through your web browser.
With SAS Studio, you can access your data files, libraries, and existing programs and write new programs. When you use SAS Studio, you are also using SAS software behind the scenes. SAS Studio connects to a SAS server in order to process SAS commands. In the SAS University Edition, the SAS server is included in the vApp file that is installed on your local machine. After the code is processed by the SAS server, the results are returned to SAS Studio in your browser. You can access SAS Studio at http://www.sas.com/en_us/software/university-edition.html. 
The book is organized so that statistical models are presented first (hopefully in a logical order), followed by machine learning models, and then applications: uplift modeling and time series. This a textbook for problem solving with open-source computer tools—there are no “by-hand” exercises.
?
This book is self-published and print-on-demand. I do not use an editor in order to keep the retail cost near production cost. The best discount is provided by the publisher, Lulu.com. If you find errors as you read, please feel free to contact me at jeff@sumulation-educators.com.
Data Analytics
Data analytics—sometimes used synonymously with data analysis—encompasses a variety of statistical techniques from modeling, machine learning, and data mining that analyze current and historical facts to make describe the past and present and make predictions about future, or otherwise unknown events (Nyce, 2007) (Eckerson, 2007).
In business, data analytics exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision making for candidate transactions.
Data analytics is used in actuarial science (Conz, 2008), marketing  (Fletcher, 2011), financial services  (Korn, 2011), insurance, telecommunications (Barkin, 2011), retail (Das & Vidyashankar, 2006), travel (McDonald, 2010), healthcare (Stevenson, 2011), pharmaceuticals (McKay, 2009) and other fields.
One of the most well-known applications is credit scoring (Nyce, 2007), which is used throughout financial services. Scoring models process a customer’s credit history, loan application, customer data, etc., in order to rank-order individuals by their likelihood of making future credit payments on time. A well-known example is the FICO® Score.
Where does data analytics lie in the world of business analytics?
Business Analytics
Business analytics (BA) refers to the skills, technologies, practices for continuous iterative exploration and investigation of past business performance to gain insight and drive business planning. Business analytics makes extensive use of statistical analysis, including descriptive and predictive modeling, and fact-based management to drive decision making. In recent years, prescriptive modeling has also taken a role in BA. It is therefore closely related to management science and operations research. Business analytics can answer questions like why is this happening, what if these trends continue, what will happen next (that is, predict), what is the best that can happen (that is, optimize).
Types
Business analytics is comprised of descriptive, predictive and prescriptive analytics, these are generally understood to be descriptive modeling, predictive modeling, and prescriptive modeling.
Descriptive analytics
Descriptive models quantify relationships in data in a way that is often used to classify customers or prospects into groups. Unlike predictive models that focus on predicting a single customer behavior (such as credit risk), descriptive models identify many different relationships between customers or products. Descriptive analytics provides simple summaries about the sample audience and about the observations that have been made. Such summaries may be either quantitative, i.e. summary statistics, or visual, i.e. simple-to-understand graphs. These summaries may either form the basis of the initial description of the data as part of a more extensive statistical analysis, or they may be sufficient in and of themselves for a particular investigation.
Predictive analytics
Predictive analytics encompasses a variety of statistical techniques from modeling, machine learning, and data mining that analyze current and historical facts to make predictions about future, or otherwise unknown, events. Predictive models are models of the relation between the specific performance of a unit in a sample and one or more known attributes or features of the unit. The objective of the model is to assess the likelihood that a similar unit in a different sample will exhibit the specific performance. This category encompasses models that are in many areas, such as marketing, where they seek out subtle data patterns to answer questions about customer performance, such as fraud detection models.
Prescriptive analytics
Prescriptive analytics not only anticipates what will happen and when it will happen, but also why it will happen. Further, prescriptive analytics suggests decision options on how to take advantage of a future opportunity or mitigate a future risk and shows the implication of each decision option. Prescriptive analytics can continually take in new data to re-predict and re-prescribe, thus automatically improving prediction accuracy and prescribing better decision options. Prescriptive analytics ingests hybrid data, a combination of structured (numbers, categories) and unstructured data (videos, images, sounds, texts), and business rules to predict what lies ahead and to prescribe how to take advantage of this predicted future without compromising other priorities
Applications
Although predictive analytics can be put to use in many applications, I list a few here:
	Analytical customer relationship management (CRM)
	Clinical decision support systems
	Collection analytics
	Cross-sell
	Customer retention
	Direct marketing
	Fraud detection
	Portfolio, product or economy-level prediction
	Risk management
	Underwriting
Definition
If you search the world over, you will not only find many definitions of data analytics, you will find conflicting ones. I provide one here for the purpose of framing this content of the text that follows. You may disagree with the definition, but it is the one I am using. 
“Data analytics (DA) is the science of examining raw data with the purpose of drawing conclusions about that information. Data analytics is used in many industries to allow companies and organization to make better business decisions and in the sciences to verify or disprove existing models or theories. Data analytics is distinguished from data mining by the scope, purpose and focus of the analysis. Data miners sort through huge data sets using sophisticated software to identify undiscovered patterns and establish hidden relationships. Data analytics focuses on inference, the process of deriving a conclusion based solely on what is already known by the researcher.” (Enterprise data analytics strategy: A guide for CIOs)
Although data analytics is not synonymous with data mining, it does deal with extracting information from data and using it to predict trends and behavior patterns. Often the unknown event of interest is in the future, but data analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs (Strickland J. , 2013). The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.
Not Statistics
Data analytics uses statistical methods, but also machine learning algorithms, and heuristics. Though statistical methods are important, the Analytics professional cannot always follow the “rules of statistics to the letter.” Instead, the analyst often implements what I call “modeler judgment”. Unlike the statistician, the analytics professional—akin to the operations research analyst—must understand the system, business, or enterprise where the problem lies, and in the context of the business processes, rules, operating procedures, budget, and so on, make judgments about the analytical solution subject to various constraints. This requires a certain degree of creativity, and lends itself to being both a science and an art, For example, a pure statistical model, say a logistic regression, may determine that the response is explained by 30 independent variables with a significance of 0.05. However, the analytics professional knows that 10 of the variables cannot be used subject to legal constraints imposed for say a bank product. Moreover, the analytics modeler is aware that variables with many degrees of freedom can lead to overfitting the model. Thus, in their final analysis they develop a good model with 12 explanatory variables using modeler judgment. The regression got them near to a solution, and their intuition carried them to the end.
Types
Generally, the term data analytics is used to mean modeling, “scoring” data with models, and forecasting. However, people are increasingly using the term to refer to related analytical disciplines, such as descriptive modeling and decision modeling or optimization. These disciplines also involve rigorous data analysis, and are widely used in business for segmentation and decision making, but have different purposes and the statistical techniques underlying them vary.
Predictive models
Predictive models are models of the relation between the specific performance of a unit in a sample and one or more known attributes or features of the unit. The objective of the model is to assess the likelihood that a similar unit in a different sample will exhibit the specific performance. This category encompasses models that are in many areas, such as marketing, where they seek out subtle data patterns to answer questions about customer performance, such as fraud detection models. Predictive models often perform calculations during live transactions, for example, to evaluate the risk or opportunity of a given customer or transaction, in order to guide a decision. With advancements in computing speed, individual agent modeling systems have become capable of simulating human behavior or reactions to given stimuli or scenarios.
The available sample units with known attributes and known performances is referred to as the “training sample.” The units in other sample, with known attributes but un-known performances, are referred to as “out of [training] sample” units. The out of sample bear no chronological relation to the training sample units. For example, the training sample may consists of literary attributes of writings by Victorian authors, with known attribution, and the out-of sample unit may be newly found writing with unknown authorship; a predictive model may aid the attribution of the unknown author. Another example is given by analysis of blood splatter in simulated crime scenes in which the out-of sample unit is the actual blood splatter pattern from a crime scene. The out of sample unit may be from the same time as the training units, from a previous time, or from a future time.
Descriptive models
Descriptive models quantify relationships in data in a way that is often used to classify customers or prospects into groups. Unlike predictive models that focus on predicting a single customer behavior (such as credit risk), descriptive models identify many different relationships between customers or products. Descriptive models do not rank-order customers by their likelihood of taking a particular action the way predictive models do. Instead, descriptive models can be used, for example, to categorize customers by their product preferences and life stage. Descriptive modeling tools can be utilized to develop further models that can simulate large number of individualized agents and make predictions.
Decision models
Decision models describe the relationship between all the elements of a decision—the known data (including results of predictive models), the decision, and the forecast results of the decision—in order to predict the results of decisions involving many variables. These models can be used in optimization, maximizing certain outcomes while minimizing others. Decision models are generally used to develop decision logic or a set of business rules that will produce the desired action for every customer or circumstance.
Applications
Although predictive analytics can be put to use in many applications, we outline a few examples where predictive analytics has shown positive impact in recent years.
Analytical customer relationship management (CRM)
Analytical Customer Relationship Management is a frequent commercial application of Predictive Analysis. Methods of predictive analysis are applied to customer data to pursue CRM objectives, which involve constructing a holistic view of the customer no matter where their information resides in the company or the department involved. CRM uses predictive analysis in applications for marketing campaigns, sales, and customer services to name a few. These tools are required in order for a company to posture and focus their efforts effectively across the breadth of their customer base. They must analyze and understand the products in demand or have the potential for high demand, predict customers’ buying habits in order to promote relevant products at multiple touch points, and proactively identify and mitigate issues that have the potential to lose customers or reduce their ability to gain new ones. Analytical Customer Relationship Management can be applied throughout the customer lifecycle (acquisition, relationship growth, retention, and win-back). Several of the application areas described below (direct marketing, cross-sell, customer retention) are part of Customer Relationship Managements.
Clinical decision support systems
Experts use predictive analysis in health care primarily to determine which patients are at risk of developing certain conditions, like diabetes, asthma, heart disease, and other lifetime illnesses. Additionally, sophisticated clinical decision support systems incorporate predictive analytics to support medical decision making at the point of care. A working definition has been proposed by Robert Hayward of the Centre for Health Evidence: “Clinical Decision Support Systems link health observations with health knowledge to influence health choices by clinicians for improved health care.” (Hayward, 2004) 
Collection analytics
Every portfolio has a set of delinquent customers who do not make their payments on time. The financial institution has to undertake collection activities on these customers to recover the amounts due. A lot of collection resources are wasted on customers who are difficult or impossible to recover. Predictive analytics can help optimize the allocation of collection resources by identifying the most effective collection agencies, contact strategies, legal actions and other strategies to each customer, thus significantly increasing recovery at the same time reducing collection costs.
Cross-sell
Often corporate organizations collect and maintain abundant data (e.g. customer records, sale transactions) as exploiting hidden relationships in the data can provide a competitive advantage. For an organization that offers multiple products, predictive analytics can help analyze customers’ spending, usage and other behavior, leading to efficient cross sales, or selling additional products to current customers. This directly leads to higher profitability per customer and stronger customer relationships.
Customer retention
With the number of competing services available, businesses need to focus efforts on maintaining continuous consumer satisfaction, rewarding consumer loyalty and minimizing customer attrition. Businesses tend to respond to customer attrition on a reactive basis, acting only after the customer has initiated the process to terminate service. At this stage, the chance of changing the customer’s decision is almost impossible. Proper application of predictive analytics can lead to a more proactive retention strategy. By a frequent examination of a customer’s past service usage, service performance, spending and other behavior patterns, predictive models can determine the likelihood of a customer terminating service sometime soon (Barkin, 2011). An intervention with lucrative offers can increase the chance of retaining the customer. Silent attrition, the behavior of a customer to slowly but steadily reduce usage, is another problem that many companies face. Predictive analytics can also predict this behavior, so that the company can take proper actions to increase customer activity.
Direct marketing
When marketing consumer products and services, there is the challenge of keeping up with competing products and consumer behavior. Apart from identifying prospects, predictive analytics can also help to identify the most effective combination of product versions, marketing material, communication channels and timing that should be used to target a given consumer. The goal of predictive analytics is typically to lower the cost per order or cost per action.
Fraud detection
Fraud is a big problem for many businesses and can be of various types: inaccurate credit applications, fraudulent transactions (both offline and online), identity thefts and false insurance claims. These problems plague firms of all sizes in many industries. Some examples of likely victims are credit card issuers, insurance companies (Schiff, 2012), retail merchants, manufacturers, business-to-business suppliers and even services providers. A predictive model can help weed out the “bads” and reduce a business’s exposure to fraud.
Predictive modeling can also be used to identify high-risk fraud candidates in business or the public sector. Mark Nigrini developed a risk-scoring method to identify audit targets. He describes the use of this approach to detect fraud in the franchisee sales reports of an international fast-food chain. Each location is scored using 10 predictors. The 10 scores are then weighted to give one final overall risk score for each location. The same scoring approach was also used to identify high-risk check kiting accounts, potentially fraudulent travel agents, and questionable vendors. A reasonably complex model was used to identify fraudulent monthly reports submitted by divisional controllers (Nigrini, 2011).
The Internal Revenue Service (IRS) of the United States also uses predictive analytics to mine tax returns and identify tax fraud (Schiff, 2012).
Recent advancements in technology have also introduced predictive behavior analysis for web fraud detection. This type of solution utilizes heuristics in order to study normal web user behavior and detect anomalies indicating fraud attempts.
Portfolio, product or economy-level prediction
Often the focus of analysis is not the consumer but the product, portfolio, firm, industry or even the economy. For example, a retailer might be interested in predicting store-level demand for inventory management purposes. Or the Federal Reserve Board might be interested in predicting the unemployment rate for the next year. These types of problems can be addressed by predictive analytics using time series techniques (see Chapter 18). They can also be addressed via machine learning approaches which transform the original time series into a feature vector space, where the learning algorithm finds patterns that have predictive power.
Risk management
When employing risk management techniques, the results are always to predict and benefit from a future scenario. The Capital asset pricing model (CAM-P) and Probabilistic Risk Assessment (PRA) examples of approaches that can extend from project to market, and from near to long term.  CAP-M (Chong, Jin, & Phillips, 2013) “predicts” the best portfolio to maximize return. PRA, when combined with mini-Delphi Techniques and statistical approaches, yields accurate forecasts (Parry, 1996). @Risk is an Excel add-in used for modeling and simulating risks (Strickland, 2005). Underwriting (see below) and other business approaches identify risk management as a predictive method.
Underwriting
Many businesses have to account for risk exposure due to their different services and determine the cost needed to cover the risk. For example, auto insurance providers need to accurately determine the amount of premium to charge to cover each automobile and driver. A financial company needs to assess a borrower’s potential and ability to pay before granting a loan. For a health insurance provider, predictive analytics can analyze a few years of past medical claims data, as well as lab, pharmacy and other records where available, to predict how expensive an enrollee is likely to be in the future. Predictive analytics can help underwrite these quantities by predicting the chances of illness, default, bankruptcy, etc. Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data. Predictive analytics in the form of credit scores have reduced the amount of time it takes for loan approvals, especially in the mortgage market where lending decisions are now made in a matter of hours rather than days or even weeks. Proper predictive analytics can lead to proper pricing decisions, which can help mitigate future risk of default.
Technology and big data influences
Big data is a collection of data sets that are so large and complex that they become awkward to work with using traditional database management tools. The volume, variety and velocity of big data have introduced challenges across the board for capture, storage, search, sharing, analysis, and visualization. Examples of big data sources include web logs, RFID and sensor data, social networks, Internet search indexing, call detail records, military surveillance, and complex data in astronomic, biogeochemical, genomics, and atmospheric sciences. Thanks to technological advances in computer hardware—faster CPUs, cheaper memory, and MPP architectures—and new technologies such as Hadoop, MapReduce, and in-database and text analytics for processing big data, it is now feasible to collect, analyze, and mine massive amounts of structured and unstructured data for new insights (Conz, 2008). Today, exploring big data and using predictive analytics is within reach of more organizations than ever before and new methods that are capable for handling such datasets are proposed (Ben-Gal I. Dana A., 2014). 
Analytical Techniques
The approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.
Regression techniques
Regression models are the mainstay of predictive analytics. The focus lies on establishing a mathematical equation as a model to represent the interactions between the different variables in consideration. Depending on the situation, there is a wide variety of models that can be applied while performing predictive analytics. Some of them are briefly discussed below.
Linear regression model
The linear regression model analyzes the relationship between the response or dependent variable and a set of independent or predictor variables. This relationship is expressed as an equation that predicts the response variable as a linear function of the parameters. These parameters are adjusted so that a measure of fit is optimized. Much of the effort in model fitting is focused on minimizing the size of the residual, as well as ensuring that it is randomly distributed with respect to the model predictions (Draper & Smith, 1998).
The goal of regression is to select the parameters of the model so as to minimize the sum of the squared residuals. This is referred to as ordinary least squares (OLS) estimation and results in best linear unbiased estimates (BLUE) of the parameters if and only if the Gauss-Markov assumptions are satisfied (Hayashi, 2000).
Once the model has been estimated we would be interested to know if the predictor variables belong in the model – i.e. is the estimate of each variable’s contribution reliable? To do this we can check the statistical significance of the model’s coefficients which can be measured using the t-statistic. This amounts to testing whether the coefficient is significantly different from zero. How well the model predicts the dependent variable based on the value of the independent variables can be assessed by using the R² statistic. It measures predictive power of the model, i.e., the proportion of the total variation in the dependent variable that is “explained” (accounted for) by variation in the independent variables.
Discrete choice models
Multivariate regression (above) is generally used when the response variable is continuous and has an unbounded range (Greene, 2011). Often the response variable may not be continuous but rather discrete. While mathematically it is feasible to apply multivariate regression to discrete ordered dependent variables, some of the assumptions behind the theory of multivariate linear regression no longer hold, and there are other techniques such as discrete choice models which are better suited for this type of analysis. If the dependent variable is discrete, some of those superior methods are logistic regression, multinomial logit and probit models. Logistic regression and probit models are used when the dependent variable is binary.
Logistic regression
For more details on this topic, see Chapter 12, Logistic Regression.
In a classification setting, assigning outcome probabilities to observations can be achieved through the use of a logistic model, which is basically a method which transforms information about the binary dependent variable into an unbounded continuous variable and estimates a regular multivariate model (Hosmer & Lemeshow, 2000).
The Wald and likelihood-ratio test are used to test the statistical significance of each coefficient b in the model (analogous to the t-tests used in OLS regression; see Chapter 8). A test assessing the goodness-of-fit of a classification model is the “percentage correctly predicted”.
Multinomial logistic regression
An extension of the binary logit model to cases where the dependent variable has more than 2 categories is the multinomial logit model. In such cases collapsing the data into two categories might not make good sense or may lead to loss in the richness of the data. The multinomial logit model is the appropriate technique in these cases, especially when the dependent variable categories are not ordered (for examples colors like red, blue, green). Some authors have extended multinomial regression to include feature selection/importance methods such as Random multinomial logit.
Probit regression
Probit models offer an alternative to logistic regression for modeling categorical dependent variables. Even though the outcomes tend to be similar, the underlying distributions are different. Probit models are popular in social sciences like economics (Bliss, 1934).
A good way to understand the key difference between probit and logit models is to assume that there is a latent variable z. We do not observe z but instead observe y which takes the value 0 or 1. In the logit model we assume that y follows a logistic distribution. In the probit model we assume that y follows a standard normal distribution. Note that in social sciences (e.g. economics), probit is often used to model situations where the observed variable y is continuous but takes values between 0 and 1.
Logit versus probit
The Probit model has been around longer than the logit model (Bishop, 2006). They behave similarly, except that the logistic distribution tends to be slightly flatter tailed. One of the reasons the logit model was formulated was that the probit model was computationally difficult due to the requirement of numerically calculating integrals. Modern computing however has made this computation fairly simple. The coefficients obtained from the logit and probit model are fairly close. However, the odds ratio is easier to interpret in the logit model (Hosmer & Lemeshow, 2000).
Practical reasons for choosing the probit model over the logistic model would be:
There is a strong belief that the underlying distribution is normal
The actual event is not a binary outcome (e.g., bankruptcy status) but a proportion (e.g., proportion of population at different debt levels).
Time series models
Time series models are used for predicting or forecasting the future behavior of variables. These models account for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for. As a result standard regression techniques cannot be applied to time series data and methodology has been developed to decompose the trend, seasonal and cyclical component of the series. Modeling the dynamic path of a variable can improve forecasts since the predictable component of the series can be projected into the future (Imdadullah, 2014).
Time series models estimate difference equations containing stochastic components. Two commonly used forms of these models are autoregressive models (AR) and moving average (MA) models. The Box-Jenkins methodology (1976) developed by George Box and G.M. Jenkins combines the AR and MA models to produce the ARMA (autoregressive moving average) model which is the cornerstone of stationary time series analysis  (Box & Jenkins, 1976). ARIMA (autoregressive integrated moving average models) on the other hand are used to describe non-stationary time series. Box and Jenkins suggest differencing a non-stationary time series to obtain a stationary series to which an ARMA model can be applied. Non stationary time series have a pronounced trend and do not have a constant long-run mean or variance.
Box and Jenkins proposed a three stage methodology which includes: model identification, estimation and validation. The identification stage involves identifying if the series is stationary or not and the presence of seasonality by examining plots of the series, autocorrelation and partial autocorrelation functions. In the estimation stage, models are estimated using non-linear time series or maximum likelihood estimation procedures. Finally the validation stage involves diagnostic checking such as plotting the residuals to detect outliers and evidence of model fit (Box & Jenkins, 1976).
In recent years, time series models have become more sophisticated and attempt to model conditional heteroskedasticity with models such as ARCH (autoregressive conditional heteroskedasticity) and GARCH (generalized autoregressive conditional heteroskedasticity) models frequently used for financial time series. In addition time series models are also used to understand inter-relationships among economic variables represented by systems of equations using VAR (vector autoregression) and structural VAR models.
Survival or duration analysis
Survival analysis is another name for time-to-event analysis. These techniques were primarily developed in the medical and biological sciences, but they are also widely used in the social sciences like economics, as well as in engineering (reliability and failure time analysis) (Singh & Mukhopadhyay, 2011).
Censoring and non-normality, which are characteristic of survival data, generate difficulty when trying to analyze the data using conventional statistical models such as multiple linear regression. The normal distribution, being a symmetric distribution, takes positive as well as negative values, but duration by its very nature cannot be negative and therefore normality cannot be assumed when dealing with duration/survival data. Hence the normality assumption of regression models is violated.
The assumption is that if the data were not censored it would be representative of the population of interest. In survival analysis, censored observations arise whenever the dependent variable of interest represents the time to a terminal event, and the duration of the study is limited in time.
An important concept in survival analysis is the hazard rate, defined as the probability that the event will occur at time t conditional on surviving until time t. Another concept related to the hazard rate is the survival function which can be defined as the probability of surviving to time t.
Most models try to model the hazard rate by choosing the underlying distribution depending on the shape of the hazard function. A distribution whose hazard function slopes upward is said to have positive duration dependence, a decreasing hazard shows negative duration dependence whereas constant hazard is a process with no memory usually characterized by the exponential distribution. Some of the distributional choices in survival models are: F, gamma, Weibull, log normal, inverse normal, exponential, etc. All these distributions are for a non-negative random variable.
Duration models can be parametric, non-parametric or semi-parametric. Some of the models commonly used are Kaplan-Meier and Cox proportional hazard model (nonparametric) (Kaplan & Meier, 1958).
Classification and regression trees
Hierarchical Optimal Discriminant Analysis (HODA), (also called classification tree analysis) is a generalization of Optimal Discriminant Analysis that may be used to identify the statistical model that has maximum accuracy for predicting the value of a categorical dependent variable for a dataset consisting of categorical and continuous variables (Yarnold & Soltysik, 2004). The output of HODA is a non-orthogonal tree that combines categorical variables and cut points for continuous variables that yields maximum predictive accuracy, an assessment of the exact Type I error rate, and an evaluation of potential cross-generalizability of the statistical model. Hierarchical Optimal Discriminant analysis may be thought of as a generalization of Fisher’s linear discriminant analysis. Optimal discriminant analysis is an alternative to ANOVA (analysis of variance) and regression analysis, which attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA and regression analysis give a dependent variable that is a numerical variable, while hierarchical optimal discriminant analysis gives a dependent variable that is a class variable.
Classification and regression trees (CART) is a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively (Rokach & Maimon, 2008).
Decision trees are formed by a collection of rules based on variables in the modeling data set:
Rules based on variables’ values are selected to get the best split to differentiate observations based on the dependent variable
Once a rule is selected and splits a node into two, the same process is applied to each “child” node (i.e. it is a recursive procedure)
Splitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later pruned.)
Each branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules.
A very popular method for predictive analytics is Leo Breiman’s Random forests (Breiman L. , Random Forests, 2001) or derived versions of this technique like Random multinomial logit (Prinzie, 2008).
Multivariate adaptive regression splines
Multivariate adaptive regression splines (MARS) is a non-parametric technique that builds flexible models by fitting piecewise linear regressions (Friedman, 1991).
An important concept associated with regression splines is that of a knot. Knot is where one local regression model gives way to another and thus is the point of intersection between two splines.
In multivariate and adaptive regression splines, basis functions are the tool used for generalizing the search for knots. Basis functions are a set of functions used to represent the information contained in one or more variables. MARS model almost always creates the basis functions in pairs.
Multivariate and adaptive regression spline approach deliberately over-fits the model and then prunes to get to the optimal model. The algorithm is computationally very intensive and in practice we are required to specify an upper limit on the number of basis functions.
Machine learning techniques
Machine learning, a branch of artificial intelligence, was originally employed to develop techniques to enable computers to learn. Today, since it includes a number of advanced statistical methods for regression and classification, it finds application in a wide variety of fields including medical diagnostics, credit card fraud detection, face and speech recognition and analysis of the stock market. In certain applications it is sufficient to directly predict the dependent variable without focusing on the underlying relationships between variables. In other cases, the underlying relationships can be very complex and the mathematical form of the dependencies unknown. For such cases, machine learning techniques emulate human cognition and learn from training examples to predict future events.
A brief discussion of some of these methods used commonly for predictive analytics is provided below. A detailed study of machine learning can be found in Mitchell’s Machine Learning  (Mitchell, 1997).
Neural networks
Neural networks are nonlinear sophisticated modeling techniques that are able to model complex functions (Rosenblatt, 1958). They can be applied to problems of prediction, classification or control in a wide spectrum of fields such as finance, cognitive psychology/neuroscience, medicine, engineering, and physics.
Neural networks are used when the exact nature of the relationship between inputs and output is not known. A key feature of neural networks is that they learn the relationship between inputs and output through training. There are three types of training in neural networks used by different networks, supervised and unsupervised training, reinforcement learning, with supervised being the most common one.
Some examples of neural network training techniques are back propagation, quick propagation, conjugate gradient descent, projection operator, Delta-Bar-Delta etc. Some unsupervised network architectures are multilayer perceptrons (Freund & Schapire, 1999), Kohonen networks (Kohonen & Honkela, 2007), Hopfield networks (Hopfield, 2007), etc.
Multilayer Perceptron (MLP)
The Multilayer Perceptron (MLP) consists of an input and an output layer with one or more hidden layers of nonlinearly-activating nodes or sigmoid nodes. This is determined by the weight vector and it is necessary to adjust the weights of the network. The back-propagation employs gradient fall to minimize the squared error between the network output values and desired values for those outputs. The weights adjusted by an iterative process of repetitive present of attributes. Small changes in the weight to get the desired values are done by the process called training the net and is done by the training set (learning rule) (Riedmiller, 2010).
Radial basis functions
A radial basis function (RBF) is a function which has built into it a distance criterion with respect to a center. Such functions can be used very efficiently for interpolation and for smoothing of data. Radial basis functions have been applied in the area of neural networks where they are used as a replacement for the sigmoidal transfer function. Such networks have 3 layers, the input layer, the hidden layer with the RBF non-linearity and a linear output layer. The most popular choice for the non-linearity is the Gaussian. RBF networks have the advantage of not being locked into local minima as do the feedforward networks such as the multilayer perceptron (?ukaszyk, 2004).
Support vector machines
Support Vector Machines (SVM) are used to detect and exploit complex patterns in data by clustering, classifying and ranking the data (Cortes & Vapnik, 1995). They are learning machines that are used to perform binary classifications and regression estimations. They commonly use kernel based methods to apply linear classification techniques to non-linear classification problems. There are a number of types of SVM such as linear, polynomial, sigmoid etc.
Naïve Bayes
Naïve Bayes based on Bayes conditional probability rule is used for performing classification tasks. Naïve Bayes assumes the predictors are statistically independent which makes it an effective classification tool that is easy to interpret. It is best employed when faced with the problem of ‘curse of dimensionality‘ i.e. when the number of predictors is very high (Rennie, Shih, Teevan, & Karger, 2003).
k-nearest neighbors
The nearest neighbor algorithm k-NN belongs to the class of pattern recognition statistical methods (Altman, 1992). The method does not impose a priori any assumptions about the distribution from which the modeling sample is drawn. It involves a training set with both positive and negative values. A new sample is classified by calculating the distance to the nearest neighboring training case. The sign of that point will determine the classification of the sample. In the k-nearest neighbor classifier, the k nearest points are considered and the sign of the majority is used to classify the sample. The performance of the k-NN algorithm is influenced by three main factors: (1) the distance measure used to locate the nearest neighbors; (2) the decision rule used to derive a classification from the k-nearest neighbors; and (3) the number of neighbors used to classify the new sample. It can be proved that, unlike other methods, this method is universally asymptotically convergent, i.e.: as the size of the training set increases, if the observations are independent and identically distributed (i.i.d.), regardless of the distribution from which the sample is drawn, the predicted class will converge to the class assignment that minimizes misclassification error (Devroye, Györfi, & Lugosi, 1996).
Geospatial predictive modeling
Conceptually, geospatial predictive modeling is rooted in the principle that the occurrences of events being modeled are limited in distribution. Occurrences of events are neither uniform nor random in distribution – there are spatial environment factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences. Geospatial predictive modeling is a process for analyzing events through a geographic filter in order to make statements of likelihood for event occurrence or emergence.
Tools
Historically, using predictive analytics tools—as well as understanding the results they delivered—required advanced skills. However, modern predictive analytics tools are no longer restricted to IT specialists. As more organizations adopt predictive analytics into decision-making processes and integrate it into their operations, they are creating a shift in the market toward business users as the primary consumers of the information. Business users want tools they can use on their own. Vendors are responding by creating new software that removes the mathematical complexity, provides user-friendly graphic interfaces and/or builds in short cuts that can, for example, recognize the kind of data available and suggest an appropriate predictive model (Halper, 2011). Predictive analytics tools have become sophisticated enough to adequately present and dissect data problems, so that any data-savvy information worker can utilize them to analyze data and retrieve meaningful, useful results (Eckerson, 2007). For example, modern tools present findings using simple charts, graphs, and scores that indicate the likelihood of possible outcomes (MacLennan, 2012).
There are numerous tools available in the marketplace that help with the execution of predictive analytics. These range from those that need very little user sophistication to those that are designed for the expert practitioner. The difference between these tools is often in the level of customization and heavy data lifting allowed.
Notable open source predictive analytic tools include:
scikit-learn
KNIME
OpenNN
Orange
R
RapidMiner
Weka
GNU Octave
Apache Mahout
Python (Anaconda)
Notable commercial predictive analytic tools include:
Alpine Data Labs
BIRT Analytics
Angoss KnowledgeSTUDIO
IBM SPSS Statistics and IBM SPSS Modeler
KXEN Modeler
Mathematica
MATLAB
Minitab
Oracle Data Mining (ODM)
Pervasive
Predixion Software
Revolution Analytics
SAP
SAS and SAS Enterprise Miner
STATA
STATISTICA
TIBCO
PMML
In an attempt to provide a standard language for expressing predictive models, the Predictive Model Markup Language (PMML) has been proposed. Such an XML-based language provides a way for the different tools to define predictive models and to share these between PMML compliant applications. PMML 4.0 was released in June, 2009.
Criticism
There are plenty of skeptics when it comes to computers and algorithms abilities to predict the future, including Gary King, a professor from Harvard University and the director of the Institute for Quantitative Social Science. People are influenced by their environment in innumerable ways. Trying to understand what people will do next assumes that all the influential variables can be known and measured accurately. “People’s environments change even more quickly than they themselves do. Everything from the weather to their relationship with their mother can change the way people think and act. All of those variables are unpredictable. How they will impact a person is even less predictable. If put in the exact same situation tomorrow, they may make a completely different decision. This means that a statistical prediction is only valid in sterile laboratory conditions, which suddenly isn’t as useful as it seemed before.” (King, 2014)
?
?
Acquiring Data with Big Data Tools
Introduction
Data storage and access used to be problematic and perhaps some would argue that it is still. However, with terabytes of storage space and parallel computing, data storage and access to much better than it has been in the past. Moreover, the quantity of data that we have access to is also greater. Big data describes the data environment of today and the Internet of Things expands that environment for tomorrow.
Hadoop
Hadoop is an open-source software framework for storing data and running applications on clusters of commodity hardware. It provides massive storage for any kind of data, enormous processing power and the ability to handle virtually limitless concurrent tasks or jobs.
One of the top reasons that organizations turn to Hadoop is its ability to store and process huge amounts of data – any kind of data – quickly. With data volumes and varieties constantly increasing, especially from social media and the Internet of Things, that’s a key consideration. Other benefits include:
Computing power: Its distributed computing model quickly processes big data. The more computing nodes you use, the more processing power you have.
Flexibility: Unlike traditional relational databases, you don’t have to preprocess data before storing it. You can store as much data as you want and decide how to use it later. That includes unstructured data like text, images and videos.
Fault tolerance: Data and application processing are protected against hardware failure. If a node goes down, jobs are automatically redirected to other nodes to make sure the distributed computing does not fail. And it automatically stores multiple copies of all data.
Low cost: The open-source framework is free and uses commodity hardware to store large quantities of data.
Scalability: You can easily grow your system simply by adding more nodes. Little administration is required.
Hadoop history
Hadoop really started with the World Wide Web. As the web grew in the late 1900s and early 2000s, we created search engines and indexes to help locate relevant information amid the text-based content. In the early years, search results really were returned by humans. But as the web grew from dozens to millions of pages, we required automation. We created web crawlers, many as university-led research projects, and search engine start-ups took off (Yahoo, AltaVista, etc.).
One such project was an open-source web search engine called Nutch – created by Doug Cutting and Mike Cafarella. They wanted to invent a way to return web search results faster by distributing data and calculations across different computers so multiple tasks could be accomplished simultaneously. During this time, another search engine project called Google was in progress. It was based on the same concept – storing and processing data in a distributed, automated way so that relevant web search results could be returned faster.
In 2006, Cutting joined Yahoo and took with him the Nutch project as well as ideas based on Google’s early work with automating distributed data storage and processing. The Nutch project was divided. The web crawler portion remained as Nutch. The distributed computing and processing portion became Hadoop (named after Cutting’s son’s toy elephant). In 2008, Yahoo released Hadoop as an open-source project. Today, Hadoop’s framework and ecosystem of technologies are managed and maintained by the non-profit Apache Software Foundation (ASF), a global community of software developers and contributors.
Components of Hadoop
Currently, five core modules are included in the basic framework from the Apache Foundation:
Hadoop Common – the libraries and utilities used by other Hadoop modules.
Hadoop Distributed File System (HDFS) – the Java-based scalable system that stores data across multiple machines without prior organization.
MapReduce – a software programming model for processing large sets of data in parallel.
YARN – resource management framework for scheduling and handling resource requests from distributed applications. (YARN is an acronym for Yet Another Resource Negotiator.)
TEZ - Tez – Hindi for “speed” provides a general-purpose, highly customizable framework that creates simplifies data-processing tasks across both small scale (low-latency) and large-scale (high throughput) workloads in Hadoop. It generalizes the MapReduce paradigm to a more powerful framework by providing the ability to execute a complex DAG (directed acyclic graph) of tasks for a single job so that projects in the Apache Hadoop ecosystem such as Apache Hive, Apache Pig and Cascading can meet requirements for human-interactive response times and extreme throughput at petabyte scale (clearly MapReduce has been a key driver in achieving this).
Other software components that can run on top of or alongside Hadoop and have achieved top-level Apache project status include:
Pig – a platform for manipulating data stored in HDFS that includes a compiler for MapReduce programs and a high-level language called Pig Latin. It provides a way to perform data extractions, transformations and loading, and basic analysis without having to write MapReduce programs.
Hive – a data warehousing and Structured Query Language (SQL)-like query language that presents data in the form of tables. Hive programming is similar to database programming. (It was initially developed by Facebook.)
HBase – a nonrelational, distributed database that runs on top of Hadoop. HBase tables can serve as input and output for MapReduce jobs.
HCatalog – a table and storage management layer that helps users share and access data.
Ambari – a web interface for managing, configuring and testing Hadoop services and components.
Cassandra – A distributed database system (DDS).
Chukwa – a data collection system for monitoring large distributed systems.
Flume – software that collects, aggregates and moves large amounts of streaming data into HDFS.
Oozie – a Hadoop job scheduler.
Sqoop – a connection and transfer mechanism that moves data between Hadoop and relational databases.
Spark – an open-source cluster computing framework with in-memory analytics.
Solr – an scalable search tool that includes indexing, reliability, central configuration, failover and recovery.
Zookeeper – an application that coordinates distributed processes.
 
Exercises
	Choose a web service for deploying Hortonworks Sandbox. Services include Amazon Web Services (AWS), Google Cloud Platform, Microsoft Azure, etc. Setup an instance of Hadoop with resources that include HDFS, Hive, Pig and Tez, at a minimum. Submit a description of your virtual machine, including computer name, user name, size, disk type, storage account, virtual network, and resource set. Make sure you select a service with a free account.
?
?
Working with Hadoop
In this chapter we will explore the capabilities of Hadoop. I wrote this chapter as a guide (tutorial-like) based on my own learning experience with Hortonworks Sandbox. However, the skills are gerneralizable and entering code, like SQL, can be performed using the termnial instead of the Ambari graphical user interface (GUI)—I did it using both the GUI and terminal commands. If you do not already have an instance of Hortonworks Sandbox deployed, see Appendix A for a step-by-step tutorial on deployment.
First, we will download the data and load it into the Hadoop Distributed File System (HDFS) using Ambari User Views. I will introduce you to the Ambari Files User View to manage files. Additionally, we can carry out tasks using Ambari User Views, such as create directories, navigate file systems, upload files to HDFS and a few other file-related tasks. Once we get the basics, we will create two directories and then load two files into HDFS using the Ambari HDFS User View.
Second, we will use terminal commands to populate the staging tables geolocation_stage and trucks_stage and also demonstrate how to use Apache Hive (with the Ambari GUI) to populate the tables, generate sample data, and create new tables from the existing ones. We will also create Optimized Row Columnar (ORC) table versions of the tables we previously formed using terminal commands. If you want to become Hadoop certified, using a terminal shell is essential for that is how the exam is written.
Third, we will use Apache Pig (using terminal commands and the Ambari GUI) to generate a table that merges data from multiple tables created in Apache Hive. We will create the table Driver_Mileage from the Truck_Mileage table. Then we will use the driver mileage data to better understand risk associated with every driver.
Next, we will perform the same steps using Apache Spark. This step is optional or can be performed in lieu of using Apache Pig.
For our next step, we will export the data from Hadoop and analyze it using R and/or SAS Studio. We will also use the data to build models in later chapters.
For our final step, we will build a report using Apache Zeppelin. Once we complete this step, we will progress through the remaining chapters learning to construct various types of models.
HDFS Introduction
A single physical machine’s storage capacity gets inundated as the data grows. This growth pushes the requirement to partition our data across several separate machines. Distributed File Systems (DFS) is the type of file system that manages storage of data across a network of machines. HDFS is designed to store huge data files and is a core component of Apache Hadoop. Moreover, DFS is designed to store large files with streaming data access configurations, running on clusters of commodity hardware. Using the Hortonworks Data Platform (HDP) 2.2, we can efficiently handle larger data jobs. Additionally, HDFS is now expanded to support heterogeneous storage media within the HDFS cluster.
Ambari Introduction
The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web Graphical User Interface (GUI) backed by its RESTful Application program interfaces (API). REST stands for Representational State Transfer, which is an architectural style for networked hypermedia applications, and is primarily used to build Web services that are lightweight, maintainable, and scalable. Web service APIs that adhere to the REST architectural constraints are called RESTful.
Before we begin executing jobs in Hive and Pig, we need to make some basic configuration changes. The Ambari dashboard can be accessed from our browser by entering http://<hostname>:8080, where <hostname> is the ip address given in the Sandbox settings. The dashboard is depicted in Figure 3-1.
 
Figure 3-1. Ambari dashboard view with Metrics tab selected
The changes apply to Hive, so we select Hive under the resources on the left of the screen and when the Hive service window opens we select the Configs tab as shown in Figure 3-2.
 
Figure 3-2. Ambari dashboard view with Configs tab selected
In the Configs widow under the Settings tab (see Figure 3-3), we scroll down to Tez Container Size under Optimization and change the default value to 1024 (the recommended value). We save the setting and proceed without accepting other suggested setting.
Next, we select the Advanced tab and scroll down to hive.tez.java.opts in the General settings and change the value to “-server -Xmx512m -Djava.net.preferIPv4Stack=true” as seen in Figure 3-4. We then save the setting and proceed without accepting other suggested settings.
 
Figure 3-3. Ambari dashboard view showing Optimization options
 
Figure 3-4. Ambari dashboard showing Advanced General setting with hive.tez.java.opts highlighted
Step 1: Using the Ambari File View to Load Data
Step 1.1: Download / Extract the Sensor Data Files
First, we can download the sample sensor data contained in a compressed (.zip) at: https://app.box.com/HadoopCrashCourseData/. We will save the Geolocation.zip file to your computer, then extract the files. We should see a Geolocation folder that contains the following files (also see Figure 3-5):
	geolocation.csv – This is the collected geolocation data from the trucks. It contains records showing truck location, date, time, type of event, speed, etc.
	trucks.csv – This is data was exported from a relational database and it shows info on truck models, driverid, truckid, and aggregated mileage info.
 
Figure 3-5. Windows file explorer showing our .CSV files
Step 1.2: Load the Sensor Data into HDFS
Next, we go to the Ambari Dashboard and open the HDFS Files view. Click on the nine-square Ambari User Views icon   next to the username button and select the HDFS Files menu item as shown in Figure 3-6.
 
Figure 3-6. Ambari menu with HDFS Files selected
If we start from the top root of the HDFS file system, we will see all the files the logged in user (maria_dev in this case) has access to see the files shown in Figure 3-7.
 
Figure 3-7. Root Ambari file structure 
Now, we’ll click on the tmp folder shown in Figure 3-8 and then click    button to create the maria_dev directory inside the tmp folder. Now let’s navigate into the maria_dev folder.
 
Figure 3-8. Ambari File structure for Users on this project
If we are not already in our newly created directory path /tmp/maria_dev/, go to the maria_dev folder. Then we’ll browse and upload the corresponding geolocation.csv and trucks.csv files. The upload button appears at the top-right of the modeling window as shown in Figure 3-9.
 
Figure 3-9. File upload buttons in Hive User View
The files we generate will appear in the directory structure as depicted in Figure 3-8. We can also perform the following operations on a file by right clicking on the file: Download, Move, Permissions, Rename and Delete.
IMPORTANT
Next, we right click on the folder maria_dev, which is contained within the directory path /tmp/ Click Permissions. Make sure that the background of all the write boxes are checked (blue) (see Figure 3-10).
 
Figure 3-10. File view with our .CSV files uploaded
Step 2: Using Hive to Manipulate Data
ApacheTM Hive is an SQL-like query language that enables analysts familiar with the Structured Query Language (SQL) to run queries on large volumes of data.  Hive has three main functions: data summarization, query and analysis. Hive provides tools that enable easy data Extraction, Transformation and Loading (ETL), as well.
Step 2.1a: Use the Terminal to Access Hive
Apache Hive presents a relational view of data in HDFS and ensures that users need not worry about where or in what format their data is stored. Hive can display data from RCFile format, text files, ORC, JSON, parquet, sequence files and many of other formats in a tabular view. Through the use of SQL, we can view our data as a table and create queries like you would in an RDBMS.
The Hive Shell can be accessed through the terminal by entering the command hive as shown in Figure 3-11. Subsequent commands are entered in the terminal at the Hive prompt. My method for doing this is to create my queries in a text editor, like NotePad++ (see Figure 3-12), and then copying and pasting them into the terminal after I am comfortable that my syntax is correct. Figure 3-13 shows an instance of Hive running in the terminal shell window.

CREATE TABLE geolocation_stage (truckid string, driverid string, event string, latitude DOUBLE, longitude DOUBLE, city string, state string, velocity BIGINT, event_ind BIGINT, idling_ind BIGINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
TBLPROPERTIES ("skip.header.line.count"="1");

 
Figure 3-11. Create table statement for geolocation_stage in Hive User View
We use the following steps from our terminal shell (or putty if using Windows) to enter the Hive shell (password is hadoop). For Azure users, we enter 
ssh<_usernae_> @<_ipaddress_>-p<_port_>.
Username is the name you gave your sandbox, and ip address is located on the Azure dashboard. The command hive starts Hive shell and now we can enter commands and SQL. Here are some additional commands:
su hive starts the Hive shell in the administrator mode 
quit; Exits out of the Hive shell.
 
Figure 3-12. Example Hive query status in the terminal shell
Step 2.1b: Use the Ambari Hive User View
To make it easy to interact with Hive, we use a tool in the Hortonworks Sandbox called Ambari and its user views. The Ambari Hive User View provides an interactive interface to Hive. We can create, edit, save and run queries, and have Hive evaluate them for us using a series of MapReduce jobs or Tez jobs.
Let’s now open the Ambari Hive User View and get introduced to the environment. Go to the nine-square Ambari User View icon   and select Hive as shown in Figure 3-13.
 
Figure 3-13. Ambari menu showing Hive selected
The Ambari Hive User View looks like the annotated view of Figure 3-14 (without the annotations, of course).
 
Figure 3-14. Annotated Hive User View query window
Now let’s take a closer look at the SQL editing capabilities in the User View, where the numbers 1 through 6 refer to Figure 3-13:
	There are four tabs to interact with SQL:
	Query: This choice is the interface shown in Figure 3-14 and the primary interface to write, edit and execute new SQL statements
	Saved Queries: We can save your favorite queries and quickly have access to them to rerun or edit.
	History: This option allows us to look at past queries or currently running queries to view, edit and rerun.  It also allows us to see all SQL queries you have authority to view.  For example, if we are an operator and an analyst needs help with a query, then the Hadoop operator can use the History feature to see the query that was sent from the reporting tool.
	UDFs:  UDFs allow us to construct User Defined Function (UDF) interfaces and associated classes so we can access them from the SQL editor.
	Database Explorer: The Database Explorer helps us navigate our database objects.  We can either search for a database object in the Search tables dialog box, or we can navigate through Database -> Table -> Columns in the navigation pane.
	Editing Pane: This is the principal pane for writing and editing SQL statements. This editor includes content assist via CTRL + Space to help you build queries. Content assist helps you with SQL syntax and table objects.
	Once we have created your SQL statement we have 3 options:
	Execute: This option runs the SQL statement.
	Explain: This option provides us a visual plan, from the Hive optimizer, of how the SQL statement will be executed.
	Save as: This option allows us to persist our queries into our list of saved queries. Kill Session terminates the SQL statement.
	When the query is executed we can see the logs or the actual query results.
	Logs: When the query is executed we can see the logs associated with the query execution.  If our query fails this is a good place to get additional information for troubleshooting.
	Results: We can view results in sets of 50 by default.
	There are six sliding views on the right hand side with the following capabilities, which are in context of the tab you are in:
	Query: This choice is the default operation, which allows us to write and edit SQL.
	Settings: This option allows us to set properties globally or associated with an individual query.
	Data Visualization: This option allows us to visualize our numeric data through different charts.
	Visual Explain: This will generate an explain for the query and also show the progress of it.
	TEZ: If we use TEZ as the query execution engine then we can view the DAG associated with the query. This option integrates the TEZ User View so we can check for correctness and helps with performance tuning by visualizing the TEZ jobs associated with a SQL query.
	Notifications: This is the way to get feedback on query execution.
Step2.2: Define a Hive Table
Now that we are familiar with the Hive User View, let’s create the initial staging tables for the geolocation and trucks data. In this section we will learn how to use the terminal shell to create two of our four tables: geolocaton_stage and geolocation (top of Figure 3-15). We will use the Ambari Hive User View to create the other two tables, trucking_stage and trucking (bottom of Figure 3-15). First we are going to create 2 tables to stage the data in their original csv text format and then will create two more tables where we will optimize the storage with ORC.
 
Figure 3-15. Depiction of the table development process from .CSV to OCR
2.2.1 Create Table Geolocation_Stage for Staging Initial Load
Now let’s copy-and-paste the following query into the terminal shell to define a new table named geolocation_stage as seen in Figure 3-16. (To paste the command, either right-click and select paste or press Ctrl+Shift+V.)
?
CREATE TABLE geolocation_stage (truckid string, driverid string, event string, latitude DOUBLE, longitude DOUBLE, city string, state string, velocity BIGINT, event_ind BIGINT, idling_ind BIGINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
TBLPROPERTIES ("skip.header.line.count"="1");


 
Figure 3-16. geolocation_stage table creation in the query editor of the Hive User View
2.2.2 Execute a Query
We’ll press the Enter key to run the SQL statement seen in Figure 3-15. Alternatively, we can also paste the Create Table statement into the query editor of the Hive User View and run it by pressing the Execute button. If successful, we should see the Succeeded status in the Query Process Results section. 
2.2.3 Verify the geolocation_stage Table Creation
We can check that the trucks_stage table is present using the command show tables, as seen in Figure 3-17.
 
 Figure 3-17. Terminal showing tables we have created
2.2.4 Create a New Query and Rename the Query Worksheet
Notice the tab of our worksheet is labeled “Worksheet”. We will double-click on this tab to rename the label to “trucks_stage” as depicted in Figure 3-18.
 
Figure 3-18. Naming the trucks_stage table in the Hive User View
2.2.5 Create Table Trucks_Stage for Staging the Initial Load 
Now we copy-and-paste the following table query into our trucks_stage Query window of the Hive User View to define a new table named trucks_stage as shown in Figure 3-19. 
CREATE TABLE trucks_stage(driverid string, truckid string, model string, jun13_miles bigint, jun13_gas bigint, may13_miles bigint, may13_gas bigint, apr13_miles bigint, apr13_gas bigint, mar13_miles bigint, mar13_gas bigint, feb13_miles bigint, feb13_gas bigint, jan13_miles bigint, jan13_gas bigint, dec12_miles bigint, dec12_gas bigint, nov12_miles bigint, nov12_gas bigint, oct12_miles bigint, oct12_gas bigint, sep12_miles bigint, sep12_gas bigint, aug12_miles bigint, aug12_gas bigint, jul12_miles bigint, jul12_gas bigint, jun12_miles bigint, jun12_gas bigint,may12_miles bigint, may12_gas bigint, apr12_miles bigint, apr12_gas bigint, mar12_miles bigint, mar12_gas bigint, feb12_miles bigint, feb12_gas bigint, jan12_miles bigint, jan12_gas bigint, dec11_miles bigint, dec11_gas bigint, nov11_miles bigint, nov11_gas bigint, oct11_miles bigint, oct11_gas bigint, sep11_miles bigint, sep11_gas bigint, aug11_miles bigint, aug11_gas bigint, jul11_miles bigint, jul11_gas bigint, jun11_miles bigint, jun11_gas bigint, may11_miles bigint, may11_gas bigint, apr11_miles bigint, apr11_gas bigint, mar11_miles bigint, mar11_gas bigint, feb11_miles bigint, feb11_gas bigint, jan11_miles bigint, jan11_gas bigint, dec10_miles bigint, dec10_gas bigint, nov10_miles bigint, nov10_gas bigint, oct10_miles bigint, oct10_gas bigint, sep10_miles bigint, sep10_gas bigint, aug10_miles bigint, aug10_gas bigint, jul10_miles bigint, jul10_gas bigint, jun10_miles bigint, jun10_gas bigint, may10_miles bigint, may10_gas bigint, apr10_miles bigint, apr10_gas bigint, mar10_miles bigint, mar10_gas bigint, feb10_miles bigint, feb10_gas bigint, jan10_miles bigint, jan10_gas bigint, dec09_miles bigint, dec09_gas bigint, nov09_miles bigint, nov09_gas bigint, oct09_miles bigint, oct09_gas bigint, sep09_miles bigint, sep09_gas bigint, aug09_miles bigint, aug09_gas bigint, jul09_miles bigint, jul09_gas bigint, jun09_miles bigint, jun09_gas bigint, may09_miles bigint, may09_gas bigint, apr09_miles bigint, apr09_gas bigint, mar09_miles bigint, mar09_gas bigint, feb09_miles bigint, feb09_gas bigint, jan09_miles bigint, jan09_gas bigint)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
TBLPROPERTIES ("skip.header.line.count"="1");


 
Figure 3-19. trucks_stage table creation in the query editor of Hive User View
2.2.6 Execute the Query and Verify it Ran Successfully
Let’s review some aspects of the CREATE TABLE statements issued above.  If we have an SQL background this statement should seem very familiar except for the last 3 lines after the columns definition:
	The ROW FORMAT clause specifies each row is terminated by the new line character.
	The FIELDS TERMINATED BY clause specifies that the fields associated with the table (in our case, the two csv files) are to be delimited by a comma.
	The STORED AS clause specifies that the table will be stored in the TEXTFILE format.
For details on these clauses consult the Apache Hive Language Manual.
2.2.7 Verify the New Tables Exist
To verify the tables were defined successfully, we click the “refresh” icon in the Database Explorer (see Figure 3-20). Under Databases, click default database to expand the list of table and the new tables should appear:
 
Figure 3-20. Database explorer components
2.2.8 View the Trucks_Stage Schema
Next, we click on the trucks_stage table name to view its schema.
2.2.9 Load Sample Data of Trucks_Stage
Now we click on the Load sample data icon to generate and execute a select SQL statement to query the table for a 100 rows. Notice our two new tables are currently empty.
	We can have multiple SQL statements within each editor worksheet, but each statement needs to be separated by a semicolon “;”.
	If we have multiple statements within a worksheet but we only want to run one of them just highlight the statement we want to run and then click the Execute button.
A few additional commands to explore tables:
	show tables; List the tables created in the database by looking up the list of tables from the metadata stored in Hcatalog 
	describe –{table_name}; Provides a list of columns for a particular table (i.e. describe geolocation_stage.)
	show create {table_name}; Provides the DDL to recreate a table (i.e., show create table geolocation_stage;)
 By default, when we create a table in Hive, a directory with the same name gets created in the /apps/hive/warehouse folder in HDFS.  Using the Ambari Files User View, we can navigate to the 
/apps/hive/warehouse
folder. We should see both a geolocation_stage and trucks_stage directory as depicted in Figure 3-21.
 
Figure 3-21. File view of the final Hive warehouse for this project
The definition of a Hive table and its associated metadata (i.e., the directory the data is stored in, the file format, what Hive properties are set, etc.) are stored in the Hive metastore, which on the Sandbox is a MySQL database.
Step 2.3: Load Data into a Hive Table
2.3.1 Populate a Hive Table with Data
Let’s load some data into our two Hive tables. Populating a Hive table can be done in various ways. A simple way to populate a table is with a load statement in the terminal. To load the geolocation data from geolocation.csv into geolocation_stage, we use the following command:
LOAD DATA INPATH '/user/maria_dev/geolocation.csv' OVERWRITE INTO TABLE geolocation_stage;

Like wise, to load trucks.csv data into trucks_stage, we use:
LOAD DATA INPATH '/user/maria_dev/trucks.csv' OVERWRITE INTO TABLE trucks_stage;

Another way is to put our file into the directory associated with the table (see Figure 3-22). Using the Ambari Files User View, we click on the Move icon next to the file /tmp/maria_dev/geolocation.csv. (Clicking on Move is similar to “cut” in cut-and-paste.)
 
Figure 3-22. File view of geolocation.csv file in user folder
After we click on the Move arrow, your screen should look like Figure 3-23.
 
Figure 3-232. File view of geolocation.csv file in user folder selected for move
2.3.2 Notice two things have changed:
	The file name geolocation.csv is grayed out.
	The icons associated with the operations on the files are removed. This is to indicate that this file is in a special state that is ready to be moved.
2.3.3 Navigate to Destination Path and Paste File
Now we navigate to the destination path /apps/hive/warehouse/geolocation_stage. We might notice that as we navigate through the directories that the file is pinned at the top.  Once we get to the appropriate directory, we click on the Paste icon to move the file as depicted in Figure 3-24.
 
Figure 3-24. File view of the Hive warehouse with geoloction_stage
2.3.4 Load Sample Data of geolocation_stage
Using the editing window in the Hive User View, we enter the following:
SELECT * FROM geolocation_stage Limit 100;

This will give us the first 100 records from the geolocaton_stage table. 
Alternatively, we can click on the Load sample data icon next to the geolocation_stage table as we did in Section 2.2.7. Notice the table in Figure 3-25 is no longer empty, and you should see the first 100 rows of the table.
 
Figure 3-25. Sample from the geolocation_stage table in the Hive User View
2.3.5 Use Terminal Shell to Load Data into trucks_stage
To access the terminal shell, type the following address in our browser:
http://<hostname>:4200.
To load the data from trucks.csv, type the following command in the terminal shell:
LOAD DATA INPATH '/tmp/maria_dev/trucks.csv' OVERWRITE INTO TABLE trucks_stage;

2.3.6 Use Terminal Shell to Load Sample Data of trucks_stage
This is performed in the same manner as we did for geolocation_stage. When we have completed the process, we should see data in the trucks_stage table as depicted in Figure 3-26.
SELECT * FROM trucks_stage LIMIT 100
 
Figure 3-26. Sample from the trucks_stage table in the Hive User View
Step 3.4: Define an ORC Table in Hive
Introducing Apache ORC
The Optimized Row Columnar (ORC) file format provides an efficient way to store Hive data. It was designed to overcome limitations of other Hive file formats. Using ORC files improves performance when Hive is reading, writing, and processing data.
To use the ORC format, we specify ORC as the file format when creating the table:
CREATE TABLE … **STORED AS ORC**
In this step, we will create two ORC tables (geolocation and trucks) that are created from the text data in our geolocation_stage and trucks_stage tables.
2.4.1 Create geolocation as ORC from geolocation_stage 
Using the terminal, we execute the following command:
CREATE TABLE geolocation STORED AS ORC AS SELECT * FROM geolocation_stage;
2.4.2 Verify geolocation table is in the Default Database
Now we enter the command below to ensure the geolocation ORC table is in our database (with geolocation_stage) as shown in Figure 3-27, and verify geolocation is an ORC Table:
DESCRIBE FORMATTED geolocation;


 
Figure 3-27. Result of the describe formatted statement in the terminal shell
2.4.3 Create geolocation ORC from geolocation_stage using Hive
From the Ambari Hive User View, execute the following Create Table query to define a new table named geolocation (see Figure 3-28):
?
CREATE TABLE geolocation STORED AS ORC AS SELECT * FROM geolocation_stage;

 
Figure 3-28. Hive User View with ORC table defend in the query editor
2.4.4 Verify geolocation table is in the Default Database
Refresh the Database Explorer and verify you have a table named geolocation in the default database, as shown in Figure 3-29.
 
Figure 3-29. Database Explorer showing the Geolocation table added
We view the contents of the geolocation table and notice it contains the same rows as geolocation_stage. We verify geolocation is an ORC Table by executing the following query:
describe formatted geolocation;

Now, we scroll down to the bottom of the Results tab and we’ll see a section labeled Storage Information. The output should look like Figure 3-30.
 
Figure 3-30. Storage information for the Geolocation ORC table
2.4.5 Create trucks Table as ORC FROM trucks_stage Table
Execute the following query to define a new ORC table named trucks that contains the data from trucks_stage:
CREATE TABLE trucks STORED AS ORC TBLPROPERTIES ("orc.compress.size"="1024") AS SELECT * FROM trucks_stage;

2.4.6 Verify Table was Properly Created
Refresh the Database Explorer and view the contents of trucks as depicted in Figure 3-31.
Step 2.5: Analyze the Trucks Data
Next we will be using Hive, Pig and Excel to analyze derived data from the geolocation and trucks tables.  The business objective is to better understand the risk the company is under from fatigue of drivers, over-used trucks, and the impact of various trucking events on risk.   In order to accomplish this, we will apply a series of transformations to the source data, mostly though SQL, and use Pig or Spark to calculate risk. Figure 3-32 depicts the way in which the transformations will be performed.  
 
Figure 3-31. Databases view of the contents of the Trucks table
 
Figure 3-32. Depiction of the source dependencies and Hadoop table transformation operations
Let’s get started with the first transformation.   We want to calculate the miles per gallon for each truck. We will start with our truck data table.  We need to sum up all the miles and gas columns on a per truck basis. Hive has a series of functions that can be used to reformat a table. The keyword LATERAL VIEW is used in conjunction with user-defined table generated functions and is a we invoke things. The stack function allows us to restructure the data into 3 columns labeled rdate, gas and mile (ex: ‘june13’, june13_miles, june13_gas) that make up a maximum of 54 rows. We pick truckid, driverid, rdate, miles, gas from our original table and add a calculated column for mpg (miles/gas). And then we will calculate average mileage.
2.5.1 Create truck_mileage Table from Trucking Data
Due to the length of this query, we’ll use the Ambari Hive User View solely to build the truck_mileage table. So, we enter the following query in the query editing pane (see Figure 3-33) and execute it:
CREATE TABLE truck_mileage STORED AS ORC AS SELECT truckid, driverid, rdate, miles, gas, miles / gas mpg FROM trucks LATERAL VIEW stack(54, 'jun13',jun13_miles,jun13_gas,'may13',may13_miles,may13_gas,'apr13',apr13_miles,apr13_gas,'mar13',mar13_miles,mar13_gas,'feb13',feb13_miles,feb13_gas,'jan13',jan13_miles,jan13_gas,'dec12',dec12_miles,dec12_gas,'nov12',nov12_miles,nov12_gas,'oct12',oct12_miles,oct12_gas,'sep12',sep12_miles,sep12_gas,'aug12',aug12_miles,aug12_gas,'jul12',jul12_miles,jul12_gas,'jun12',jun12_miles,jun12_gas,'may12',may12_miles,may12_gas,'apr12',apr12_miles,apr12_gas,'mar12',mar12_miles,mar12_gas,'feb12',feb12_miles,feb12_gas,'jan12',jan12_miles,jan12_gas,'dec11',dec11_miles,dec11_gas,'nov11',nov11_miles,nov11_gas,'oct11',oct11_miles,oct11_gas,'sep11',sep11_miles,sep11_gas,'aug11',aug11_miles,aug11_gas,'jul11',jul11_miles,jul11_gas,'jun11',jun11_miles,jun11_gas,'may11',may11_miles,may11_gas,'apr11',apr11_miles,apr11_gas,'mar11',mar11_miles,mar11_gas,'feb11',feb11_miles,feb11_gas,'jan11',jan11_miles,jan11_gas,'dec10',dec10_miles,dec10_gas,'nov10',nov10_miles,nov10_gas,'oct10',oct10_miles,oct10_gas,'sep10',sep10_miles,sep10_gas,'aug10',aug10_miles,aug10_gas,'jul10',jul10_miles,jul10_gas,'jun10',jun10_miles,jun10_gas,'may10',may10_miles,may10_gas,'apr10',apr10_miles,apr10_gas,'mar10',mar10_miles,mar10_gas,'feb10',feb10_miles,feb10_gas,'jan10',jan10_miles,jan10_gas,'dec09',dec09_miles,dec09_gas,'nov09',nov09_miles,nov09_gas,'oct09',oct09_miles,oct09_gas,'sep09',sep09_miles,sep09_gas,'aug09',aug09_miles,aug09_gas,'jul09',jul09_miles,jul09_gas,'jun09',jun09_miles,jun09_gas,'may09',may09_miles,may09_gas,'apr09',apr09_miles,apr09_gas,'mar09',mar09_miles,mar09_gas,'feb09',feb09_miles,feb09_gas,'jan09',jan09_miles,jan09_gas ) dummyalias AS rdate, miles, gas;

 
Figure 3-33. Truck Mileage table creation in Hive User View
2.5.2 Generate Sample Data from truck_mileage
To check that our query worked as expected, we type in the following query in the query editing pane (we could also do this with a terminal entry):
SELECT truckid, avg(mpg) avgmpg FROM truck_mileage GROUP BY truckid;

Executing the query produces the results shown in Figure 3-34.
Now, we type in the following query, using Ctrl+space throughout your typing so that you can get an idea of what content assist can do and how it works:
SELECT truckid, avg(mpg) avgmpg FROM truck_mileage GROUP BY truckid;

Next, we click the “Save as …” button to save the query as “average mpg”. Notice our query now shows up in the list of “Saved Queries“, which is one of the tabs at the top of the Hive User View. Now we execute the “average mpg” query and view its results.
 
Figure 3-34. Sample data from the Truck Mileage table
?
2.5.3 Explore Explain Features of the Hive Query Editor
Now let’s explore the various explain features to better understand the execution of a query: Text Explain, Visual Explain and Tez Explain. Click on the Explain button, as seen in Figure 3-35.
 
Figure 3-35. Depiction of the Explain button in Hive User Views
Now, we add the Explain command at the beginning of the query:
EXPLAIN SELECT truckid, avg(mpg) avgmpg FROM truck_mileage GROUP BY truckid;

Next, we execute the query. An alternative way to execute explain results is to press the Explain button. The results should look like the ones shown in Figure 3-36.
 
Figure 3-36. Result of the Explain option for Truck Mileage table
Next, we click on STAGE-0: to view its output, which displays the flow of the resulting Tez job as shown in Figure 3-37.
To see the Visual Explain, we click on the Visual Explain icon on the right tabs. Figure 3-38 shows a much more readable summary of the explain plan.
 
Figure 3-37. Stage 0 of the Truck Mileage query Explain option
 
Figure 3-38. Visual explain version of Truck Mileage table
2.5.4 Create Truck avg_mileage Table Form Existing trucks_mileage Data
We may persist these results into a table, which is a fairly common pattern in Hive. We call this Create Table As Select (CTAS ).  We paste the following script into a new Worksheet and then click the Execute button (see Figure 3-39a and Figure 3-39b):
CREATE TABLE avg_mileage STORED AS ORC AS SELECT truckid, avg(mpg) avgmpg FROM truck_mileage GROUP BY truckid;

 	
Figure 3-39a. Average Mileage query in terminal shell
 
Figure 3-39b. Average Mileage run status in terminal shell
As an alternative, we may pass commands directly to Hive is to pass commands as a query string for the Hive shell to execute. We accomplish this with the -e option.
hive -e "CREATE TABLE average_mileage STORED AS ORC AS SELECT truckid, avg(mpg) avgmpg FROM truck_mileage GROUP BY truckid;"

Finally, we can perform the transformations using the Hive User View as shown in Figure 3-39c.
 
Figure 3-39c. Average Mileage in the Hive User View query editor
Alternatively, we can write the data to a comma separated text file using:
CREATE TABLE avg_mileage1 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE  
AS 
SELECT truckid, avg(mpg) avgmpg 
FROM truck_mileage 
GROUP BY truckid;

2.6.5 Load Sample Data from avg_mileage
To view the data generated by the script, we click Load sample data icon in the Database Explorer next to avg_mileage. We see our table is now a list of each trip made by a truck as shown in Figure 3-40.
 
Figure 3-40. Load sample data query and results in Hive User View
?
Step 3: Use Pig to Compute Driver Risk Factor
In this section, we will be discuss and use Apache Pig. In the last sections of lab, we learned how to load data into HDFS and then manipulate it using Hive. We are using the Truck sensor data to better understand risk associated with every driver. This section will demonstrate to computation of risk using Apache Pig.
Pig is a high-level scripting language used with Apache Hadoop. Pig enables data analyst to write complex data transformations without knowing Java. Pig’s simple SQL-like scripting language is called Pig Latin, and appeals to developers already familiar with scripting languages and SQL.
Pig is a complete package, so we can do all required data manipulations in Apache Hadoop with Pig. Through the User Defined Functions (UDF) facility in Pig, Pig can invoke code in many languages like JRuby, Jython and Java. We can also embed Pig scripts in other languages. The result is that we can use Pig as a component to build larger and more complex applications that tackle real business problems.
Pig works with data from many sources, including structured and unstructured data, and store the results into the Hadoop Data File System.
Pig scripts are translated into a series of MapReduce jobs that are run on the Apache Hadoop cluster.
Step 3.1: Define the Table Schema
Now we have refined the truck data to get the average mpg for each truck. The next task is to compute the risk factor for each driver which is the total miles driven/abnormal events. We can get the event information from the geolocation table as seen in Figure 3-41.
 
Figure 3-41. Geolocation table samle in Hive User View
If we look at the truck_mileage table, we have the driverid and the number of miles for each trip. To get the total miles for each driver, we can group those records by driverid and then sum the miles.
3.1.1 Create driver_mileage table From Existing truck_mileage Data
We will start by creating a table named driver_mileage that is created from a query of the columns we want from truck_mileage. The following query groups the records by driverid and sums the miles in the select statement. We execute the query below in a new Worksheet:
CREATE TABLE driver_mileage
STORED AS ORC
AS
SELECT driverid, sum(miles) totmiles
FROM truck_mileage
GROUP BY driverid;

3.1.2 View Data Generated by the Query
To view data, we click the Load sample data icon in the Database Explorer next to driver_mileage. The results should look like those in Figure 3-42.
 
Figure 3-42. Driver mileage generated from the Load Sample Data button in Hive User View
3.1.3 Create riskfactor Table From Existing trucks_mileage Data 
Next, we will use Pig to compute the risk factor of each driver. Before we can run the Pig code, the table must already exist in Hive to satisfy one of the requirements for the HCatStorer() class. The Pig code expects the following structure for a table named riskfactor. We execute the following DDL command:
CREATE TABLE riskfactor (driverid string, events bigint, totmiles bigint, riskfactor float)
STORED AS ORC;

3.1.4 Verify riskfactor Table was Created Successfully 
Now, we verify the riskfactor table was created successfully. It will be empty now, but we will populate it from a Pig script. We are now ready to compute the risk factor using Pig. Let’s take a look at Pig and how to execute Pig scripts from within Ambari.
Step 3.2: Create Pig Script
Now, we will create and run a Pig script using the Ambari Pig User View. 
3.2.1 Log in to Ambari Pig User Views
To get to the Ambari Pig User View, we click on the User Views icon at top right and select Pig as shown in Figure 5-43.
 
Figure 3-43. Ambari menu with Pig selected
This will bring up the Ambari Pig User View interface. Our Pig View does not have any scripts to display, so it will look like Figure 5-44.
 
Figure 3-44. Pig scripts window with no scripts defined
On the left is a list of our scripts, and on the right is a composition box for writing scripts. A special interface feature is the Pig helper located below the name of our script file. The Pig helper provides us with templates for the statements, functions, I/O statements, HCatLoader() and Python user defined functions. At the very bottom are status areas that will show the results of our script and log files.
3.2.2 Create a New Script
Let’s enter a Pig script. We click the New Script button in the upper-right corner of the view as shown in Figure 3-45.
 
Figure 3-45. Pig script window showing New Script button
We name the script riskfactor.pig and then click the Create button.
3.2.3 Load Data in Pig using HCatalog 
We will use HCatalog to load data into Pig. HCatalog allows us to share schema across tools and users within our Hadoop environment. It also allows us to factor out schema and location information from our queries and scripts and centralize them in a common repository. Since it is in HCatalog we can use the HCatLoader() function. Pig allows us to give the table a name or alias and not have to worry about allocating space and defining the structure. We just have to worry about how we are processing the table.
	We can use the Pig helper located below the name of your script file to give us a template for the line. Click on Pig helper -> HCatalog->load template
	The entry TABLE is highlighted in red for us. Type the name of the table which is geolocation.
	Remember to add the a = before the template. This saves the results into a. Note the ‘=’ has to have a space before and after it.
	Our completed line of code will look like:
a = LOAD 'geolocation' using org.apache.hive.hcatalog.pig.HCatLoader();

The script above loads data, in our case, from a file named geolocation using the HCatLoader()function. We copy-and-paste the above Pig code into the riskfactor.pig window.
3.2.4 Filter Our Dataset
The next step is to select a subset of the records, so we have the records of drivers for which the event is not normal. To do this in Pig we use the Filter operator. We instruct Pig to Filter our table and keep all records where event !=”normal” and store this in b. With this one simple statement, Pig will look at each record in the table and filter out all the ones that do not meet our criteria.
	We can use Pig Help again by clicking on Pig helper->Relational Operators->FILTER template
	We can replace VAR with “a” (hint: tab jumps you to the next field)
	Our COND is “event !=’normal’; ” (note: single quotes are needed around normal and don’t forget the trailing semi-colon)
	The complete line of code will look like:
b = filter a by event != 'normal';

Now we copy-and-paste the above Pig code into the riskfactor.pig window.
3.2.5 Iterate Our Dataset
Since we have the right set of records, let’s iterate through them. We use the “foreach” operator on the grouped data to iterate through all the records. We would also like to know the number of non normal events associated with a driver, so to achieve this we add ‘1’ to every row in the data set.
	Pig helper ->Relational Operators->FOREACH template will get us the code
	Our DATA is b and the second NEW_DATA is “driverid, event, (int) ‘1’ as occurrance;“
	The complete line of code will look like:
c = foreach b generate driverid, event, (int) '1' as occurrance;

Now we copy-and-paste the above Pig code into the riskfactor.pig window:
3.2.6 Calculate the Total Non-Normal Events for Each Driver 
The group statement is important because it groups the records by one or more relations. In our case, we want to group by driver id and iterate over each row again to sum the non normal events.
	Pig helper ->Relational Operators->GROUP VAR BY VAR template will get us the code
	First VAR takes “c” and second VAR takes “driverid;“
	The complete line of code will look like:
d = group c by driverid;

Now we copy-and-paste the above Pig code into the riskfactor.pig window.
Next we use Foreach statement again to add the occurrence.
e = foreach d generate group as driverid, SUM(c.occurance) as t_occ;

3.2.7 Load driver_mileage Table and Perform a Joint Operation
In this part, we will load driver_mileage table into Pig using HCatalog and perform a join operation on driverid. The resulting data set will give us total miles and total non normal events for a particular driver.
	Load driver_mileage using HcatLoader()
g = LOAD 'driver_mileage' using org.apache.hive.hcatalog.pig.HCatLoader();

	Pig helper ->Relational Operators->JOIN VAR BY template will get us the code
	Replace VAR by ‘e‘ and after BY put ‘driverid, g by driverid;‘
	The complete line of code will look like:
h = join e by driverid, g by driverid;

Now we copy-and-paste the above two Pig codes into the riskfactor.pig window.
3.2.8 Compute Driver Risk Factor
In this part, we will associate a driver risk factor with every driver. To calculate driver risk factor, divide total miles travelled by non normal event occurrences.
	We will use Foreach statement again to compute driver risk factor for each driver.
	Use the following code and paste it into your Pig script.
final_data = foreach h generate $0 as driverid, $1 as events, $3 as totmiles, (float) $3/$1 as riskfactor;

	As a final step, store the data into a table using Hcatalog.
store final_data into 'riskfactor' using org.apache.hive.hcatalog.pig.HCatStorer();

Here is the final code and what it will look like once you paste it into the editor (see Figure 3-46). 
a = LOAD 'geolocation' using org.apache.hive.hcatalog.pig.HCatLoader( );
b = filter a by event != 'normal';
c = foreach b generate driverid, event, (int) '1' as occurance;
d = group c by driverid;
e = foreach d generate group as driverid, SUM(c.occurance) as t_occ;
g = LOAD 'driver_mileage' using org.apache.hive.hcatalog.pig.HCatLoader( );
h = join e by driverid, g by driverid;
final_data = foreach h generate $0 as driverid, $1 as events, $3 as totmiles, (float) $3/$1 as riskfactor;
store final_data into 'user/jstrickland/data' using PigStorage(',');

Now, we save the file riskfactor.pig by clicking the Save button in the left-hand column.
 
Figure 3-46. Pig script editing wind with our final script
Step 3.3: Pig Script Recap
Before we execute the code, let’s review the code again:
	The line a loads the geolocation table from HCatalog.
	The line b filters out all the rows where the event is not ‘Normal’.
	Then we add a column called occurrence and assign it a value of 1.
	We then group the records by driverid and sum up the occurrences for each driver.
	At this point we need the miles driven by each driver, so we load the table we created using Hive.
	To get our final result, we join by the driverid the count of events in e with the mileage data in g.
	Now it is real simple to calculate the risk factor by dividing the miles driven by the number of events
We need to configure the Pig Editor to use HCatalog so that the Pig script can load the proper libraries. In the Pig arguments text box, enter -useHCatalog and click the Add button as seen in Figure 3-47.
Note this argument is case sensitive. It should be typed exactly “-useHCatalog”.
 
Figure 3-47. Pig script argument which will be added to our script
Step 3.4: Execute Pig Script on Tez
3.4.1 Execute Pig Script
Next, we click Execute on Tez checkbox and finally hit the blue Execute button to submit the job. Pig job will be submitted to the cluster. This will generate a new tab with a status of the running of the Pig job and at the top you will find a progress bar that shows the job status as depicted in Figure 3-48.
 
Figure 3-48. Pig script running with job status showing
3.4.2 View Results Section
Now we wait for the job to complete. The output of the job is displayed in the Results section. Notice our script does not output any result – it stores the result into a Hive table – so our Results section will be empty (see Figure 3-49).
 
Figure 3-49. Pig script window showing completion status for this job
Now we click on the Logs dropdown menu to see what happened when your script ran. Errors will appear here.
3.4.3 Verify Pig Script Successfully Populated the Hive Table 
Now let’s go back to the Ambari Hive User View and browse the data in the riskfactor table to verify that your Pig job successfully populated this table. Figure 3-50 shows what is should look like.
At this point we now have our truck miles per gallon table and our risk factor table. The next step is to pull this data into Excel to create the charts for the visualization step.
Summary
In this section we learned Pig commands to compute risk factor analysis on the geolocation and truck data. We learned to use Pig to access the data from Hive using the LOAD {hive_table} …HCatLoader() script. Therefore, we were able to perform the filter, foreach, group, join, and store {hive_table} …HCatStorer() scripts to manipulate, transform and process this data. To review these bold pig Latin operators, view the Pig Latin Basics, which contains documentation on each operator.
 
Figure 3-50. Pigscript window show sample risk factor data
?
Exercises
	Download the data set Lahman Baseball Database – lahman591-csv.zip, < http://seanlahman.com/files/database/baseballdatabank-master_2016-03-02.zip>. We have several files of baseball statistics and we are going to bring them into Hadoop and do some simple computing with them. We are going to find the player with the highest runs for each year. This file has all the statistics from 1871–2011 and contains more that 90,000 rows. Once we have the highest runs we will extend the script to translate a player id field into the first and last names of the players.
	Using your Hadoop environment, write a Pig script that generates player ID, year, and total runs by player ID. Sort the query by year.
	Using your Hadoop environment, write a series of Hive queries that will generate a file with player ID, year, and total runs by player ID. Sort the query by year.
	What is the difference in performing the task in Pig (a) and Hive (b)?
	Using your Hadoop environment, write a script that generates regular season pitcher (player ID), year, and ERA (earned run average) by pitcher player ID. Sort the list by ERA
	Download the data set RefineDemoData.zip (data for a fictitious web retail store) < https://s3.amazonaws.com/hw-sandbox/tutorial8/RefineDemoData.zip> Here’s a summary of the data we’re working with:
	omniturelogs – website logs containing information such as URL, timestamp, IP address, geocoded IP, and session ID.
	users – CRM user data listing SWIDs (Software User IDs) along with date of birth and gender.
	products – CMS data that maps product categories to website URLs.
	Create the tables users, products and omniturelogs.
	load the data into the tables
	create a table containing col_2 ts (timestamp), col_8 ip, col_13 url, col_14 swid, col_50 city, col_51 country, col_53 state
	Create a script that joins the omniture website log data to the CRM data (registered users) and CMS data (products), containing logdate (web), url (web), ip-address (web), city (web), state (web), country (web), category (product) birth date (user), age (user), and gender (user).
?
?
Working With Apache Spark
Using Apache Spark to Compute Driver Risk Factor 
In this chapter we will introduce Apache Spark. In the previous chapter we learned how to load data into HDFS and then manipulate it using Hive and Pig. We are using the Truck sensor data to better understand risk associated with every driver. This chapter will teach you how to compute risk using Apache spark.
Introduction
Although MapReduce has been useful, the amount of time it takes for the tasks to run can be lengthy at times. Moreover, MapReduce only work for a specific set of use cases. We have a need for a computing structure that works for a broader set of use cases.
Developers of Apache Spark designed it to be a fast, general-purpose, user friendly computing platform. It extends the MapReduce model and takes it to a new level. In memory computations produces additional speed. When applications run in memory it allows for much faster processing and response.
Apache Spark has sophisticated development APIs programmed in Scala, Java, and Python. These APIs allow data analysts to efficiently execute machine learning algorithms, which need fast iterative access to datasets. Spark on Apache Hadoop YARN enables integration with Hadoop and other YARN enabled jobs in the enterprise.
We can run batch applications and iterative algorithms that build upon each other, For instance Tez-type jobs and machine learning algorithms. We can also run interactive queries and process streaming data with our applications. Spark also provides a plentiful libraries that we can use to expand beyond the basic Spark capabilities such as machine learning algorithms, structured query language (SQL), and graph processing. Spark runs on Hadoop clusters such as Hadoop YARN or Apache Mesos, or even in a Standalone Mode with its own scheduler. The Spark architecture is depicted in Figure 4-1.
 
Figure 4-1. Apache Spark architecture includes components of data management, data access, security, operations, and governance.
STEP 1: Configure Spark Service Using Ambari
First, we log on to Ambari Dashboard as a maria_dev. At the bottom left corner of the services column, check that Spark and Zeppelin are running as shown in Figure 4-2.
Note: If these services are disabled, we will need to login in as an admin user to start all services. Refer to the Hortonworks tutorial Learning the Ropes of Hortonworks Sandbox at http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/ for steps to gain admin privileges.
 
Figure 4-2. Ambari dashboard under the maria_dev user. Spark and Zeppelin services are listed at the bottom of the left-side list
Next, we open a new browser tab and type the following address to access Zeppelin (hostname is the ip address, for instance):
<hostname>:9995

Now we should see a Zeppelin Welcome Page As seen in Figure 4-3.
 
Figure 4-3. The Zeppelin Notebook welcome page, with Notebook menu at the top-left
If we need assistance figuring out our hostname we can refer to the tutorial Learning the Ropes of Hortonworks Sandbox .
Optionally, if you want to find out how to access the Spark shell to run code on Spark refer to last section of this chapter
Next, we create a Zeppelin Notebook by clicking on the Notebook tab at the top-left and select Create new note. We’ll name our notebook ComputeRiskfactor with Spark as seen in Figure 4-4. By the default, the notebook will load Spark Scala API.
 
Figure 4-4. New notebook named Compute Riskfactor with Spark showing in the Notebook pull-down menu
STEP 2: Create a HiveContext
For improved Hive integration, HDP 2.4 offers ORC file support for Spark. This allows Spark to read data stored as ORC files. Spark can leverage ORC file’s efficient columnar storage and establish pushdown capability for even faster in-memory processing. There are tow SQL contexts in Spark. HiveContext is an instance of the Spark SQL execution engine that integrates with data stored in Hive. The basic SQLContext provides a subset of the Spark SQL support that does not depend on Hive. It reads the configuration for Hive from hive-site.xml on the classpath.
Step 2.1 Import SQL Libraries:
First, we copy and paste the following code into your Zeppelin notebook (see Figure 4-5), then click the play button. Alternatively, press shift+enter to run the code.
import org.apache.spark.sql.hive.orc._
import org.apache.spark.sql._

 
Figure 4-5. Zeppelin notebook with import SQL libraries code
Step 2.2 Instantiate HiveContext
Now we enter the following code to instantiate HiveContext (see Figure 4-6):
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

 
Figure 4-6. Zeppelin window with HiveContext instantiation code
SC stands for Spark Context. SparkContext is the main entry point for everything Spark. It can be used to create Resilient Distributed Dataset (RDDs) and shared variables on the cluster. When we start the Spark Shell, the SparkContext is automatically initialized for us with the variable SC.
STEP 3: Create a RDD From Hive Context
Spark’s primary core abstraction is called a Resilient Distributed Dataset or RDD. It is a distributed collection of elements that is parallelized across the cluster. In other words, a RDD is an immutable collection of objects that is partitioned and distributed across multiple physical nodes of a YARN cluster that can be operated in parallel.
There are three methods for creating a RDD:
	Parallelize an existing collection. This means that the data already resides within Spark and can now be operated on in parallel.
	We create a RDD by referencing a dataset. This dataset can come from any storage source supported by Hadoop such as HDFS, Cassandra, HBase etc.
	We create a RDD by transforming an existing RDD to create a new RDD.
We will be using the latter two methods in this chapter.
RDD Transformations and Actions
Typically, RDDs are instantiated by loading data from a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat on a YARN cluster.
Once a RDD is instantiated, you can apply a series of operations. All operations fall into one of two types: transformations or actions.
	Transformation operations, as the name suggests, create new datasets from an existing RDD and build out the processing DAG that can then be applied on the partitioned dataset across the YARN cluster. Transformations do not return a value. In fact, nothing is evaluated during the definition of these transformation statements. Spark just creates these Direct Acyclic Graphs or DAG, which will only be evaluated at runtime. We call this lazy evaluation.
	An Action operation, on the other hand, executes a DAG and returns a value.
3.1 View List of Tables in Hive Warehouse
We will use a simple show command to see the list of tables in Hive warehouse (see Figure 4-7).
hiveContext.sql("show tables").collect.foreach(println)

 
Figure 4-7. Zeppelin window with HiveContext code showing a list of tables in the Hive warehouse directory. Note: false indicates whether the column requires data.
We notice that the geolocation table and the driver mileage table we created earlier are already listed in Hive metastore and can be directly queried upon.
3.2 Query Tables To Build Spark RDD
We will do a simple Select query to fetch data from geolocation and driver_mileage tables to a spark variable. Getting data into Spark this way also allows us to copy table schema to RDD (see Figure 4-8).
val geolocation_temp1 = hiveContext.sql("select * from geolocation")

val driver_mileage_temp1 = hiveContext.sql("select * from driver_mileage")

 
Figure 4-8. Select statements using HiveContext in Spark
3.3 Querying Against a Table
3.3.1 Registering a Temporary Table
Now let’s register a temporary table and use SQL syntax to query against that table.
geolocation_temp1.registerTempTable("geolocation_temp1")
driver_mileage_temp1.registerTempTable("driver_mileage_temp1")

Next, we will perform an iteration and a filter operation. First, we need to filter drivers that have non-normal events associated with them and then count the number for non-normal events for each driver.
val geolocation_temp2 = hiveContext.sql("SELECT driverid, count(driverid) occurance from geolocation_temp1 where event!='normal' group by driverid")

As stated earlier about RDD transformations, a Select operation is a RDD transformation and therefore does not return anything.
The resulting table will have a count of total non-normal events associated with each driver. We register this filtered table as a temporary table so that subsequent SQL queries can be applied to it.
geolocation_temp2.registerTempTable("geolocation_temp2")

We can view the result by executing an action operation on the RDD (see Figure 4-9).
geolocation_temp2.collect.foreach(println)

 
Figure 4-9. Geolocation with total non-normal events grouped by driverid
3.3.2 Perform join Operation
In this section we will perform a join operation. The geolocation_temp2 table has details of drivers and count of their respective non-normal events. The driver_mileage_temp1 table has details of total miles travelled by each driver.
We will join two tables on common column, which in our case is driverid.
val joined = hiveContext.sql("select a.driverid,a.occurance,b.totmiles from geolocation_temp2 a,driver_mileage_temp1 b where a.driverid=b.driverid")

The resulting data set will give us total miles and total non-normal events for a particular driver. Register this filtered table as a temporary table so that subsequent SQL queries can be applied to it.
joined.registerTempTable("joined")

We can view the result by executing action operation on RDD (see Figure 4-10).
joined.collect.foreach(println)

 
Figure 4-10. Zeppelin window with the join rsult showing driverid, total number of non-normal events, and total miles
3.3.3 Compute Driver Risk Factor
In this part we will associate a driver risk factor with every driver. Driver risk factor will be calculated by dividing total miles travelled by non-normal event occurrences.
val risk_factor_spark=hiveContext.sql("select driverid, occurance, totmiles, totmiles/occurance riskfactor from joined")

The resulting data set will give us total miles and total non normal events and what is a risk for a particular driver. Register this filtered table as a temporary table so that subsequent SQL queries can be applied to it.
risk_factor_spark.registerTempTable("risk_factor_spark")

Now let’s view the results (see Figure 4-11) using the following:
risk_factor_spark.collect.foreach(println)

?
 
Figure 4-22. Zeppelin window showing the joined tables with driverid, occurance, totmiles, totmiles/occurance riskfactor
STEP 4: Load and Save Data Into Hive As ORC
In this step we store data in a smart Optimized Row Columnar (ORC) format using Spark. ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads and with integrated support for finding required rows fast. Storing data in a columnar format lets the reader read, decompress, and process only the values required for the current query. Because ORC files are type aware, the writer chooses the most appropriate encoding for the type and builds an internal index as the file is persisted.
Predicate pushdown uses those indexes to determine which stripes in a file need to be read for a particular query and the row indexes can narrow the search to a particular set of 10,000 rows. ORC supports the complete set of types in Hive, including the complex types: structures, lists, maps, and unions.
4.1 Create an ORC table
Here, we create a table and store it as ORC. Specifying as orc at the end of the SQL statement below ensures that the Hive table is stored in the ORC format.
hiveContext.sql("create table finalresults( driverid String, occurance bigint,totmiles bigint,riskfactor double) stored as orc").toDF()

Note: to DF() creates a DataFrame with columns driverid String, occurance begin, etc.
4.2 Convert data into ORC table
Before we load the data into hive table that we created above, we will have to convert our data file into ORC format too.
Note: For Spark 1.3.1, use
risk_factor_spark.saveAsOrcFile("risk_factor_spark")

Note: For Spark 1.4.1 and higher, use
risk_factor_spark.write.format("orc").save("risk_factor_spark")

4.3 Load the data into Hive table using load data command
Now let’s load the data using:
hiveContext.sql("load data inpath 'risk_factor_spark' into table finalresults")

4.4 Verify Data Successfully Populated Table
Execute a select query to verify your table has been successfully stored. We can go to Ambari Hive user view to check whether the Hive table you created has the data populated in it (see Figure 4-12).
hiveContext.sql("select * from finalresults")
 
Figure 4-12. Hive riskfactor sample results table populated
Full Spark Code For This Chapter
import org.apache.spark.sql.hive.orc._
import org.apache.spark.sql._

val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

hiveContext.sql("show tables").collect.foreach(println)

val geolocation_temp1 = hiveContext.sql("select * from geolocation")

val drivermileage_temp1 = hiveContext.sql("select * from drivermileage")

geolocation_temp1.registerTempTable("geolocation_temp1")
drivermileage_temp1.registerTempTable("drivermileage_temp1")

val geolocation_temp2 = hiveContext.sql("SELECT driverid, count(driverid) occurance from geolocation_temp1  where event!='normal' group by driverid")

geolocation_temp2.registerTempTable("geolocation_temp2")

geolocation_temp2.collect.foreach(println)

val joined = hiveContext.sql("select a.driverid,a.occurance,b.totmiles from geolocation_temp2 a,drivermileage_temp1 b where a.driverid=b.driverid")

joined.registerTempTable("joined")

joined.collect.foreach(println)

val risk_factor_spark=hiveContext.sql("select driverid, occurance, totmiles, totmiles/occurance riskfactor from joined")

risk_factor_spark.registerTempTable("risk_factor_spark")

risk_factor_spark.collect.foreach(println)

hiveContext.sql("create table finalresults( driverid String, occurance bigint,totmiles bigint,riskfactor double) stored as orc").toDF()

risk_factor_spark.write.orc("risk_factor_spark")

hiveContext.sql("load data inpath 'risk_factor_spark' into table finalresults")

hiveContext.sql("select * from finalresults")
Run Spark Code in Interactive Terminal Shell
To run Spark code in the terminal shell, we open our terminal or putty and SSH into the Sandbox using root as login and hadoop as password.
login: root
password: hadoop

Optionally, if we don’t have an SSH client installed and configured we can use the built-in web client which can be accessed from here: http://hostname:4200 (use the same username and password provided above)
Now let’s enter the Spark interactive shell (spark repl), by typing the command
spark-shell

This will load the default Spark Scala API as seen in Figure 4-13.
 
Figure 4-13. Spark Shell startup screen
Note: Hive comes preconfigured with HDP Sandbox.
The coding exercise we just went through can be also completed using a Spark shell. Just as we did in Zeppelin, you can copy and paste the code to the scala prompt, as shown in Figure 4-14.
 Figure 4-14. Partial HiveContext code in Spark showing the list of tables the Hive warehouse directory
Summary
In this chapter we acquired the spark coding skills and knowledge necessary to compute risk factor associated with every driver. Apache Spark is efficient for computation because of its in-memory data processing engine. We learned how to integrate hive with spark by creating a Hive Context. We used our existing data from Hive to create an RDD. We learned to perform RDD transformations and actions to create new datasets from existing RDDs. These new datasets include filtered, manipulated and processed data. After we computed risk factor, we learned to load and save data into Hive as ORC.

Exercises
	Using the datasets contained in RefineDemoData.zip from Chapter 3, repeat the data manipulations using Apache Spark:
	Create the tables users, products and omniturelogs.
	load the data into the tables
	create a table containing col_2 ts, col_8 ip, col_13 url, col_14 swid, col_50 city, col_51 country, col_53 state
	Create a script that joins the omniture website log data to the CRM data (registered users) and CMS data (products).
	Download the dataset SensorFiles.zip (HVAC and Building data) http://s3.amazonaws.com/hw-sandbox/tutorial14/SensorFiles.zip
	Create a hvac table
	Create a building table
	Load data into the tables
	Calculate Cost Base Optimization (CBO) column statistics
?
Modeling with Data
Introduction
Predictive modeling is the process by which a logical rule, mathematical expression, or another algorithm is created or chosen to try to best describe or predict the probability of an outcome (Geisser, 1993). In many cases the model is chosen on the basis of detection theory to try to guess the probability of an outcome given a set amount of input data, for example, given an email determining how likely that it is spam. Models can use one or more classifiers in trying to determine the probability of a set of data belonging to another set, say spam or ‘ham’.
There are several types of models we should know about, but I will only talk about a few in the text. One major class of models cluster models and we will address some of these later. Another is propensity models. Propensity models are what most people think of when they hear “predictive analytics”. Propensity models make predictions about a customer’s future behavior. With propensity models you can anticipate a customers’ future behavior. However, keep in mind that even propensity models are abstractions and do not necessarily predict absolute true behavior.  We will go through six examples of propensity models to explain the concept.
Propensity Models
Model 1: Predicted customer lifetime value
CLV (Customer Lifetime Value) is a prediction of all the value a business will derive from their entire relationship with a customer. The Pareto Principle states that, for many events, roughly 80% of the effects come from 20% of the causes. When applied to e-commerce, this means that 80% of your revenue can be attributed to 20% of your customers. While the exact percentages may not be 80/20, it is still the case that some customers are worth a whole lot more than others, and identifying your “All-Star” customers can be extremely valuable to your business. Algorithms can predict how much a customer will spend with you long before customers themselves realizes this.
At the moment a customer makes their first purchase you may know a lot more than just their initial transaction record: you may have email and web engagement data for example, as well as demographic and geographic information. By comparing a customer to many others who came before them, you can predict with a high degree of accuracy their future lifetime value. This information is extremely valuable as it allows you to make value based marketing decisions. For example, it makes sense to invest more in those acquisition channels and campaigns that produce customers with the highest predicted lifetime value.
Model 2: Predicted share of wallet
Predicted share of wallet refers to the amount of the customer’s total spending that a business captures in the products and services that it offers. Increasing the share of a customer’s wallet a company receives is often a cheaper way of boosting revenue than increasing market share. For example if a customer spends $100 with you on groceries, is this 10% or 90% of their grocery spending for a given year? Knowing this allows you to see where future revenue potential is within your existing customer base and to design campaigns to capture this revenue.
Model 3: Propensity to engage
A propensity to engage model predicts the likelihood that a person will engage in some activity, like unethical behavior or post purchases. For example, a propensity to engage model can predict how likely it is that a customer will click on your email links. Armed with this information you can decide not to send an email to a certain “low likelihood to click” segment.
Model 4: Propensity to unsubscribe
A propensity to unsubscribe model tells you which customers not to touch: if there are high value customers you are at risk of losing to unsubscribe, you need to find other ways to reaching out to them that are not by email. For example, you can predict how likely it is that a customer will unsubscribe from your email list at any given point in time. Armed with this information you can optimize email frequency. For “high likelihood to unsubscribe” segments, you should decrease send frequency; whereas for “low likelihood to unsubscribe” segments, you can increase email send frequency. You could also decide to use different channels (like direct mail or LinkedIn) to reach out to “high likelihood to unsubscribe” customers.
Model 5: Propensity to buy
The propensity to buy model tells you which customers are ready to make their purchase, so you can find who to target. Moreover, once you know who is ready and who is not helps you provide the right aggression in your offer. Those that are likely to buy won’t need high discounts (You can stop cannibalizing your margin) while customers who are not likely to buy may need a more aggressive offer, thereby bringing you incremental revenue.
For example, a “propensity to buy a new vehicle” model built with only data the automotive manufacturer has in their database can be used to predict percent of sales. By incorporating demographic and lifestyle data from third parties, the accuracy of that model can be improved. That is, if the first model predicts 50% sales in the top five deciles (there are ten deciles), then the latter could improve the result to 70% in the top five deciles.
Model 6: Propensity to churn
Companies often rely on customer service agents to “save” customers who call to say they are taking their business elsewhere. But by this time, it is often too late to save the relationship. The propensity to churn model tells you which active customers are at risk, so you know which high value, at risk customers to put on your watch list and reach out. Armed with this information, you may be able to save those customers with preemptive marketing programs designed to retain them.
Often propensity models can be combined to make campaign decisions. For example, you may want to do an aggressive customer win back campaign for customers who have both a high likelihood to unsubscribe and a high predicted lifetime value.
Cluster Models
Clustering is the predictive analytics term for customer segmentation. Clustering, like classification, is used to segment the data. Unlike classification, clustering models segment data into groups that were not previously defined. Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them.
With clustering you let the algorithms, rather than the marketers, create customer segments. Think of clustering as auto-segmentation. Algorithms are able to segment customers based on many more variables than a human being ever could. It’s not unusual for two clusters to be different on 30 customer dimensions or more. In this article I will talk about three different types of predictive clustering models.
Model 7: Behavioral clustering
Behavioral clustering informs you how people behave while purchasing. Do they use the web site or the call center? Are they discount addicts? How frequently do they buy? How much do they spend? How much time will go buy before they purchase again? This algorithm helps set the right tone while contacting the customer. For instance, customers that buy frequently but with low sized orders might react well to offers like ‘Earn double rewards points when you spend $100 or more.
Behavioral clustering can also informs us on other behaviors, such as crime (see Figure 5-1) and is used in performing crime analysis. In the example below there are three crime clusters (only the top ten are shown in the table).
 
Figure 5-1. Crime Data Clsutering
Graphically, the clusters appear as in Figure 5-2.
 
Figure 5-2. Cluster plot created in R Studio
Model 8: Product based clustering (also called category based clustering)
Product based clustering algorithms discover what different groupings of products people buy from. See the example in Figure 5-3 of a category (or product) based segment or cluster. You can see people in one customer segment ONLY buy Pinot Noir, whereas those in another customer segment buy different types of Varietal products, such as Champagne, Chardonnay, Pinot Grigio and Prosecco – but never Cabernet Sauvignon, Malbec or Espumante. This is useful information when deciding which product offers or email content to send to each of these customer segments.
 
Figure 5-3. Product-based clustering
Model 9: Brand based clustering
Brand-based clustering, on the other hand, focuses on the brand of items customers purchase. Marketers can use this information to project what other brands those customers are likely to buy. Customers are then ordered according to Nike, Adidas, Under Armour, etc. Now you know what specific brands to pitch to certain customers. When a brand releases new products – you know who is likely to be interested.
Conclusion
Predictive analytics models are great, but they are ultimately useless unless you can actually tie them to your day-to-day marketing campaigns. This leads me to the first rule of predictive analytics:
 “Always make sure that your predictive analytics platform is directly integrated with your marketing execution systems such as your email service provider, web site, call center or Point of Sell (POS) system.”
It is better to start with just one model that you use in day-to-day marketing campaigns than to have 10 models without the data being actionable in the hands of marketers.
Presenting and Using the Results of a Predictive Model
Predictive models can either be used directly to estimate a response (output) given a defined set of characteristics (input), or indirectly to drive the choice of decision rules (Steyerberg, 2010).
Depending on the methodology employed for the prediction, it is often possible to derive a formula that may be used in a spreadsheet software.  This has some advantages for end users or decision makers, the main one being familiarity with the software itself, hence a lower barrier to adoption.
Nomograms are useful graphical representation of a predictive model. As in spreadsheet software, their use depends on the methodology chosen. The advantage of nomograms is the immediacy of computing predictions without the aid of a computer.
Point estimates tables are one of the simplest form to represent a predictive tool. Here combination of characteristics of interests can either be represented via a table or a graph and the associated prediction read off the y-axis or the table itself.
Tree based methods (e.g. CART, survival trees) provide one of the most graphically intuitive ways to present predictions. However, their usage is limited to those methods that use this type of modeling approach which can have several drawbacks (Breiman L. , 1996). Trees can also be employed to represent decision rules graphically.
Score charts are graphical tabular or graphical tools to represent either predictions or decision rules.
A new class of modern tools are represented by web based applications. For example, Shiny is a web based tool developed by Rstudio, an R IDE (integrated development environment). With a Shiny app, a modeler has the advantage to represent any which way he or she chooses to represent the predictive model while allowing the user some control. A user can choose a combination of characteristics of interest via sliders or input boxes and results can be generated, from graphs to confidence intervals to tables and various statistics of interests. However, these tools often require a server installation of Rstudio.
Applications
Uplift Modeling
Uplift Modeling (see Chapter 17) is a technique for modeling the change in probability caused by an action. Typically this is a marketing action such as an offer to buy a product, to use a product more or to re-sign a contract. For example in a retention campaign you wish to predict the change in probability that a customer will remain a customer if they are contacted. A model of the change in probability allows the retention campaign to be targeted at those customers on whom the change in probability will be beneficial. This allows the retention program to avoid triggering unnecessary churn or customer attrition without wasting money contacting people who would act anyway.
Archaeology
Predictive modeling in archaeology gets its foundations from Gordon Willey‘s mid-fifties work in the Virú Valley of Peru (Willey, 1953). Complete, intensive surveys were performed then covariability between cultural remains and natural features such as slope, and vegetation were determined. Development of quantitative methods and a greater availability of applicable data led to growth of the discipline in the 1960s and by the late 1980s, substantial progress had been made by major land managers worldwide.
Generally, predictive modeling in archaeology is establishing statistically valid causal or covariable relationships between natural proxies such as soil types, elevation, slope, vegetation, proximity to water, geology, geomorphology, etc., and the presence of archaeological features. Through analysis of these quantifiable attributes from land that has undergone archaeological survey, sometimes the “archaeological sensitivity” of unsurveyed areas can be anticipated based on the natural proxies in those areas. Large land managers in the United States, such as the Bureau of Land Management (BLM), the Department of Defense (DOD) (Altschul, Sebastian, & Heidelberg, 2004), and numerous highway and parks agencies, have successfully employed this strategy. By using predictive modeling in their cultural resource management plans, they are capable of making more informed decisions when planning for activities that have the potential to require ground disturbance and subsequently affect archaeological sites.
Customer relationship management
Predictive modeling is used extensively in analytical customer relationship management and data mining to produce customer-level models that describe the likelihood that a customer will take a particular action. The actions are usually sales, marketing and customer retention related.
For example, a large consumer organization such as a mobile telecommunications operator will have a set of predictive models for product cross-sell, product deep-sell and churn. It is also now more common for such an organization to have a model of “savability” using an uplift model. This predicts the likelihood that a customer can be saved at the end of a contract period (the change in churn probability) as opposed to the standard churn prediction model.
Auto insurance
Predictive Modeling is utilized in vehicle insurance to assign risk of incidents to policy holders from information obtained from policy holders. This is extensively employed in usage-based insurance solutions where predictive models utilize telemetry based data to build a model of predictive risk for claim likelihood. Black-box auto insurance predictive models utilize GPS or accelerometer sensor input only. Some models include a wide range of predictive input beyond basic telemetry including advanced driving behavior, independent crash records, road history, and user profiles to provide improved risk models. 
Health care
In 2009 Parkland Health & Hospital System began analyzing electronic medical records in order to use predictive modeling to help identify patients at high risk of readmission. Initially the hospital focused on patients with congestive heart failure, but the program has expanded to include patients with diabetes, acute myocardial infarction, and pneumonia.
Notable failures of predictive modeling
Although not widely discussed by the mainstream predictive modeling community, predictive modeling is a methodology that has been widely used in the financial industry in the past and some of the spectacular failures have contributed to the financial crisis of 2008. These failures exemplify the danger of relying blindly on models that are essentially backforward looking in nature. The following examples are by no mean a complete list:
1) Bond rating. S&P, Moody’s and Fitch quantify the probability of default of bonds with discrete variables called rating. The rating can take on discrete values from AAA down to D. The rating is a predictor of the risk of default based on a variety of variables associated with the borrower and macro-economic data that are drawn from historicals. The rating agencies failed spectacularly with their ratings on the 600 billion USD mortgage backed CDO market. Almost the entire AAA sector (and the super-AAA sector, a new rating the rating agencies provided to represent super safe investment) of the CDO market defaulted or severely downgraded during 2008, many of which obtained their ratings less than just a year ago.
2) Statistical models that attempt to predict equity market prices based on historical data. So far, no such model is considered to consistently make correct predictions over the long term. One particularly memorable failure is that of Long Term Capital Management, a fund that hired highly qualified analysts, including a Nobel Prize winner in economics, to develop a sophisticated statistical model that predicted the price spreads between different securities. The models produced impressive profits until a spectacular debacle that caused the then Federal Reserve chairman Alan Greenspan to step in to broker a rescue plan by the Wall Street broker dealers in order to prevent a meltdown of the bond market. 
Possible fundamental limitations of predictive model based on data fitting
1) History cannot always predict future: using relations derived from historical data to predict the future implicitly assumes there are certain steady-state conditions or constants in the complex system. This is almost always wrong when the system involves people.
2) The issue of unknown unknowns: in all data collection, the collector first defines the set of variables for which data is collected. However, no matter how extensive the collector considers his selection of the variables, there is always the possibility of new variables that have not been considered or even defined, yet critical to the outcome.
3) Self-defeat of an algorithm: after an algorithm becomes an accepted standard of measurement, it can be taken advantage of by people who understand the algorithm and have the incentive to fool or manipulate the outcome. This is what happened to the CDO rating. The CDO dealers actively fulfilled the rating agencies input to reach an AAA or super-AAA on the CDO they are issuing by cleverly manipulating variables that were “unknown” to the rating agencies’ “sophisticated” models.
Software
Throughout the main chapters (3-16) we will give examples of software packages that have the functionality to perform the modeling discussed in a chapter. The “main” software packages are discussed here, due to the expanse of functionality. Software packages built specifically for a functionality, like Support Vector Machines, will be addressed further in the chapters. We will not address software for Uplift models and Time Series models, since they are applications of methods discussed in the main chapters.
Open Source
DAP – GNU Dap is a statistics and graphics program, that performs data management, analysis, and graphical visualization tasks which are commonly required in statistical consulting practice. Dap was written to be a free replacement for SAS, but users are assumed to have a basic familiarity with the C programming language in order to permit greater flexibility. Unlike R it has been designed to be used on large data sets.
KNIME – GNU KNIME, the Konstanz Information Miner, is an open source data analytics, reporting and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining concept. A graphical user interface allows assembly of nodes for data preprocessing (ETL: Extraction, Transformation, Loading), for modeling and data analysis and visualization.
Octave – GNU Octave is a high-level programming language, primarily intended for numerical computations. It provides a command-line interface for solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with MATLAB. WE have used Octave extensively for predictive modeling.
Orange – GNU Orange is a component-based data mining and machine learning software suite, featuring a visual programming front-end for explorative data analysis and visualization, and Python bindings and libraries for scripting. It includes a set of components for data preprocessing, feature scoring and filtering, modeling, model evaluation, and exploration techniques. It is implemented in C++ and Python.
PNL (Probabilistic Networks Library) – is a tool for working with graphical models, supporting directed and undirected models, discrete and continuous variables, various inference and learning algorithms. 
R – GNU R is an open source software environment for statistical computing. It uses “packages”, which are loaded with commands in a console, to provide modeling functionality. As mentioned, all the computer examples in the book are implementations of R packages. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. R is an implementation of the S programming language. We have used R extensively for predictive modeling, using fairly large data sets, but not greater than 900,000 records.
scikit-learn – GNU scikit-learn (formerly scikits.learn) is an open source machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, logistic regression, naive Bayes, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.
Weka – GNU Weka (Waikato Environment for Knowledge Analysis) is a popular suite of machine learning software written in Java, developed at the University of Waikato, New Zealand. Weka is free software available under the GNU General Public License.
Commercial
Analytica – by Lumina Decision Systems, is an influence diagram-based, visual environment for creating and analyzing probabilistic models. 
IBM SPSS Modeler – is a data mining and text analytics software application built by IBM. It is used to build predictive models and conduct other analytic tasks. It has a visual interface which allows users to leverage statistical and data mining algorithms without programming. We will refer to it later as SPSS Modeler.
MATLAB – (matrix laboratory) is a multi-paradigm numerical computing environment and fourth-generation programming language. Developed by MathWorks, MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages, including C, C++, Java, and Fortran. An additional package, Simulink, adds graphical multi-domain simulation and Model-Based Design for dynamic and embedded systems. We have used MATLAB and Simulink extensively for predictive modeling.
RapidMiner – is a software platform developed by the company of the same name that provides an integrated environment for machine learning, data mining, text mining, predictive analytics and business analytics.
SAS Enterprise Miner – SAS (Statistical Analysis System) is a software suite developed by SAS Institute for advanced analytics, business intelligence, data management, and predictive analytics. Enterprise Miner, as the name suggests, is the SAS data mining and modeling tool. 
STATISTICA – is a statistics and analytics software package developed by StatSoft. STATISTICA provides data analysis, data management, statistics, data mining, and data visualization procedures.
Introduction to R
This introduction serves as background material for our examples with R. The only hardware requirement for is a PC with the latest free open source R software installed. R has extensive documentation and active online community support. It is the perfect environment to get started in predictive modeling. 
Installation. R can be downloaded from one of the mirror sites in http://cran.r-project.org/mirrors.html. You should pick your nearest location. 
Using External Data. R offers plenty of options for loading external data, including Excel, Minitab and SPSS files. 
R Session. After R is started, there is a console awaiting for input. At the prompt (>), you can enter numbers and perform calculations. 
> 1 + 2 
[1] 3 

Variable Assignment. We assign values to variables with the assignment operator “=“. Just typing the variable by itself at the prompt will print out the value. We should note that another form of assignment operator “<-” is also in use. 
> x = 1 
> x 
[1] 1 

Functions. R functions are invoked by its name, then followed by the parenthesis, and zero or more arguments. The following apply the function c to combine three numeric values into a vector. 
> c(1, 2, 3) 
[1] 1 2 3 

Model Assignments. We can assign a model (function) to a name (placeholder) “<-“ before the function definition. The following assigns a general linear model (glm) to “fit1”.
fit1 <- glm(cbind(status,1-status) ~ rx, family=binomial, data = rats)

Comments. All text after the pound sign “#” within the same line is considered a comment. 
> 1 + 1      # this is a comment 
[1] 2 

Extension Package. Sometimes we need additional functionality beyond those offered by the core R library. In order to install an extension package, you should invoke the install.packages function at the prompt and follow the instruction. 
> install.packages() 

Getting Help. R provides extensive documentation. For example, entering ?c or help(c) at the prompt gives documentation of the function c in R. 
> help(c) 

?
If you are not sure about the name of the function you are looking for, you can perform a fuzzy search with the apropos function. 
> apropos(“nova”) 
[1] “anova”                “anova.glm” 
   .... 

Finally, there is an R specific Internet search engine at http://www.rseek.org for more assistance. 
?
Exercises
	List at least three R packages that could be used for propensity modeling.
	Using the riskfactor data from Chapter 3, describe how we might model the propensity of a driver to have an incident.
	Research: Describe a case of a company that used an uplift model to solve a business problem.
?
Modeling Techniques
Introduction
There are many ways to construct the models that we just discussed. Propensity models are often constructed using statistical techniques and machine learning. Cluster models may also use statistical techniques, but may also employ heuristic algorithms.
Models start with a requirement for building a model, often expressed as a modeling objective. This objective answers the question: What are we measuring with the model? An example is: What is the propensity to purchase an automobile Insurance policy?
Model requirements arise form a business problem or business case. The business case states the problem the company faces, the requirements for a solution, and the metrics to measure success. An example is: People are shedding life insurance policies and ABC company wants to know what factors are causing product shedding.
The target outcome is based on the business case and might be expressed as a question or statement. It identifies the overarching metric of the probable. For instance, what is the probability that a customer will shed a product.
The target variable (dependent variable) is a data element that can be used to arrive at the target outcome. It is sometimes a single variable, such as the binary variable indicating shedding or not shedding. It may also be comprised of a number or variables. For instance, suppose we are concerned with engagement in a service. Engagement might be defined as: a car shopper customer visits our website AND clicks on Get a Quote OR clicks on Find and Dealer OR clicks on Get and Offer.
The type of model we employ to solve a problem should stem from a thorough analysis of the business case and modeling objective, as well as the target outcome. Understanding the business case is of paramount importance. Even though we may be experts at model building, we can deliver the “wrong” model if we do not fully understand the business case.
Statistical Modeling
Nearly any regression model (linear, logistic, general linear model (GLM), robust regression, etc.) can be used for prediction purposes. We will cover all of these in the chapters to follow. In particular, logistic regression is a very popular modeling technique for propensity models with a binary (e.g., Yes or No) response (dependent) variable.
Broadly speaking, there are two classes of predictive models: parametric and non-parametric. A third class, semi-parametric models, includes features of both. Parametric models make “specific assumptions with regard to one or more of the population parameters that characterize the underlying distribution(s)” (Sheskin, 2011), while non-parametric regressions make fewer assumptions than their parametric counterparts. These models fall under the class of statistical models (Marascuilo, 1977).
A statistical model is a formalization of relationships between variables in the form of mathematical equations. A statistical model describes how one or more random variables are related to one or more other variables. The model is statistical as the variables are not deterministically but stochastically related. In mathematical terms, a statistical model is frequently thought of as a pair (Y,P) where Y is the set of possible observations and P the set of possible probability distributions on Y. It is assumed that there is a distinct element of P which generates the observed data. Statistical inference enables us to make statements about which element(s) of this set are likely to be the true one.
Most statistical tests can be described in the form of a statistical model. For example, the Student’s t-test for comparing the means of two groups can be formulated as seeing if an estimated parameter in the model is different from 0. Another similarity between tests and models is that there are assumptions involved. Error is assumed to be normally distributed in most models.
Formal definition
A statistical model is a collection of probability distribution functions or probability density functions (collectively referred to as distributions for brevity). A parametric model is a collection of distributions, each of which is indexed by a unique finite-dimensional parameter: P{P_0:???} , where ? is a parameter and ??R^d is the feasible region of parameters, which is a subset of d-dimensional Euclidean space. A statistical model may be used to describe the set of distributions from which one assumes that a particular data set is sampled. For example, if one assumes that data arise from a univariate Gaussian distribution, then one has assumed a Gaussian model
P={P(x;?,?)=1/(?2? ?) "exp" {-1/(2?^2 ) (x-?)^2 }:??R,?>0}.
A non-parametric model is a set of probability distributions with infinite dimensional parameters, and might be written as P={"all distributions" }. A semi-parametric model also has infinite dimensional parameters, but is not dense in the space of distributions. For example, a mixture of Gaussians with one Gaussian at each data point is dense in the space of distributions. Formally, if d is the dimension of the parameter, and n is the number of samples, if d??as n?? and d?n?0 as n??, then the model is semi-parametric.
Model comparison
Models can be compared to each other. This can either be done when you have performed an exploratory data analysis or a confirmatory data analysis. In an exploratory analysis, you formulate all models you can think of, and see which describes your data best. In a confirmatory analysis you test which of your models you have described before the data was collected fits the data best, or test if your only model fits the data. In linear regression analysis you can compare the amount of variance explained by the independent variables, R2, across the different models. In general, you can compare models that are nested by using a Likelihood-ratio test. Nested models are models that can be obtained by restricting a parameter in a more complex model to be zero.
An example
Height and age are probabilistically distributed over humans. They are stochastically related; when you know that a person is of age 10, this influences the chance of this person being 6 feet tall. You could formalize this relationship in a linear regression model of the following form: ?"height" ?_i  = b_0  + b_1 ?"age" ?_i  + ?_i, where b_0 is the intercept, b_1 is a parameter that "age"  is multiplied by to get a prediction of "height" , ? is the error term, and i is the subject. This means that height starts at some value, there is a minimum height when someone is born, and it is predicted by age to some amount. This prediction is not perfect as error is included in the model. This error contains variance that stems from sex and other variables. When "sex"  is included in the model, the error term will become smaller, as you will have a better idea of the chance that a particular 16-year-old is 6 feet tall when you know this 16-year-old is a girl. The model would become ?"height" ?_i  = b_0  + b_1 ?"age" ?_i+ b_2 ?"sex" ?_i+ ?_i, where the variable "sex"  is dichotomous. This model would presumably have a higher R^2. The first model is nested in the second model: the first model is obtained from the second when b_2 is restricted to zero.
Classification
According to the number of the endogenous variables and the number of equations, models can be classified as complete models (the number of equations equal to the number of endogenous variables) and incomplete models. Some other statistical models are the general linear model (restricted to continuous dependent variables), the generalized linear model (for example, logistic regression), the multilevel model, and the structural equation model.
Machine Learning
Machine learning is a scientific discipline that explores the construction and study of algorithms that can learn from data (Kovahi & Provost, 1998). Such algorithms operate by building a model based on inputs (Bishop, Pattern Recognition and Machine Learning, 2006) and using them to make predictions or decisions, rather than following only explicitly programmed instructions.
Machine learning can be considered a subfield of computer science and statistics. It has strong ties to artificial intelligence and optimization, which deliver methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit, rule-based algorithms is infeasible. Example applications include spam filtering, optical character recognition (OCR) (Wernick, Yang, Brankov, Yourganov, & Strother, 2010), search engines and computer vision. Machine learning is sometimes conflated with data mining (Mannila, 1996), although that focuses more on exploratory data analysis (Friedman, Data Mining and Statistics: What's the connection?, 1998). Machine learning and pattern recognition “can be viewed as two facets of the same field.” (Bishop, Pattern Recognition and Machine Learning, 2006)
Types of problems/tasks
 Machine learning tasks are typically classified into three broad categories, depending on the nature of the learning “signal” or “feedback” available to a learning system. These are (Russell & Norvig, 2003):
Supervised learning. The computer is presented with example inputs and their desired outputs, given by a “teacher”, and the goal is to learn a general rule that maps inputs to outputs.
Unsupervised learning. Here, no labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end.
Reinforcement learning. In this instance, a computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle), without a teacher explicitly telling it whether it has come close to its goal or not. Another example is learning to play a game by playing against an opponent (Bishop, Pattern Recognition and Machine Learning, 2006).
Between supervised and unsupervised learning is semi-supervised learning, where the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing. Transduction is a special case of this principle where the entire set of problem instances is known at learning time, except that part of the targets are missing.
Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers, and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.
Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system (Bishop, Pattern Recognition and Machine Learning, 2006):
	In classification, inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one (or multi-label classification) or more of these classes. This is typically tackled in a supervised way. Spam filtering is an example of classification, where the inputs are email (or other) messages and the classes are “spam” and “not spam”.
	In regression, also a supervised problem, the outputs are continuous rather than discrete.
	In clustering, a set of inputs is to be divided into groups. Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.
	Density estimation finds the distribution of inputs in some space.
	Dimensionality reduction simplifies inputs by mapping them into a lower-dimensional space. Topic modeling is a related problem, where a program is given a list of human language documents and is tasked to find out which documents cover similar topics.
Approaches
There are numerous approaches to machine learning. We discuss several of these in the chapters to follow. To these we will devote less narrative here. And describe those not in the book in more detail.
Decision tree learning
Decision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item’s target value.
Association rule learning
Association rule learning is a method for discovering interesting relations between variables in large databases.
Artificial neural networks
An artificial neural network (ANN) learning algorithm, usually called “neural network” (NN), is a learning algorithm that is inspired by the structure and functional aspects of biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.
Support vector machines
Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.
Clustering
Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.
Bayesian networks
A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.
Inductive logic programming
Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.
Reinforcement learning
Reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.
Representation learning
Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing to reconstruct the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.
Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors (Lu, Plataniotis, & Venetsanopoulos, 2011). Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data (Bengio, 2009).
Similarity and metric learning
In this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.
Sparse dictionary learning
In this method, a datum is represented as a linear combination of basis functions, and the coefficients are assumed to be sparse. Let x be a d-dimensional datum, D be a d by n matrix, where each column of D represents a basis function. r is the coefficient to represent x using D. Mathematically, sparse dictionary learning means the following x?Dr  where r is sparse. Generally speaking, n is assumed to be larger than d to allow the freedom for a sparse representation.
Learning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately (Tillmann, 2015). A popular heuristic method for sparse dictionary learning is K-SVD.
Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it’s best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image path can be sparsely represented by an image dictionary, but the noise cannot (Aharon, Elad, & Bruckstein, 2006).
Genetic algorithms
A genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s (Goldberg & Holland, 1988).
Applications
	Applications for machine learning include:
	Machine perception
	Computer vision, including object recognition
	Natural language processing
	Syntactic pattern recognition
	Search engines
	Medical diagnosis
	Bioinformatics
	Brain-machine interfaces
	Cheminformatics
	Detecting credit card fraud
	Stock market analysis
	Sequence mining
	Speech and handwriting recognition
	Game playing
	Adaptive websites
	Computational advertising
	Computational finance
	Structural health monitoring
	Sentiment analysis (or opinion mining)
	Affective computing
	Information retrieval
	Recommender systems
	Optimization and Metaheuristic
In 2006, the online movie company Netflix held the first “Netflix Prize” competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.
In 2010 The Wall Street Journal wrote about a money management firm Rebellion Research’s use of machine learning to predict economic movements, the article talks about Rebellion Research’s prediction of the financial crisis and economic recovery.
In 2014 it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.
Exercises
	Research: Describe the use of any machine learning algorithm by a company or business (not already mentioned in the chapter). At a minimum include a discussion of the business case, modeling objective, target (dependent) variable, and use case.
?

Empirical Bayes method

Empirical Bayes methods are procedures for statistical inference in which the prior distribution is estimated from the data. This approach stands in contrast to standard Bayesian methods, for which the prior distribution is fixed before any data are observed. Despite this difference in perspective, empirical Bayes may be viewed as an approximation to a fully Bayesian treatment of a hierarchical model wherein the parameters at the highest level of the hierarchy are set to their most likely values, instead of being integrated out. Empirical Bayes, also known as maximum marginal likelihood (Bishop, Neural networks for pattern recognition, 2005), represents one approach for setting hyperparameters.
Introduction
Empirical Bayes methods can be seen as an approximation to a fully Bayesian treatment of a hierarchical Bayes model.
In, for example, a two-stage hierarchical Bayes model, observed data y={y_1,y_2,…,y_N } are assumed to be generated from an unobserved set of parameters ?={?_1,?_2,…,?_? } according to a probability distribution. In turn, the parameters ? can be considered samples drawn from a population characterized by hyperparameters ? according to a probability distribution p(???). In the hierarchical Bayes model, though not in the empirical Bayes approximation, the hyperparameters ? are considered to be drawn from an unparameterized distribution   p(???).
Information about a particular quantity of interest ?_i therefore comes not only from the properties of those data which directly depend on it, but also from the properties of the population of parameters ? as a whole, inferred from the data as a whole, summarized by the hyperparameters ?.
Using Bayes’ theorem,

p(??y)=(p(y??)p(?))/p(y) =(p(y??)p)/p(y)  ???p(???)p(?)d?.?
In general, this integral will not be tractable analytically and must be evaluated by numerical methods. Stochastic approximations using, e.g., Markov Chain Monte Carlo sampling or deterministic approximations such as quadrature are common. 
Alternatively, the expression can be written as
p(??y)=???p(???,y)p(??|y?)d?=? ???(p(y??)p(??|??))/p(y?|??)  p(? ??|y)d??,
and the term in the integral can in turn be expressed as
p(??y)=???p(???)p(??|y?)d?.?
These suggest an iterative scheme, qualitatively similar in structure to a Gibbs sampler, to evolve successively improved approximations to p(??|y?) and p(? ??|y). First, we calculate an initial approximation to p(??|y?) ignoring the ? dependence completely; then we calculate an approximation to p(? ??|y) based upon the initial approximate distribution of p(??|y?); then we use this p(? ??|y) to update the approximation for p(??|y?); then we update p(? ??|y); and so on.
When the true distribution p(? ??|y) is sharply peaked, the integral determining  p(??|y?) may not be changed much by replacing the probability distribution over ? with a point estimate ?^* representing the distribution’s peak (or, alternatively, its mean),
p(??y)?(p(y??)p(??|?^* ?))/p(y?|?^* ?) 
With this approximation, the above iterative scheme becomes the EM algorithm.
The term “Empirical Bayes“ can cover a wide variety of methods, but most can be regarded as an early truncation of either the above scheme or something quite like it. Point estimates, rather than the whole distribution, are typically used for the parameter(s) ?. The estimates for ?^* are typically made from the first approximation to p(??y) without subsequent refinement. These estimates for ?^* are usually made without considering an appropriate prior distribution for ?.
Point estimation
Robbins method: non-parametric empirical Bayes (NPEB)
Robbins (Robbins, 1956) considered a case of sampling from a compound distribution, where probability for each y_i (conditional on ?_i) is specified by a Poisson distribution,
p(y_i??_i )=(?_i^(y_i ) e^(?-??_i ))/(y_i !),
while the prior is unspecified except that it is also i.i.d. from an unknown distribution, with cumulative distribution function G(?). Compound sampling arises in a variety of statistical estimation problems, such as accident rates and clinical trials. We simply seek a point prediction of ?_i given all the observed data. Because the prior is unspecified, we seek to do this without knowledge of G (Carlin & Louis, 2000).
Under mean squared error loss (SEL), the conditional expectation E(?_i |Y_i) is a reasonable quantity to use for prediction (Nikulin, 2001). For the Poisson compound sampling model, this quantity is
E(?_i?y_i )=(??((?^(y+1) e^(-?))?(y_i !))dG(?) )/(??((?^y e^(-?))?(y_i !))  dG(?) ).
This can be simplified by multiplying the expression by ((y_i+1)?(y_i+1)), yielding
E(?_i?y_i )=((y_i+1) p_G (y_i+1))/(p_G (y_i ) ),
where p_G is the marginal distribution obtained by integrating out ? over G (Wald, 1971).
To take advantage of this, Robbins (Robbins, 1956) suggested estimating the marginals with their empirical frequencies, yielding the fully non-parametric estimate as:
E(?_i?y_i )=(y_i+1)  (#{?Y_j=y?_i+1})/(#{?Y_j=y?_i } ),
where # denotes “number of” (Good, 1953). 
Example - Accident rates
Suppose each customer of an insurance company has an “accident rate” ? and is insured against accidents; the probability distribution of ? is the underlying distribution, and is unknown. The number of accidents suffered by each customer in a specified time period has a Poisson distribution with expected value equal to the particular customer’s accident rate. The actual number of accidents experienced by a customer is the observable quantity. A crude way to estimate the underlying probability distribution of the accident rate ? is to estimate the proportion of members of the whole population suffering 0,1,2,3,… accidents during the specified time period as the corresponding proportion in the observed random sample. Having done so, we then desire to predict the accident rate of each customer in the sample. As above, one may use the conditional expected value of the accident rate ? given the observed number of accidents during the baseline period. Thus, if a customer suffers six accidents during the baseline period, that customer’s estimated accident rate is 
7 × [the proportion of the sample who suffered 7 accidents] / [the proportion of the sample who suffered 6 accidents].
Note that if the proportion of people suffering k accidents is a decreasing function of k, the customer’s predicted accident rate will often be lower than their observed number of accidents. This shrinkage effect is typical of empirical Bayes analyses.
Parametric empirical Bayes
If the likelihood and its prior take on simple parametric forms (such as 1- or 2-dimensional likelihood functions with simple conjugate priors), then the empirical Bayes problem is only to estimate the marginal m(y??) and the hyperparameters ? using the complete set of empirical measurements. For example, one common approach, called parametric empirical Bayes point estimation, is to approximate the marginal using the maximum likelihood estimate (MLE), or a Moments expansion, which allows one to express the hyperparameters ? in terms of the empirical mean and variance. This simplified marginal allows one to plug in the empirical averages into a point estimate for the prior ?. The resulting equation for the prior ? is greatly simplified, as shown below.
There are several common parametric empirical Bayes models, including the Poisson–gamma model (below), the Beta-binomial model, the Gaussian–Gaussian model, the Dirichlet-multinomial model (Johnson, Kotz, & Kemp, 1992), as well specific models for Bayesian linear regression (see below) and Bayesian multivariate linear regression. More advanced approaches include hierarchical Bayes models and Bayesian mixture models.
Poisson–gamma model
For example, in the example above, let the likelihood be a Poisson distribution, and let the prior now be specified by the conjugate prior, which is a gamma distribution (G(?,?)) (where ?=(?,?)):
?(???,?)=(?^(?-1) e^((-?)??))/(?^? ?(?) ) ",for" ?>0,?>0,?>0.
It is straightforward to show the posterior is also a gamma distribution. Write
?(??y)??(y??)?(???,?),
where the marginal distribution has been omitted since it does not depend explicitly on ?. Expanding terms which do depend on ? gives the posterior as:
?(??y)?(?^y e^(-?) )(?^(?-1) e^((-?)??) )=?^(y+?-1) e^(-?((1+1)??) ).
So the posterior density is also a gamma distribution G(?',?'), where ?^'=y+?, and ?^'=((1+1)??)^(-1). Also notice that the marginal is simply the integral of the posterior over all ?, which turns out to be a negative binomial distribution.
To apply empirical Bayes, we will approximate the marginal using the maximum likelihood estimate (MLE). However, since the posterior is a gamma distribution, the MLE of the marginal turns out to be just the mean of the posterior, which is the point estimate E(??y) we need. Recalling that the mean ? of a gamma distribution G(?',?') is simply ?'?', we have
E(??y)=?^' ?^'=(y ?+?)/(1+1??)=?/(1+?) y ?+1/(1+?) (??).
To obtain the values of ? and ?, empirical Bayes prescribes estimating mean ?? and variance ????^2 using the complete set of empirical data.
The resulting point estimate E(??y) is therefore like a weighted average of the sample mean y ? and the prior mean ?=??. This turns out to be a general feature of empirical Bayes; the point estimates for the prior (i.e., mean) will look like a weighted averages of the sample estimate and the prior estimate (likewise for estimates of the variance).
Bayesian Linear Regression
Bayesian linear regression is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference. When the regression model has errors that have a normal distribution, and if a particular form of prior distribution is assumed, explicit results are available for the posterior probability distributions of the model’s parameters.
Consider a standard linear regression problem, in which for i=1,...,n. We specify the conditional distribution of y_i given a k×1 predictor vector x_i:
y_i=X_i^T ?+?_i,
where ? is a k×1 vector, and the ?_i are independent and identical normally distributed random variables:
?_i~N(?,?^2 ).
This corresponds to the following likelihood function:
?(y?X,?,?^2 )?(?^2 )^(-n?2)  exp??(-1/(2?^2 ) (y-X?)^T (y-X?)).?
The ordinary least squares solution is to estimate the coefficient vector using the Moore-Penrose pseudoinverse (Penrose, 1955) (Ben-Israel & Greville, 2003):
? ?=(X^T X)^(-1) X^T y,
Where X is the n×k design matrix, each row of which is a predictor vector X_i^T; and y is the column n-vector [y_1?y_n ]^T. 
This is a “frequentist” approach (Neyman, 1937), and it assumes that there are enough measurements to say something meaningful about ?. In the Bayesian approach, the data are supplemented with additional information in the form of a prior probability distribution. The prior belief about the parameters is combined with the data’s likelihood function according to Bayes theorem to yield the posterior belief about the parameters ? and ?. The prior can take different functional forms depending on the domain and the information that is available a priori.
Software
Several software packages are available that perform Empirical Bayes, including the Open Source software R with the limma package. To start the package in R, one simply enters the following in the R console at the prompt 
> source(“http://bioconductor.org/biocLite.R”)biocLite(“limma”). 

Commercial software includes MATLAB, SAS and SPSS.
Example Using R
Model Selection in Bayesian Linear Regression
Consider data generated by y_i=b_1 x_i+b_3 x_i^3+?_i, and suppose we wish to fit a polynomial of degree 3 to the data. There are then 4 regression coefficients, namely, the intercept and the three coefficients of the power of x. This yields 2^4=16 models possible models for the data. Let b_1=8 and b_3=-0.5 so that the data looks like this in R:
> rm(list=ls())
> x=runif(200,-10,10)
> a=c(18,0,-0.5,0)
> Y=a[1]*x^1+a[2]*x^2+a[3]*x^3+a[4]
> Y=Y+rnorm(length(Y),0,5)
> plot(x,Y)
 
Figure 7-1. Plot of our data
The code to generate the data and calculate the log marginal likelihood for the different models appears below.
> p=4
> X=cbind(x,x^2,x^3,1)
> tf <- c(TRUE, FALSE)
> models <- expand.grid(replicate(p,tf,simplify=FALSE))
> names(models) <- NULL
> models=as.matrix(models)
> models=models[-dim(models)[1],]

> a_0=100
> b_0=0.5
> mu_0=rep(0,p)
> lambda_0=diag(p)

> lml <- function(model){
+     n=length(Y)
+     Y=as.matrix(Y)
+     X=as.matrix(X[,model])
+     mu_0=as.matrix(mu_0[model])
+     lambda_0=as.matrix(lambda_0[model,model])
+         XtX=t(X)%*%X
+     lambda_n=lambda_0 + XtX
+     BMLE=solve(XtX)%*%t(X)%*%Y
+     mu_n=solve(lambda_n)%*%(t(X)%*%Y+lambda_0%*%mu_0)
+     a_n = a_0 + 0.5*n
+     b_n=b_0 + 0.5*(t(Y)%*%Y + t(mu_0)%*%lambda_0%*%mu_0 – 
+     t(mu_n)%*%lambda_n%*%mu_n)
+     log_mar_lik  <-  -0.5*n*log(2*pi) + 
+     0.5*log(det(lambda_0)) - 0.5*log(det(lambda_n)) + 
+     lgamma(a_n) - lgamma(a_0) + a_0*log(b_0) – 
+     a_n*log(b_n)
+     return(mle)
+     }

> lml.all=apply(models,1,lml)
> results=cbind(lml.all, models)
> order=sort(results[,1],index=TRUE,decreasing=TRUE)
> results[order$ix,]

Model Evaluation
The models are listed in order of descending log marginal likelihood below:
            lml x x^2 x^3 c       
 [1,] -1339.085 1   0   1 0
 [2,] -1341.611 1   0   1 1
 [3,] -1345.397 1   1   1 0
 [4,] -1347.116 1   1   1 1
 [5,] -2188.934 0   0   1 0
 [6,] -2190.195 0   0   1 1
 [7,] -2194.238 0   1   1 0
 [8,] -2196.109 0   1   1 1
 [9,] -2393.395 1   0   0 0
[10,] -2395.309 1   0   0 1
[11,] -2399.188 1   1   0 0
[12,] -2401.248 1   1   0 1
[13,] -2477.084 0   0   0 1
[14,] -2480.784 0   1   0 0
[15,] -2483.047 0   1   0 1

> BMLE
          [,1]
x 18.241814068
   0.008942083
  -0.502597759
  -0.398375650

The model with the highest log marginal likelihood is the model which includes x and x^3 only, for which the MLE of the regression coefficients are 18.241814068 and -0.502597759 for x and x^3 respectively. Compare this to how the data was generated.
Example Using Python
Introduction
Bayesian networks are a method for building machine learning models.
	With Bayesian networks, we model our data problem as a causal network, or a story involving hidden and observed variables.
	We come up with a story that we think explains our data.
	We use Bayes’ rule to find posterior probability distributions of the hidden variables given our observed variables (data).
	We use the full posterior distributions for our next action (decision, recommendation, prediction).
Advantages:
	Confidence estimates.
	Flexibility. We can change the story and re-train the model.
	Classical machine learning methods are inflexible: Code and theory only works for its specific problem.
	Your real world data probably has some important Difficult to adapt to your particular problem, which may not have been studied by researchers.
Disadvantages:
	Slow (unless you do a lot of math)
	You have to think. For many of your data problems, you may want or need to fit a custom machine learning model.
Bayesian Statistics
Story: Since the enactment of Colorado Amendment 64 in November 2012, adults aged 21 or older can grow up to six marijuana plants (with no more than half being mature flowering plants) privately in a locked space, legally possess all marijuana from the plants they grow (as long as it stays where it was grown), legally possess up to one ounce of marijuana while traveling, and give as a gift up to one ounce to other citizens 21 years of age or older. Consumption is permitted in a manner similar to alcohol, with equivalent offenses proscribed for driving. Consumption in public remains illegal. Yet, the possession and use of marijuana remains a federal offense and most counties and city governments have made possession and use of marijuana against policy. So, though the possession (one ounce) and use of marijuana are legal in Colorado, city and county employees can be terminated from employment if caught using of test positive in random drug tests.
Suppose an employee of El Paso County Colorado is tested for the use of marijuana. (0.03% of county employees sate-wide use marijuana)
For marijuana positive subjects, test returns positive 99% of the time. (True positive rate.) For marijuana negative subjects, test returns negative 0.1% of the time. (False positive rate.) Does the person use marijuana? We don't know. But we can compute probabilities, given this story, using Bayes' rule.
Let + denote the event the 'test positive', ? denote the event 'test negative', and H denote the event 'marijuana positive'. Then using Bayes’ rule,
p(H?+)=(p(+?H)p(H))/p(+) 
=(p(+?H)P(H))/(p(+?H)p(H)+p(+?H)p(H) )
=(0.99)(0.0003)/((0.99)(0.0003)+(0.001)(0.9997) )
?0.229
Bayesian Statistics, More Formally
To do Bayesian statistics, we need
	A probability model, a story of how the data came to be.
	Hidden variables ?, about which which we want to infer probabilities.
	Observed variables X.
	A prior on our hidden variables p(?): What do we believe before seeing any data?
	A likelihood p(X|?) relating hidden variables to observed variables
Then Bayes' rule ties them together:
p(??X)=(p(X??)p(?))/p(X)  
p(??X)=(p(X??)P(?))/(?p(X??)p(?)d?) 
We have N users and M items. Given a small samples of ratings r_ij, predict r ?_i^' j^' for pairs (i',j') we don't observe.
The dataset we will use is MovieLens. This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100234 ratings and 2213 tag applications across 8927 movies. These data were created by 718 users between March 26, 1996 and August 05, 2015. This dataset was generated on August 06, 2015. The dataset can be downloaded at http://mlcomp.org/datasets/736. The column headings are userId, movieId, rating, timestamp. We have sampled the dataset to make a training set of 190 training ratings and a test set of 91 test ratings, with N=6 and M=91
Loading Packages
There are several packages and function that need to be loaded before we proceed. These include Numpy (NumPy.org, 2016), Panda (Panda.org, 2011), SciPy (SciPy.org, 2016), Matplotlib (Whitaker, 2011), and Sklearn (scikit-learn.org, 2016), which we have used previously. We will also use Pymc, the Markov Chain Monte Carlo sampling toolkit; daft, a package that uses matplotlib to render pixel-perfect probabilistic graphical models for publication in a journal or on the internet; and html, a package to construct HTML start with an instance of html.HTML().
In [1]:
import numpy as np
import pandas as pd
import scipy as sp
import pymc
import html
import daft
import sklearn as skimport
import html
import csv
import matplotlib.pyplot as plt
from scipy.sparse import coo_matrix
from html import HTML
#from sklearn.decomposition import ProjectedGradientNMF
from itertools import product, izip

Next we define and load the datasets that we will use for this example. Recall we are using MovieLens, which we partitioned into a training set and a test set.
In [2]:
def loadData(filename):
    data = np.loadtxt(filename)
    users = data[:,0].astype(int)
    items = data[:,1].astype(int)
    ratings = data[:,2]
    
    nExamples = len(users)
    
    return (nExamples, users, items, ratings)

TRAIN_FILE = 'C:/Users/Strickland/Documents/Python Scripts/736/train'
TEST_FILE  = 'C:/Users/Strickland/Documents/Python Scripts/736/test'

(nTrain, users, items, ratings) = loadData(TRAIN_FILE)
(nTest,  testUsers, testItems, testRatings) = loadData(TEST_FILE)

N = np.max(users)
M = np.max(items)

Now, we check to make sure the test set has the correct dimensions of M=91 and N=6.
In [3]:
N
Out[3]:
6
In [4]:
M
Out[4]:
91

Next, we plot histograms for the training ratings and test ratings.
In [5]:
%matplotlib notebook 
plt.figure()
plt.hist(ratings)
plt.figure()
plt.hist(testRatings)
 
Figure 7-2. Histogram of training ratings
 
Figure 7-3. Histogram of test ratings
Out[5]:
(array([ 47.,  18.,   9.,   2.,   0.,   3.,   1.,   0.,   0.,   1.]),
 array([ 0.75432862,  1.2940267 ,  1.83372478,  2.37342286,  2.91312094,
         3.45281902,  3.9925171 ,  4.53221518,  5.07191326,  5.61161134,
         6.15130942]),
 <a list of 10 Patch objects>)

From the test set, it appears like we have 1-6 stars, but something weird is happening in the training set! The histogram says there are outliers, so let's take a look:
In [6]:
outlierIdxs = ratings > 6
print np.flatnonzero(outlierIdxs)
print ratings[outlierIdxs]
[ 83 163 165 167 168]
[    9.54361436  1379.98131862     8.76865101  2907.19215935  1380.47446215]

If we delete the outliers from our data file, the the result is much better. When doing this, always make a new copy; do not overwrite your original data! Now, let's plot the "clean" data.
In [7]:
%matplotlib notebook 
CLEAN_TRAIN_FILE = 'C:/Users/Strickland/Documents/Python Scripts/736/train'
(nTrain, users, items, ratings)=loadData(CLEAN_TRAIN_FILE)

plt.hist(ratings)
 
Figure 7-4. Histogram of training ratings with outliers removed

Out[7]:
(array([ 187.,    0.,    0.,    0.,    2.,    0.,    0.,    0.,    0.,    1.]),
 array([  7.57225234e-01,   2.91400719e+02,   5.82044212e+02,
          8.72687705e+02,   1.16333120e+03,   1.45397469e+03,
          1.74461819e+03,   2.03526168e+03,   2.32590517e+03,
          2.61654867e+03,   2.90719216e+03]),
 <a list of 10 Patch objects>)

Now that we've taken a look at our data, let's tell a story.
The classic matrix factorization method for recommendations represents users and items as d-dimensional vectors. Each dimension represents some abstract "taste": horror? chick flick? comedy?
A rating r_ij is just the dot product of the respective user and item vectors: u_i^? v_j.
In Bayesian notation,
	For user i=1,…,N, draw u_i?N(0,?_0 I_d ) 
	For item j=1,…,M draw v_j?N(0,?_0 I_d ) 
	For i=1,…,N  and j=1,…,M,
	Compute z_ij=u_i^? v_(j )
	Draw r_ij?LN(z_ij,?_1 ).
The Bayesian network looks like the following:
In [8]:
from matplotlib import rc

# rc settings are used to customize matplotlib defaults
rc("font", family="serif", size=36)
rc("text", usetex=True)

# All daft scripts will start with the creation of a PGM object.
# This object contains a list of Node objects and Edge objects connecting them.
pgm = daft.PGM([5, 4], grid_unit=5, node_unit=3)
pgm.add_node(daft.Node("tau0", r"$\tau_0$",    2, 3.5, fixed=True))
pgm.add_node(daft.Node("u", r"$\mathbf{u}_i$", 1, 2.5))
pgm.add_node(daft.Node("v", r"$\mathbf{v}_j$", 3, 2.5))

pgm.add_node(daft.Node("z", r"$z_{ij}$", 2, 2.5))
pgm.add_node(daft.Node("r", r"$r_{ij}$", 2, 1.5, observed=True))

pgm.add_node(daft.Node("tau1", r"$\tau_1$", 0.25, 1.5, fixed=True))

pgm.add_edge("tau0", "u")
pgm.add_edge("tau0", "v")
pgm.add_edge("u", "z")
pgm.add_edge("v", "z")
pgm.add_edge("z", "r")
pgm.add_edge("tau1", "r")

# [start_x, start_y, xlen, ylen]
pgm.add_plate(daft.Plate([0.5, 0.5, 2, 2.5], label=r"$i = 1, \ldots, N$"))
pgm.add_plate(daft.Plate([1.5, 0.25, 2, 2.7], shift=0.5, label=r"$j = 1, \ldots, M$"))

pgm.render()

Out[8]:
 
Figure 7-5. Our Bayesian network
Finding hyperparameters is the unprincipled black magic of machine learning. I basically tried some numbers until they worked.
In [9]:
D    = 10
tau0 = 5
tau1 = 5
In [10]:
u = pymc.Normal('u', 0, tau0, size=(D, N))
v = pymc.Normal('v', 0, tau0, size=(D, M))

z = pymc.Lambda('z', lambda u=u, v=v: np.dot(u.T, v))
r = np.empty(nTrain, dtype=object)

for n, (i, j) in enumerate(izip(users, items)):
    r[n] = pymc.Lognormal('x_%d_%d' % (i, j),
                           z[i-1, j-1],    # mean
                           tau1,           # precision (confidence)
                           observed=True,
                           value=ratings[n])

model = pymc.Model([u, v, z, r])

Posterior Inference and Markov Chain Monte Carlo
In [11]:
# There was a bug in PyMC; this is just to hack around code.
if 3 in model.__dict__:
    del model.__dict__[3]
    
pymc.MAP(model).fit()
mcmc = pymc.MCMC(model)
mcmc.sample(10000)
 [-----------------100%-----------------] 10000 of 10000 complete in 44.1 sec

Joint probability:
p?((z_ij)?_(ij?I) ?(u_i)?_(i=1)^N,?(v_j)?_(j=1)^M,?_0,?_1=?_(ij?I)??LN(r_ij?u_i^?  v_i,?_1 ) ? ?_(i=I)^N?N(u_i,?_0 I_d )  ?_(i=I)^M?N(v_j,?_0 I_d ) 
Posterior probabilities:
p?((z_ij)?_(ij?I) ?(u_i)?_(i=1)^N,?(v_j)?_(j=1)^M,?_0,?_1 |?(r_ij)?_(ij?I)=(?_(ij?I)??LN(r_ij?u_i^?  v_i,?_1 ) ? ?_(i=I)^N?N(u_i,?_0 I_d )  ?_(i=I)^M?N(v_j,?_0 I_d ) )/(??_(ij?I)??LN(r_ij?u_i^?  v_i,?_1 ) ? ?_(i=I)^N?N(u_i,?_0 I_d )  ?_(i=I)^M?N(v_j,?_0 I_d ) )dzdudv)
Posterior probabilities are the cornerstone of Bayesian statistics. They give us information like
	Predicted ratings, r_ij
	Error bars on predictions, ?(V[r_ij ] )
	Shape of latent representations of movies and users, p(u_i?…),p(v_i?…)
Markov chain Monte Carlo: (MCMC) Do not evaluate the posterior explicitly. Construct a Markov chain whose invariant distribution is the posterior.
In fact, with MCMC we never know how to write down the posterior. Eventually, our Markov chain generates samples from it.
	Markov chain: a process whose future depends only on the present, not on its own history
	Computationally easy to simulate.
	Analytically, rich theory, which justifies the method.
Gibbs sampling: The easiest MCMC method.
	Let x_1,…,x_n be random variables, and y be observed.
	For t=1,2,…: 
	For i=1,…,n 
	Sample p(x_i?x_1,…,x_(i-1),x_(i+1),x_i,y) 
Properties:
	Each inner loop generates a vector x(t)_(1:n)^t. These vectors form a Markov chain.
	At the invariant distribution (when you run then chain for "long enough"), the Markov chain generates samples from the conditional p(x|y).
	For any subset of variables x_i,x_j,x_k, keeping just those variables from the Markov chain is a sample from the marginal distribution p(x_i,x_j,x_k |y).
Posterior Analysis
In [12]:
zsamples = mcmc.trace('z')[5000:]
zmean    = np.mean(zsamples, 0)

meanExpZ = np.mean(np.exp(zsamples), 0)

We learned a distribution over all log-ratings z_ij. Let's take a look at the first rating. The shape of the histogram captures our knowledge, including knowledge of uncertainty.
In [13]:
%matplotlib notebook 
plt.hist(np.exp(zsamples[:,1,1]))
 
Figure 7-6. learned a distribution over all log-ratings z_ij
iFirst, jFirst, rFirst = users[1], items[1], ratings[1]
print meanExpZ[iFirst,jFirst]
print rFirst

1.00000058515
1.5098059294

Accuracy Validation
Let's output our predicted ratings as the mean rating. Let I be the set of item-user pairs rated, with true ratings r_ij. Recall z_ij is the logarithm of the rating. The standard error metric is root-mean squared error (RMSE):
RMSE(I)=?(1/|I|  ?_(i,j?I)?? (expz_ij-r_ij )^2 ?)
Let us calculate the RMSE for both training and test sets. In general, if the RMSE is way better for training set, then we've overfit.
In [14]:
#trainRMSE = np.sqrt(np.mean((np.exp(rmean[users-1, items-1]) - ratings) ** 2))
#testRMSE  = np.sqrt(np.mean((np.exp(rmean[testUsers-1, testItems-1]) - testRatings) ** 2))
trainRMSE = np.sqrt(np.mean( (meanExpZ[users-1, items-1] - ratings) ** 2))
testRMSE  = np.sqrt(np.mean( (meanExpZ[testUsers-1, testItems-1] - testRatings) ** 2))

print trainRMSE
print testRMSE
104.21762407
4.26955005761

The Competitor, Nonnegative Matrix Factorization
Again, I experimented with the sparsity (beta0) and these were the best results I saw. Note exactly a fair comparison, since the NMF code is far better vectorized.
In [15]:
# Non-Negative matrix factorization by Projected Gradient (NMF)
from sklearn.decomposition import ProjectedGradientNMF
In [16]:
R = coo_matrix((ratings, (users - 1, items - 1))).todense()
skm = ProjectedGradientNMF(n_components=D, sparseness='data', beta=70)
W = skm.fit_transform(R.T)

reconstruct = np.dot(skm.components_.T, W.T)

nmfTrainRMSE  = np.sqrt(np.mean( (reconstruct[users-1, items-1] - ratings) ** 2))
nmfTestRMSE   = np.sqrt(np.mean( (reconstruct[testUsers-1, testItems-1] - testRatings) ** 2))

print nmfTrainRMSE
print nmfTestRMSE
1.54022321501
1.47762626729
?
Example Using SAS Studio
The NLMIXED procedure fits nonlinear mixed models—that is, models in which both fixed and random effects enter nonlinearly. These models have a wide variety of applications, two of the most common being pharmacokinetics and overdispersed binomial data (as is our bank data in this example). This enables us to use the estimated model to construct predictions of arbitrary functions by using empirical Bayes estimates of the random effects.
Code
PROC NLMIXED DATA=work.bank
METHOD=gauss noad noadscale
/*Start Empirical Bayes Options*/
EBSTEPS=40 	/*number of Newton steps*/
EBSUBSTEPS=25 	/*number of substeps*/
EBSSFRAC=0.8 	/*step-shortening fraction*/
EBSSTOL=1E-8 	/*step-shortening tolerance*/
EBTOL=1E-12 	/*convergence tolerance*/
EBOPT 	/*comprehensive optimization*/
EBZSTART 	/*zero starting values*/
/*End Empirical Bayes Options*/
ECORR
ECOV
QTOL=0.005
ALPHA=0.05;
OUTQ=WORK.SCORE_SET;
parms t1=1 t2=1 t3=1 t4=1 t5=1 t6=1 s1=.05 s2=1;
eta = x1*t1 + x2*t2 +x3*t3 +x4*t4 + x5*t5 + x6*t6 + alpha;
p = probnorm(eta);
model resp ~ binomial(m,p);
random alpha ~ normal(0,x1*s1*s1*s3+x2*s2*s2) subject=contact;
estimate 'gamma2' t2/sqrt(1+s2*s2);
predict p out=p;
RUN;
?
Output
The “Specifications” table provides basic information about the nonlinear mixed model (Figure 7-7). For example, the distribution of the response variable, conditional on normally distributed random effects, is binomial. The "Dimensions" table provides counts of various variables. You should check this table to make sure the data set and model have been entered properly. PROC NLMIXED selects five quadrature points to achieve the default accuracy in the likelihood calculations.
Specifications
Data Set	WORK.BANK
Dependent Variable	RESP
Distribution for Dependent Variable	Binomial
Random Effects	alpha
Distribution for Random Effects	Normal
Subject Variable	contact
Optimization Technique	Dual Quasi-Newton
Integration Method	Gaussian Quadrature

Dimensions
Observations Used	22533
Observations Not Used	0
Total Observations	22533
Subjects	3
Max Obs per Subject	14527
Parameters	17
Quadrature Points	41
Figure 7-7. Model Information and Dimensions for Logistic-Normal Model
The “Parameters” table lists the starting point of the optimization and the negative log likelihood at the starting values (Figure 7-8).
Initial Parameters
t1	t2	t3	t4	t5	t6	s1	s2	x1	x2	x3	x4	x5	x6	m	s3	Negative
Log
Likelihood
1	1	1	1	1	1	0.05	1	1	1	1	1	1	1	1	1	8088.22274
Figure 7-8. Starting Values of Parameter Estimates
The “Iteration History” table indicates successful convergence in seven iterations (Figure 7-9). The “Fit Statistics” table lists some useful statistics based on the maximized value of the log likelihood.
Iteration History
Iteration	Calls	Negative
Log
Likelihood	Difference	Maximum
Gradient	Slope
1	8	7962.3967	125.8261	848.763	-1380456
2	10	7961.2726	1.124029	175.293	-2.01639
3	102	7961.2719	0.000745	175.277	-0.42646
4	115	7961.2719	-909E-15	175.277	-12.4444

NOTE: FCONV convergence criterion satisfied.

Note:	Moore-Penrose inverse is used in covariance matrix.

Fit Statistics
-2 Log Likelihood	15923
AIC (smaller is better)	15957
AICC (smaller is better)	15957
BIC (smaller is better)	15941

Figure 7-9. Iteration History and Fit Statistics for Logistic-Normal Model
The “Parameter Estimates” table indicates marginal significance of the two fixed-effects parameters (Figure 7-10). The positive value of the estimate of  indicates that the treatment significantly increases the chance of a favorable cure.
Parameter Estimates
Parameter	Estimate	Standard
Error	DF	t Value	Pr > |t|	95% Confidence Limits	Gradient
t1	1.0026	0.08601	2	11.66	0.0073	0.6325	1.3727	-15.5402
t2	1.0026	0.3442	2	2.91	0.1004	-0.4786	2.4838	-15.3988
t3	1.0026	0.08475	2	11.83	0.0071	0.6380	1.3672	-15.5406
t4	1.0026	0.08475	2	11.83	0.0071	0.6380	1.3672	-15.5406
t5	1.0026	0.08475	2	11.83	0.0071	0.6380	1.3672	-15.5406
t6	1.0026	0.08475	2	11.83	0.0071	0.6380	1.3672	-15.5406
s1	0.04909	.	2	.	.	.	.	1.71093
s2	0.9818	0.05341	2	18.38	0.0029	0.7520	1.2116	33.9054
x1	1.0026	0.08474	2	11.83	0.0071	0.6380	1.3672	-15.4987
x2	0.9935	0.08301	2	11.97	0.0069	0.6363	1.3506	1.21312
x3	1.0026	0.08475	2	11.83	0.0071	0.6380	1.3672	-15.5406
x4	1.0026	0.08475	2	11.83	0.0071	0.6380	1.3672	-15.5406
x5	1.0026	0.08475	2	11.83	0.0071	0.6380	1.3672	-15.5406
x6	1.0026	0.08475	2	11.83	0.0071	0.6380	1.3672	-15.5406

Figure 7-10. Parameter Estimates for Logistic-Normal Model

The “Additional Estimates” table displays results from the ESTIMATE statement (Figure 7-11). The estimate of  equals  and its standard error equals  by the delta method (Billingsley 1986, Cox 1998). Note that this particular approximation produces a -statistic identical to that for the estimate of .
Additional Estimates
Label	Estimate	Standard
Error	DF	t Value	Pr > |t|	Alpha	Lower	Upper
gamma2	0.7154	0.2064	2	3.47	0.0741	0.05	-0.1726	1.6035
Figure 7-11. Table of Additional Estimates

 
Covariance Matrix of Parameter Estimates
 	t1	t2	t3	t4	t5	t6	s2
t1	0.007397	-0.00359	0.00716	0.00716	0.00716	0.00716	-0.00204
t2	-0.00359	0.1185	-0.00356	-0.00356	-0.00356	-0.00356	0.03548
t3	0.00716	-0.00356	0.007182	0.007182	0.007182	0.007182	-0.00201
t4	0.00716	-0.00356	0.007182	0.007182	0.007182	0.007182	-0.00201
t5	0.00716	-0.00356	0.007182	0.007182	0.007182	0.007182	-0.00201
t6	0.00716	-0.00356	0.007182	0.007182	0.007182	0.007182	-0.00201
s2	-0.00204	0.03548	-0.00201	-0.00201	-0.00201	-0.00201	0.002853
x1	-0.05777	-0.00356	0.007182	0.007182	0.007182	0.007182	-0.00201
x2	0.007014	-0.06768	0.007035	0.007035	0.007035	0.007035	-0.00197
x3	0.00716	-0.00356	-0.05775	0.007182	0.007182	0.007182	-0.00201
x4	0.00716	-0.00356	0.007182	-0.05775	0.007182	0.007182	-0.00201
x5	0.00716	-0.00356	0.007182	0.007182	-0.05775	0.007182	-0.00201
x6	0.00716	-0.00356	0.007182	0.007182	0.007182	-0.05775	-0.00201
Covariance Matrix of Parameter Estimates	
 	x1	x2	x3	x4	x5	x6	
t1	-0.05777	0.007014	0.00716	0.00716	0.00716	0.00716	
t2	-0.00356	-0.06768	-0.00356	-0.00356	-0.00356	-0.00356	
t3	0.007182	0.007035	-0.05775	0.007182	0.007182	0.007182	
t4	0.007182	0.007035	0.007182	-0.05775	0.007182	0.007182	
t5	0.007182	0.007035	0.007182	0.007182	-0.05775	0.007182	
t6	0.007182	0.007035	0.007182	0.007182	0.007182	-0.05775	
s2	-0.00201	-0.00197	-0.00201	-0.00201	-0.00201	-0.00201	
x1	0.007181	0.007035	0.007182	0.007182	0.007182	0.007182	
x2	0.007035	0.006891	0.007035	0.007035	0.007035	0.007035	
x3	0.007182	0.007035	0.007182	0.007182	0.007182	0.007182	
x4	0.007182	0.007035	0.007182	0.007182	0.007182	0.007182	
x5	0.007182	0.007035	0.007182	0.007182	0.007182	0.007182	
x6	0.007182	0.007035	0.007182	0.007182	0.007182	0.007182	
Figure 7-12. Table of covariation
Covariance Matrix of Parameter Estimates
 	t1	t2	t3	t4	t5	t6	s2
t1	1	-0.1214	0.9824	0.9824	0.9824	0.9824	-0.4442
t2	-0.1214	1	-0.1221	-0.1221	-0.1221	-0.1221	1
t3	0.9824	-0.1221	1	1	1	1	-0.4444
t4	0.9824	-0.1221	1	1	1	1	-0.4444
t5	0.9824	-0.1221	1	1	1	1	-0.4444
t6	0.9824	-0.1221	1	1	1	1	-0.4444
s2	-0.4442	1	-0.4444	-0.4444	-0.4444	-0.4444	1
x1	-1	-0.1221	1	1	1	1	-0.4444
x2	0.9824	-1	1	1	1	1	-0.4444
x3	0.9824	-0.1221	-1	1	1	1	-0.4444
x4	0.9824	-0.1221	1	-1	1	1	-0.4444
x5	0.9824	-0.1221	1	1	-1	1	-0.4444
x6	0.9824	-0.1221	1	1	1	-1	-0.4444
Covariance Matrix of Parameter Estimates	
 	x1	x2	x3	x4	x5	x6	
t1	-1	0.9824	0.9824	0.9824	0.9824	0.9824	
t2	-0.1221	-1	-0.1221	-0.1221	-0.1221	-0.1221	
t3	1	1	-1	1	1	1	
t4	1	1	1	-1	1	1	
t5	1	1	1	1	-1	1	
t6	1	1	1	1	1	-1	
s2	-0.4444	-0.4444	-0.4444	-0.4444	-0.4444	-0.4444	
x1	1	1	1	1	1	1	
x2	1	1	1	1	1	1	
x3	1	1	1	1	1	1	
x4	1	1	1	1	1	1	
x5	1	1	1	1	1	1	
x6	1	1	1	1	1	1	
Figure 7-13. Table of correlation coefficients 

Exercises
Exercises may entail some data preparation in your Hadoop environment.
	Using the risk factor data from Chapter 3, fit a Empirical Bayes model to riskfactor using explanatory variables from truck_mileage, driver_mileage, avg_mileage, geolocation and trucks as appropriate.
	Using the Baseball Pitching data from Chapter 3, construct a Empirical Bayes model that predicts a pitcher’s ERA based on pitcher stats (innings pitched, strikeouts, base-on-balls, hits, homeruns, wins, losses).
	Predict the ERA of a pitcher with 12 wins, 5 losses, 500 innings pitched, 20 Strikeouts, 12 bases-on-balls, 200 hits, and 3 homeruns.
	Predict the ERA of a pitcher with 10 wins, 10 losses, 600 innings pitched, 25 Strikeouts, 22 bases-on-balls, 270 hits, and 6 homeruns.
	Use the Baseball Batting data from Chapter 3 and filter out pitchers in Hadoop (usually the weakest hitters). 
	Estimate a prior from all your data
	Use that distribution as a prior for each individual estimate
	Refer to the data set RefineDemoData.zip from Chapter 3. Use the table formed in part (d) by joining the omniture website log data to the CRM data (registered users) and CMS data (products). Construct a model to determine if males or females are more likely to buy products based on website activity.


?
?
Naïve Bayes classifier

In machine learning, Naïve Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes’ theorem with strong (naive) independence assumptions between the features.
Naïve Bayes is a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (such as spam or legitimate, sports or politics, etc.) with word frequencies as the features. With appropriate preprocessing, it is competitive in this domain with more advanced methods including support vector machines (Rennie, Shih, Teevan, & Karger, 2003).
Training Naïve Bayes can be done by evaluating an approximation algorithm in closed form in linear time, rather than by expensive iterative approximation.
Introduction
In simple terms, a Naïve Bayes classifier assumes that the value of a particular feature is unrelated to the presence or absence of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 3” in diameter. A Naïve Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of the presence or absence of the other features.
For some types of probability models, Naïve Bayes classifiers can be trained very efficiently in a supervised learning setting. In many practical applications, parameter estimation for Naïve Bayes models uses the method of maximum likelihood; in other words, one can work with the Naïve Bayes model without accepting Bayesian probability or using any Bayesian methods.
Despite their naive design and apparently oversimplified assumptions, Naïve Bayes classifiers have worked quite well in many complex real-world situations. In 2004, an analysis of the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of Naïve Bayes classifiers (Zhang, 2004). Still, a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches, such as boosted trees or random forests (Caruana & Niculescu-Mizil, 2006).
An advantage of Naïve Bayes is that it only requires a small amount of training data to estimate the parameters (means and variances of the variables) necessary for classification. Because independent variables are assumed, only the variances of the variables for each class need to be determined and not the entire covariance matrix.
Probabilistic model
Abstractly, the probability model for a classifier is a conditional model
p(C?F_1,…,F_n )
over a dependent class variable C with a small number of outcomes or classes, conditional on several feature variables F_1 through F_n. The problem is that if the number of features n is large or if a feature can take on a large number of values, then basing such a model on probability tables is infeasible. We therefore reformulate the model to make it more tractable.
Using Bayes’ theorem, this can be written
p(C?F_1,…,F_n )=(p(C)p(F_1,…,F_n?C))/p(F_1,…,F_n ) .
In plain English, using Bayesian Probability terminology, the above equation can be written as
"posterior="  "prior × likelihood" /"evidence" .
In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on C and the values of the features F_i are given, so that the denominator is effectively constant. The numerator is equivalent to the joint probability model
p(C?F_1,…,F_n ),
which can be rewritten as follows, using the chain rule for repeated applications of the definition of conditional probability:
p(C?F_1,…,F_n )	=p(C)p(F_1,…,F_n?C)
	=p(C)p(F_1?C)p(F_2,…,F_n?C,F_1 )
	=p(C)p(F_1?C)p(F_2?C,F_1 )p(F_3,…,F_n?C,F_(1,) F_2 )
	=p(C)p(F_1?C)p(F_2?C,F_1 )p(F_3?C,F_1,F_2 )
     p(F_4,…,F_n?C,F_(1,) F_2,F_3 )
	=p(C)p(F_1?C)p(F_2?C,F_1 )…p(F_n?C,F_(1,) F_2,F_3,…,F_n )

Now the “naïve” conditional independence assumptions come into play: assume that each feature F_i is conditionally independent of every other feature F_j for i?j given the category C. This means that
p(F_i?C,F_j )=p(F_i?C)
p(F_i?C,F_j,F_k )=p(F_i?C)
p(F_i?C,F_j,F_k,F_l )=p(F_i?C)
and so on, for i?j,k,l. Thus, the joint model can be expressed as
p(C?F_1,…,F_n )?p(C,F_1,…,F_n )
?p(C)p(F_1?C)p(F_2?C)p(F_3?C)?
?p(C) ?_(i-1)^n??p(F_i?C).?
This means that under the above independence assumptions, the conditional distribution over the class variable C is:
p(C?F_1,…,F_n )=1/Z p(C) ?_(i-1)^n??p(F_i?C) ?,
where the evidence Z=p(F_1,…,F_n ) is a scaling factor dependent only on F_1,…,F_n, that is, a constant if the values of the feature variables are known (Rish, 2001).
Constructing a classifier from the probability model
The discussion so far has derived the independent feature model, that is, the Naïve Bayes probability model. The Naïve Bayes classifier combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable; this is known as the maximum a posteriori or MAP decision rule. The corresponding classifier, a Bayes classifier, is the function "classify"  defined as follows:
"classify" (f_1,…,f_n )=argmax?C??p(C=c) ?_(i=1)^n??p(F_i=f_i?C=c) ??
Parameter estimation and event models
All model parameters (i.e., class priors and feature probability distributions) can be approximated with relative frequencies from the training set. These are maximum likelihood estimates of the probabilities. A class’ prior may be calculated by assuming equiprobable classes (i.e., priors = 1 / (number of classes)), or by calculating an estimate for the class probability from the training set (i.e., ("prior for a given class") =  (number of samples in the class) / (total number of samples)). To estimate the parameters for a feature’s distribution, we must assume a distribution or generate nonparametric models for the features from the training set (John & Langley, 1995).
The assumptions on distributions of features are called the event model of the Naïve Bayes classifier. For discrete features like the ones encountered in document classification (include spam filtering), multinomial and Bernoulli distributions are popular. These assumptions lead to two distinct models, which are often confused (McCallum & Nigam, 1998) (Metsis, Androutsopoulos, & Paliouras, 2006).
Gaussian Naïve Bayes
When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a Gaussian distribution. For example, suppose the training data contain a continuous attribute,	x. We first segment the data by the class, and then compute the mean and variance of x in each class. Let ?_c be the mean of the values in	associated with class c, and let ?_c^2 be the variance of the values in associated with class c. Then, the probability density of some value given a class,	P(x=v?c), can be computed by plugging v into the equation for a Normal distribution parameterized by ?_c and  ?_c. That is,
P(x=v?c)=1/?(?2???_c^2 ) e^(-(v-?_c )^2/(?2??_c^2 ))
Another common technique for handling continuous values is to use binning to discretize the feature values, to obtain a new set of Bernoulli-distributed features. In general, the distribution method is a better choice if there is a small amount of training data, or if the precise distribution of the data is known. The discretization method tends to do better if there is a large amount of training data because it will learn to fit the distribution of the data. Since Naïve Bayes is typically used when a large amount of data is available (as more computationally expensive models can generally achieve better accuracy), the discretization method is generally preferred over the distribution method.
Multinomial Naïve Bayes
With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial (p_1,…,p_n ) where p_iis the probability that event i occurs (or k such multinomials in the multiclass case). This is the event model typically used for document classification; the feature values are then term frequencies, generated by a multinomial that produces some number of words (“bag of words” assumption, where word order is ignored). The likelihood of observing a feature vector (histogram) F is given by
p(F?C)=(?_i?F_i )!/(?_i??F_i !?) ?_i?p_i^(F_i ) 
The multinomial Naïve Bayes classifier becomes a linear classifier when expressed in log-space:
log??p(C?F)?=log?(p(C) ?_(i=1)^n??p(F_i?C) ?)
=log??p(C)+?_(i=1)^n?log??p(F_i?C)? ?
=b+w_C^T F
Where b=log?p(C) and w_(c_i )=log??p(F_i?C)?.
If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudo-count, in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing Naïve Bayes is called Additive smoothing when the pseudo-count is one, and Lidstone smoothing in the general case.
Rennie et al. (Rennie, Lawrence, Teevan, & Karger, 2003) discuss problems with the multinomial assumption in the context of document classification and possible ways to alleviate those problems, including the use of tf–idf weights instead of raw term frequencies and document length normalization, to produce a Naïve Bayes classifier that is competitive with support vector machines.
Bernoulli Naïve Bayes
In the multivariate Bernoulli event model, features are independent Booleans (binary variables) describing inputs. This model is also popular for document classification tasks, where binary term occurrence features are used rather than term frequencies. If F_i is a Boolean expressing the occurrence or absence of the i-th term from the vocabulary, then the likelihood of a document given a class C is given by
p(F_1,…,F_n?C)=?_(i=1)^n?[F_i p(w_i?C)+(1-F_i )(1-p(w_i?C))] 
where p(w_i?C) is the probability of class C generating the term w_i. This event model is especially popular for classifying short texts. It has the benefit of explicitly modeling the absence of terms. Note that a Naïve Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one.
Discussion
Despite the fact that the far-reaching independence assumptions are often inaccurate, the Naïve Bayes classifier has several properties that make it surprisingly useful in practice. In particular, the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one-dimensional distribution. This helps alleviate problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. While Naïve Bayes often fails to produce a good estimate for the correct class probabilities, this may not be a requirement for many applications. For example, the Naïve Bayes classifier will make the correct MAP decision rule classification so long as the correct class is more probable than any other class. This is true regardless of whether the probability estimate is slightly, or even grossly inaccurate. In this manner, the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model. Other reasons for the observed success of the Naïve Bayes classifier are discussed in the literature cited below.
Examples
Sex classification
Problem: classify whether a given person is a male or a female based on the measured features. The features include height, weight, and foot size.
Training
Example training set below.
sex	height (feet)	weight (lbs)	foot size(inches)
male	6	180	12
male	5.92 (5’11”)	190	11
male	5.58 (5’7”)	170	12
male	5.92 (5’11”)	165	10
female	5	100	6
female	5.5 (5’6”)	150	8
female	5.42 (5’5”)	130	7
female	5.75 (5’9”)	150	9

The classifier created from the training set using a Gaussian distribution assumption would be (given variances are sample variances):
sex	mean (height)	variance (height)	mean (weight)	variance (weight)	Mean (foot size)	Variance (foot size)
male	5.8550	3.50e-02	176.25	1.23e+02	11.25	9.16e-01
female	5.4175	9.72e-02	132.50	5.58e+02	7.50	1.66e+00

Let’s say we have equiprobable classes so P("male")= P("female") = 0.5. This prior probability distribution might be based on our knowledge of frequencies in the larger population, or on frequency in the training set.
Testing
Below is a sample to be classified as a male or female.
sex	height (feet)	weight (lbs)	foot size(inches)
sample	6	130	8

We wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by
posterior("male" )=(P("male" )p("height" ?"male" )p("weight" ?"male" )p("foot size" ?"male" ))/"evidence" 
For the classification as female the posterior is given by
posterior("female" )=(P("female" )p("height" ?"female" )p("weight" ?"female" )p("foot size" ?"female" ))/"evidence" 
The evidence (also termed normalizing constant) may be calculated:
evidence=P("male" )p("height" ?"male" )p("weight" ?"male" )p("foot size" ?"male" )+P("female" )p("height" ?"female" )p("weight" ?"female" )p("foot size" ?"female" )
However, given the sample the evidence is a constant and thus scales both posteriors equally. It therefore does not affect classification and can be ignored. We now determine the probability distribution for the sex of the sample.
P("male" )=0.5
p("height" ?"male" )=1/?(2??^2 )  exp?((-(6-?)^2)/(2?^2 ))
where ?=5.855 and ?^2=3.5033??10?^(-2) are the parameters of normal distribution which have been previously determined from the training set. Note that a value greater than 1 is OK here—it is a probability density rather than a probability, because height is a continuous variable.
p("weight" ?"male" )=5.9881??10?^(-6)
p("foot size" ?"male" )=1.3112??10?^(-3)
posterior numerator male=their product=6.1984?10-9
P("female" )=0.5
p("height" ?"female" )=2.2346??10?^(-1)
p("weight" ?"female" )=1.6789??10?^(-2)
p("foot size" ?"female" )=2.8669??10?^(-3)
posterior numerator female=their product=5.3378?10-4
Since posterior numerator is greater in the female case, we predict the sample is female.
Document classification
Here is a worked example of Naïve Bayesian classification to the document classification problem. Consider the problem of classifying documents by their content, for example into spam and non-spam e-mails. Imagine that documents are drawn from a number of classes of documents which can be modelled as sets of words where the (independent) probability that the i-th word of a given document occurs in a document from class C can be written as
p(w_i?C)
(For this treatment, we simplify things further by assuming that words are randomly distributed in the document—that is, words are not dependent on the length of the document, position within the document with relation to other words, or other document-context.)
Then the probability that a given document D contains all of the words w_i, given a class C, is
p(D?C)=?_i??p(w_i?C) ?
The question that we desire to answer is: “what is the probability that a given document D belongs to a given class C?” In other words, what is p(C?D)? Now by definition
p(D?C)=p(D?C)/p(C) 
and
p(C?D)=p(D?C)/p(D) 
Bayes’ theorem manipulates these into a statement of probability in terms of likelihood.
p(C?D)=p(C)/p(C)  p(D?C)
Assume for the moment that there are only two mutually exclusive classes, S and ¬S (e.g. spam and not spam), such that every element (email) is in either one or the other;
p(D?S)=?_i??p(w_i?S) ?
And
p(D?¬S)=?_i??p(w_i?¬S) ?
Using the Bayesian result above, we can write:
p(S?D)=p(S)/p(D)  ?_i??p(w_i?S) ?
p(¬S?D)=p(¬S)/p(D)  ?_i??p(w_i?¬S) ?
Dividing one by the other gives:
(p(S?D))/(p(¬S?D) )=(p(S) ?_i??p(w_i?S) ?)/(p(¬S) ?_i??p(w_i?¬S) ?)
Which can be re-factored as:
(p(S?D))/(p(¬S?D) )=p(S)/p(¬S)  ?_i?(p(w_i?S))/(p(w_i?¬S) )
Thus, the probability ratio (p(S?D))?(p(¬S?D) ) can be expressed in terms of a series of likelihood ratios. The actual probability p(S?D) can be easily computed from log?((p(S?D))?(p(¬S?D) )) based on the observation that p(S?D)+p(¬S?D)=1.
Taking the logarithm of all these ratios, we have:
ln??(p(S?D))/(p(¬S?D) )?=ln??p(S)/p(¬S) ?=?_i?ln??(p(w_i?S))/(p(w_i?¬S) )? 
(This technique of “log-likelihood ratios“ is a common technique in statistics. In the case of two mutually exclusive alternatives (such as this example), the conversion of a log-likelihood ratio to a probability takes the form of a sigmoid curve: see logit for details.)
Finally, the document can be classified as follows. It is spam if p(S?D)>p(¬S?D) (i.e., ln??(p(S?D))/(p(¬S?D) )?>0), otherwise it is not spam (Kibriya, Frank, Pfahringer, & Holmes, 2008).
Software
Open Source software packages include PNL and R. Software packages that are specifically designed for this function include:
	Bayesian belief network software – from J. Cheng, includes: 
	BN PowerConstructor: An efficient system for learning BN structures and parameters from data; 
	BN PowerPredictor: A data mining program for data modeling/classification/prediction. 
	Bayesian Network tools in Java (BNJ) – is an open-source suite of Java tools for probabilistic learning and reasoning (Kansas State University KDD Lab) 
	GeNIe – is a  decision modeling environment implementing influence diagrams and Bayesian networks. 
	JNCC2 (Naive Credal Classifier 2) – is an extension of Naive Bayes towards imprecise probabilities; it is designed to return robust classification, even on small and/or incomplete data sets. 
Commercial software packages include:
	AgenaRisk – by Ajena, is a visual tool, combining Bayesian networks and statistical simulation. 
	BayesiaLab – by, Bayesia is a complete set of Bayesian network tools, including supervised and unsupervised learning, and analysis toolbox. 
	BNet – by Charles River Analytics, includes BNet.Builder for rapidly creating Belief Networks, entering information, and getting results and BNet.EngineKit for incorporating Belief Network Technology in your applications. 
	Flint – by Logic Programming Associates, combines bayesian networks, certainty factors and fuzzy logic within a logic programming rules-based environment. 
	Netica – by NORSYS, is Bayesian network development software provides Bayesian network tools. 
	PrecisionTree – by Palisade (makers of @Risk), is an add-in for Microsoft Excel for building decision trees and influence diagrams directly in the spreadsheet
Example Using R
The Iris dataset is pre-installed in R, since it is in the standard datasets package. To access its documentation, click on ‘Packages’ at the top-level of the R documentation, then on ‘datasets’ and then on ‘iris’. As explained, there are 150 data points and 5 variables. Each data point concerns a particular iris flower and gives 4 measurements of the flower: Sepal.Length, Sepal.Width, Petal.Length and Petal.Width together with the flower’s Species. The goal is to build a classifier that predicts species from the 4 measurements, so species is the class variable. 
To get the iris dataset into your R session, do:
> data(iris)

at the R prompt. As always, it makes sense to look at the data. The following R command (from the Wikibook) does a nice job of this.
> pairs(iris[1:4],main=“Iris Data 
+    (red=setosa,green=versicolor,blue=virginica)”, pch=21, 
+     bg=c(“red”,”green3”,”blue”)[unclass(iris$Species)])

The ‘pairs’ command creates a scatterplot. Each dot is a data point and its position is determined by the values that data point has for a pair of variables. The class determines the color of the data point. From the plot note that Setosa irises have smaller petals than the other two species. 
 
Figure 8-1. Iris data matrix plot (scatterplots)
Typing: 
> summary(iris)

provides a summary of the data. 
Sepal.Length	Sepal.Width	Petal.Length	Petal.Width
Min.   :4.300	Min.   :2.000	Min.   :1.000	Min.   :0.100
1st Qu.:5.100	1st Qu.:2.800	1st Qu.:1.600	1st Qu.:0.300
Median :5.800	Median :3.000	Median :4.350	Median :1.300
Mean   :5.843	Mean   :3.057	Mean   :3.758	Mean   :1.199
3rd Qu.:6.400	3rd Qu.:3.300	3rd Qu.:5.100	3rd Qu.:1.800
Max.   :7.900	Max.   :4.400	Max.   :6.900	Max.   :2.500
Species			
setosa    :50			
versicolor:50			
virginica :50			

Typing: 
> iris

prints out the entire dataset to the screen. 
	Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species
1	5.1	3.5	1.4	0.2	setosa
2	4.9	3	1.4	0.2	setosa
3	4.7	3.2	1.3	0.2	setosa
4	4.6	3.1	1.5	0.2	setosa
5	5	3.6	1.4	0.2	setosa
-----------------------------Data omitted-------------------------
149	6.2	3.4	5.4	2.3	virginica
150	5.9	3	5.1	1.8	virginica

Constructing a Naïve Bayes classifier
We will use the e1071 R package to build a Naïve Bayes classifier. Firstly you need to download the package (since it is not pre-installed here). Do:
> install.packages(“e1071”)

Choose a mirror in US from the menu that will appear. You will be prompted to create a personal R library (say yes) since you don’t have permission to put e1071 in the standard directory for R packages. 
To (1) load e1071 into your workspace (2) build a Naïve Bayes classifier and (3) make some predictions on the training data, do:
> library(e1071)
> classifier<-naiveBayes(iris[,1:4], iris[,5]) 
> table(predict(classifier, iris[,-5]), iris[,5], 
+     dnn=list(‘predicted’,’actual’))

As you should see the classifier does a pretty good job of classifying. Why is this not surprising? 
predicted    setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         47         3
  virginica       0          3        47

To see what’s going on ‘behind-the-scenes’, first do:
> classifier$apriori

This gives the class distribution in the data: the prior distribution of the classes. (‘A priori’ is Latin for ‘from before’.)
iris[, 5]
    setosa versicolor  virginica 
        50         50         50

Since the predictor variables here are all continuous, the Naïve Bayes classifier generates three Gaussian (Normal) distributions for each predictor variable: one for each value of the class variable Species. If you type: 
> classifier$tables$Petal.Length

You will see the mean (first column) and standard deviation (second column) for the 3 class-dependent Gaussian distributions:
            Petal.Length
iris[, 5]     [,1]      [,2]
  setosa     1.462 0.1736640
  versicolor 4.260 0.4699110
  virginica  5.552 0.5518947

You can plot these 3 distributions against each other with the following three R commands: 
> plot(function(x) dnorm(x, 1.462, 0.1736640), 0, 8, 
+    col=“red”, main=“Petal length distribution for the 3 
+    different species”)
> curve(dnorm(x, 4.260, 0.4699110), add=TRUE, col=“blue”)
> curve(dnorm(x, 5.552, 0.5518947), add=TRUE, col=“green”)

Note that setosa irises (the red curve) tend to have smaller petals (mean value = 1.462) and there is less variation in petal length (standard deviation is only 0.1736640).
 
Figure 8-2. Iris petal length distribution for three species
Understanding Naïve Bayes
In the previous example you were given a recipe which allowed you to construct a Naïve Bayes classifier. This was for a case where we had continuous predictor variables. In this question you have to work out what the parameters of a Naïve Bayes model should be for some discrete data. 
The dataset in question is called HairEyeColor and has three variables: Sex, Eye and Hair, giving values for these 3 variables for each of 592 students from the University of Delaware. First have a look at the numbers:
> HairEyeColor

, , Sex = Male

       Eye
Hair    Brown Blue Hazel Green
  Black    32   11    10     3
  Brown    53   50    25    15
  Red      10   10     7     7
  Blond     3   30     5     8

, , Sex = Female

       Eye
Hair    Brown Blue Hazel Green
  Black    36    9     5     2
  Brown    66   34    29    14
  Red      16    7     7     7
  Blond     4   64     5     8

You can also plot it as a ‘mosaic’ plot which uses rectangles to represent the numbers in the data:
> mosaicplot(HairEyeColor)

Your job here is to compute the parameters for a Naïve Bayes classifier which attempts to predict Sex from the other two variables. The parameters should be estimated using maximum likelihood. To save you the tedium of manual counting, here’s how to use margin.table to get the counts you need:
> margin.table(HairEyeColor,3)

Sex
  Male Female 
   279    313 

 
Figure 8-3. Mosaic plot Hair and Eye color

> margin.table(HairEyeColor,c(1,3))

       Sex
Hair    Male Female
  Black   56     52
  Brown  143    143
  Red     34     37
  Blond   46     81

Note that Sex is variable 3, and Hair is variable 1. Once you think you have the correct parameters speak to me or one of the demonstrators to see if you have it right. (Or if you can manage it, construct the Naïve Bayes model using the naiveBayes function and yank out the parameters from the model. Read the documentation to do this.) 
Example Using SAS Studio
The problem we will use in this example is the Pima Indians Diabetes problem.
This problem is comprised of 768 observations of medical details for Pima Indians patents. The records describe instantaneous measurements taken from the patient such as their age, the number of times pregnant and blood workup. All patients are women aged 21 or older. All attributes are numeric, and their units vary from attribute to attribute.
Each record has a class value that indicates whether the patient suffered an onset of diabetes within 5 years of when the measurements were taken (1) or not (0). (This will be left as an exercise.)
For this example, we will fit a Naïve Bayes model using PROC GENMOD with Diabetes pedigree function as the dependent variable.
Attribute Information:
	X1= Number of times pregnant 
	X2=Plasma glucose concentration a 2 hours in an oral glucose tolerance test 
	X3=Diastolic blood pressure (mm Hg) 
	X4=Triceps skin fold thickness (mm) 
	X5=2-Hour serum insulin (mu U/ml) 
	X6=Body mass index (weight in kg/(height in m)^2) 
	X7=Diabetes pedigree function 
	X8=Age (years) 
	X9=Class variable (0 or 1)
This is a standard dataset that has been studied a lot in machine learning literature. A good prediction accuracy is 70%-76%.
The sample from the pima-indians.data.csv file can be downloaded at https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data .
Program
PROC GENMOD DATA=PIMA_INDIANS;
MODEL x7 = x1  x2  x3  x3  x5  x6  x8  x9/
DIST=normal;
BAYES OUTPOST=POST;
RUN;
Output
The results of this analysis are shown in the following figures.
The “Model Information” table in Figure 8-4 summarizes information about the model you fit and the size of the simulation.
Model Information
Data Set	WORK.PIMA_INDIANS
Burn-In Size	2000
MC Sample Size	10000
Thinning	1
Sampling Algorithm	Conjugate
Distribution	Normal
Link Function	Identity
Dependent Variable	x7
	
Number of Observations Read	768
Number of Observations Used	768
Algorithm converged.
Figure 8-4. Model Information
The “Analysis of Maximum Likelihood Parameter Estimates” table in Figure 8-5 summarizes maximum likelihood estimates of the model parameters.
Analysis Of Maximum Likelihood Parameter Estimates
Parameter	DF	Estimate	Standard
Error	Wald 95% Confidence Limits
Intercept	1	0.2902	0.0719	0.1492	0.4312
x1	1	-0.0079	0.0041	-0.0160	0.0002
x2	1	0.0001	0.0004	-0.0008	0.0009
x3	1	-0.0000	0.0006	-0.0013	0.0012
x5	1	0.0004	0.0001	0.0002	0.0006
x6	1	0.0029	0.0016	-0.0002	0.0061
x8	1	0.0013	0.0012	-0.0011	0.0037
x9	1	0.0956	0.0288	0.0392	0.1521
Scale	1	0.3200	0.0082	0.3044	0.3364
Note: The scale parameter was estimated by maximum likelihood.
Figure 8-5. Maximum Likelihood Parameter Estimates
Since no prior distributions for the regression coefficients were specified, the default noninformative uniform distributions shown in the “Uniform Prior for Regression Coefficients” table in Figure 8-6 are used. Noninformative priors are appropriate if you have no prior knowledge of the likely range of values of the parameters, and if you want to make probability statements about the parameters or functions of the parameters. See, for example, Ibrahim, Chen, and Sinha (2001) for more information about choosing prior distributions.
Uniform Prior for Regression Coefficients
Parameter	Prior
Intercept	Constant
x1	Constant
x2	Constant
x3	Constant
x5	Constant
x6	Constant
x8	Constant
x9	Constant
Algorithm converged.
Figure 8-6. Regression Coefficient Priors
The default noninformative prior distribution for the normal scale parameter is shown in the “Independent Prior Distributions for Model Parameters” table in Figure 8-7.
Independent Prior Distributions for Model Parameters
Parameter	Prior Distribution
Dispersion	Improper
Figure 8-7. Scale Parameter Prior by default, the maximum likelihood estimates of the regression parameters are used as the starting values for the simulation when noninformative prior distributions are used. These are listed in the “Initial Values and Seeds” table in Figure 8-8.
Initial Values of the Chain
Chain	Seed	Intercept	x1	x2	x3
1	828772549	0.290169	-0.00793	0.000076	-0.00002
	x5	x6	x8	x9	Dispersion
	0.000423	0.002931	0.001317	0.095646	0.102113
Figure 8-8. MCMC Initial Values and Seeds
Summary statistics for the posterior sample are displayed in the “Fit Statistics,” “Descriptive Statistics for the Posterior Sample,” “Interval Statistics for the Posterior Sample,” and “Posterior Correlation Matrix” tables in Figure 8-9, Figure 8-10, Figure 8-11, and Figure 8-12, respectively.
Fit Statistics
DIC (smaller is better)	447.220
pD (effective number of parameters)	8.992
Figure 8-9. Fit Statistics

Posterior Summaries
Parameter	N	Mean	Standard
Deviation	Percentiles
				25%	50%	75%
Intercept	10000	0.2918	0.0726	0.2419	0.2925	0.3418
x1	10000	-0.00796	0.00419	-0.0108	-0.00801	-0.00516
x2	10000	0.000071	0.000443	-0.00023	0.000072	0.000368
x3	10000	-0.00002	0.000645	-0.00045	-0.00002	0.000418
x5	10000	0.000422	0.000109	0.000347	0.000422	0.000496
x6	10000	0.00290	0.00163	0.00180	0.00290	0.00400
x8	10000	0.00132	0.00124	0.000490	0.00130	0.00214
x9	10000	0.0958	0.0287	0.0764	0.0953	0.1155
Dispersion	10000	0.1037	0.00528	0.1001	0.1036	0.1072
Figure 8-10. Descriptive Statistics

Posterior Intervals
Parameter	Alpha	Equal-Tail Interval	HPD Interval
Intercept	0.050	0.1517	0.4339	0.1495	0.4304
x1	0.050	-0.0161	0.000348	-0.0157	0.000653
x2	0.050	-0.00079	0.000962	-0.00081	0.000943
x3	0.050	-0.00129	0.00125	-0.00133	0.00120
x5	0.050	0.000210	0.000636	0.000217	0.000641
x6	0.050	-0.00031	0.00607	-0.00038	0.00597
x8	0.050	-0.00105	0.00375	-0.00107	0.00370
x9	0.050	0.0395	0.1513	0.0390	0.1504
Dispersion	0.050	0.0937	0.1146	0.0930	0.1137
Figure 8-11. Interval Statistics

Posterior Correlation Matrix
Para-meter	Inter-cept	x1	x2	x3	x5	x6	x8	x9	Disper-sion
Inter-cept	1.000	0.031	-0.494	-0.272	0.156	-0.518	-0.245	0.325	-0.008
x1	0.031	1.000	0.048	-0.040	0.048	0.040	-0.495	-0.133	-0.010
x2	-0.494	0.048	1.000	-0.065	-0.295	-0.030	-0.190	-0.389	0.003
x3	-0.272	-0.040	-0.065	1.000	-0.038	-0.269	-0.176	0.093	-0.005
x5	0.156	0.048	-0.295	-0.038	1.000	-0.136	0.090	0.024	0.005
x6	-0.518	0.040	-0.030	-0.269	-0.136	1.000	0.046	-0.239	0.010
x8	-0.245	-0.495	-0.190	-0.176	0.090	0.046	1.000	-0.056	0.005
x9	0.325	-0.133	-0.389	0.093	0.024	-0.239	-0.056	1.000	0.007
Dispersion	-0.008	-0.010	0.003	-0.005	0.005	0.010	0.005	0.007	1.000
Figure 8-12. Posterior Sample Correlation Matrix 
By default, PROC GENMOD computes three convergence diagnostics: the lag1, lag5, lag10, and lag50 autocorrelations (Figure 8-13); Geweke diagnostic statistics (Figure 8-14); and effective sample sizes (Figure 8-15). There is no indication that the Markov chain has not converged.
Posterior Autocorrelations
Parameter	Lag 1	Lag 5	Lag 10	Lag 50
Intercept	-0.0096	-0.0049	-0.0076	-0.0043
x1	0.0033	-0.0010	0.0012	0.0148
x2	0.0040	-0.0122	-0.0194	-0.0083
x3	0.0127	0.0002	0.0030	-0.0033
x5	-0.0077	0.0021	-0.0084	-0.0061
x6	0.0062	-0.0043	0.0108	-0.0098
x8	-0.0026	0.0170	0.0078	-0.0098
x9	0.0005	-0.0157	-0.0078	-0.0131
Dispersion	0.0094	0.0013	-0.0034	0.0166
Figure 8-13. Posterior Sample Autocorrelations
Geweke Diagnostics
Parameter	z	Pr > |z|
Intercept	0.2584	0.7961
x1	-0.1427	0.8865
x2	0.4263	0.6699
x3	0.0812	0.9353
x5	-0.1587	0.8739
x6	-0.6058	0.5447
x8	-0.1973	0.8436
x9	0.4084	0.6830
Dispersion	0.3882	0.6978
Figure 8-14. Geweke Diagnostic Statistics
Effective Sample Sizes
Parameter	ESS	Autocorrelation
Time	Efficiency
Intercept	10000.0	1.0000	1.0000
x1	10000.0	1.0000	1.0000
x2	10000.0	1.0000	1.0000
x3	9753.2	1.0253	0.9753
x5	10000.0	1.0000	1.0000
x6	10000.0	1.0000	1.0000
x8	10000.0	1.0000	1.0000
x9	10000.0	1.0000	1.0000
Dispersion	10000.0	1.0000	1.0000
Figure 8-15. Effective Sample Sizes
Trace, autocorrelation, and density plots for the seven model parameters, shown in Figure 8-16 through Figure 8-24, are useful in diagnosing whether the Markov chain of posterior samples has converged. These plots show no evidence that the chain has not converged.
 
Figure 8-16. Diagnostic plots for the Intercept
 
Figure 8-17. Diagnostic plots for Number of times pregnant
 
Figure 8-18. Diagnostic plots for Plasma glucose concentration a 2 hours in an oral glucose tolerance test
 
Figure 8-19. Diagnostic plots for Diastolic blood pressure (mm Hg)
 
Figure 8-20. Diagnostic plots for 2-Hour serum insulin (mu U/ml)
 
Figure 8-21. Diagnostic plots for Body mass index (weight in kg/(height in m)^2)
 
Figure 8-22. Diagnostic plots for Age (years)
 
Figure 8-23. Diagnostic plots for Class variable (0 or 1)
 
Figure 8-24. Diagnostic plots for Dispersion
Exercises
Exercises may entail some data preparation in your Hadoop environment.
	Assume that a driver with a riskfactor greater than 196650 is unsafe and a driver with a riskfactor less than or equal to 196650 is safe. Using the risk factor data from Chapter 3, fit a Naïve Bayes model to driver classification (safe, unsafe) using explanatory variables from truck_mileage, driver_mileage, avg_mileage, geolocation and trucks as appropriate.
	Using the Baseball data from Chapter 3, construct a model that predicts a players 1994 salary using the effects: Games Played, At-Bats, Runs, Hits (1-base), Doubles (2-bases), Triples (3-bases), Homeruns, Runs Batted In (RBI). 
	What would be the 1994 salary of a player with 90 games played, 270 At-Bats, 100 Hits, 20 Doubles, 10 Homeruns, and 200 RBIs?
	What would be the 1994 salary of a player with 90 games played, 270 At-Bats, 50 Hits, 20 Doubles, 15 Homeruns, and 100 RBIs?
	Are Batting statistics the only basis for salaries?
	Referring to the Example Using SAS for the Pima Indians Diabetes Study, each record has a class value that indicates whether the patient suffered an onset of diabetes within 5 years of when the measurements were taken (1) or not (0). Model this situation with Naïve Bayes using a binomial distribution.
?
Decision tree learning

Decision tree learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the item’s target value. It is one of the predictive modeling approaches used in statistics, data mining and machine learning. More descriptive names for such tree models are classification trees or regression trees. In these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels.
In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data but not decisions; rather the resulting classification tree can be an input for decision making. This chapter deals with decision trees in data mining.
General
Decision tree learning is a method commonly used in data mining (Rokach & Maimon, 2008). The goal is to create a model that predicts the value of a target variable based on several input variables. An example is shown on the right. Each interior node corresponds to one of the input variables; there are edges to children for each of the possible values of that input variable. Each leaf represents a value of the target variable given the values of the input variables represented by the path from the root to the leaf.
A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning. For this section, assume that all of the features have finite discrete domains, and there is a single target feature called the classification. Each element of the domain of the classification is called a class. A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature (see Figure 9-1). The arcs coming from a node labeled with a feature are labeled with each of the possible values of the feature. Each leaf of the tree is labeled with a class or a probability distribution over the classes.
 
Figure 9-1. Example of a Decision Tree
A tree showing survival of passengers on the Titanic (“sibsp” is the number of spouses or siblings aboard). The figures under the leaves show the probability of survival and the percentage of observations in the leaf.
A tree can be “learned” by splitting the source set into subsets based on an attribute value test. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions. This process of top-down induction of decision trees (TDIDT) (Quinlan, 1986) is an example of a “greedy” algorithm, and it is by far the most common strategy for learning decision trees from data.
In data mining, decision trees can be described also as the combination of mathematical and computational techniques to aid the description, categorization and generalization of a given set of data.
Data comes in records of the form:
(x,Y)=(x_1,x_2,x_3,…,x_k,Y).
The dependent variable, Y, is the target variable that we are trying to understand, classify or generalize. The vector x is composed of the input variables, x_1,x_2,x_3, etc., that are used for that task.
Types
Decision trees used in data mining are of two main types:
Classification tree analysis is when the predicted outcome is the class to which the data belongs.
Regression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient’s length of stay in a hospital).
The term Classification And Regression Tree (CART) analysis is an umbrella term used to refer to both of the above procedures, first introduced by Breiman et al. (Breiman, Friedman, Olshen, & Stone, 1984). Trees used for regression and trees used for classification have some similarities—but also some differences, such as the procedure used to determine where to split.
Some techniques, often called ensemble methods, construct more than one decision tree:
Bagging decision trees, an early ensemble method, builds multiple decision trees by repeatedly resampling training data with replacement, and voting the trees for a consensus prediction (Breiman L. , Bagging predictors, 1996).
A Random Forest classifier uses a number of decision trees, in order to improve the classification rate.
Boosted Trees can be used for regression-type and classification-type problems (Friedman, Stochastic Gradient Boosting, 1999) (Hastie, Tibshirani, & Friedman, The elements of statistical learning : Data mining, inference, and prediction, 2001).
Rotation forest, in which every decision tree is trained by first applying principal component analysis (PCA) on a random subset of the input features (Rodriguez, Kuncheva, & Alonso, 2006).
Decision tree learning is the construction of a decision tree from class-labeled training tuples. A decision tree is a flow-chart-like structure, where each internal (non-leaf) node denotes a test on an attribute, each branch represents the outcome of a test, and each leaf (or terminal) node holds a class label. The topmost node in a tree is the root node.
There are many specific decision-tree algorithms. Notable ones include:
ID3 (Iterative Dichotomiser 3)
C4.5 (successor of ID3)
CART (Classification And Regression Tree)
CHAID (CHi-squared Automatic Interaction Detector). Performs multi-level splits when computing classification trees (Kass, 1980).
MARS: extends decision trees to handle numerical data better.
Conditional Inference Trees. Statistics-based approach that uses non-parametric tests as splitting criteria, corrected for multiple testing to avoid overfitting. This approach results in unbiased predictor selection and does not require pruning (Hothorn, Hornik, & Zeileis, 2006).
ID3 and CART were invented independently at around same time (between 1970-1980), yet follow a similar approach for learning decision tree from training tuples.
Metrics
Algorithms for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items (Rokach & Maimon, Top-down induction of decision trees classifiers-a survey, 2005). Different algorithms use different metrics for measuring “best”. These generally measure the homogeneity of the target variable within the subsets. Some examples are given below. These metrics are applied to each candidate subset, and the resulting values are combined (e.g., averaged) to provide a measure of the quality of the split.
Gini impurity
Used by the CART (classification and regression tree) algorithm, Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled, if it were randomly labeled according to the distribution of labels in the subset. Gini impurity can be computed by summing the probability of each item being chosen times the probability of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category. This should not be confused with Gini coefficient.
To compute Gini impurity for a set of items, suppose i takes on values in {1,2,…,m}, and let f_i be the fraction of items labeled with value i in the set.
I_G (f)=?_(i=1)^m??f_i (1-f_i ) ?=?_(i=1)^m?(f_i-f_i^2 ) =?_(i=1)^m?f_i -?_(i=1)^m?f_i^2 =1-?_(i=1)^m?f_i^2 
Information gain
In information theory and machine learning, information gain is a synonym for Kullback–Leibler divergence. However, in the context of decision trees, the term is sometimes used synonymously with mutual information, which is the expectation value of the Kullback–Leibler divergence of a conditional probability distribution.
In particular, the information gain about a random variable X obtained from an observation that a random variable A takes the value A=a is the Kullback-Leibler divergence D_KL (p(x|a)||p(x|I)) of the prior distribution p(x|I) for x from the posterior distribution p(x|a) for x given a.
The expected value of the information gain is the mutual information I(X;A) of X and A, i.e., the reduction in the entropy of X achieved by learning the state of the random variable A.
In machine learning, this concept can be used to define a preferred sequence of attributes to investigate to most rapidly narrow down the state of X. Such a sequence (which depends on the outcome of the investigation of previous attributes at each stage) is called a decision tree. Usually an attribute with high mutual information should be preferred to other attributes.
General definition
In general terms, the expected information gain is the change in information entropy H from a prior state to a state that takes some information as given:
IG(T,a)=H(T)-H(T?a).
Formal Definition
Let T denote a set of training examples, each of the form (x,y)=(x_1,x_2,x_3,…,x_k,y) where x_a?vals(a) is the value of the  th attribute of example x and y is the corresponding class label. The information gain for an attribute a is defined in terms of entropy H( ) as follows:
IG(T,a)=H(T)-?_(v?vals(a))??|{x?T?x_a=v}|/|T| ?H({x?T?x_a=v}).?
The mutual information is equal to the total entropy for an attribute if for each of the attribute values a unique classification can be made for the result attribute. In this case, the relative entropies subtracted from the total entropy are 0.
Drawbacks
Although information gain is usually a good measure for deciding the relevance of an attribute, it is not perfect. A notable problem occurs when information gain is applied to attributes that can take on a large number of distinct values. For example, suppose that one is building a decision tree for some data describing the customers of a business. Information gain is often used to decide which of the attributes are the most relevant, so they can be tested near the root of the tree. One of the input attributes might be the customer’s credit card number. This attribute has a high mutual information, because it uniquely identifies each customer, but we do not want to include it in the decision tree: deciding how to treat a customer based on their credit card number is unlikely to generalize to customers we haven’t seen before (overfitting).
Information gain ratio is sometimes used instead. This biases the decision tree against considering attributes with a large number of distinct values. However, attributes with very low information values then appeared to receive an unfair advantage. In addition, methods such as permutation tests have been proposed to correct the bias (Deng, Runger, & Tuv, 2011).
Decision tree advantages
Amongst other data mining methods, decision trees have various advantages:
Simple to understand and interpret. People are able to understand decision tree models after a brief explanation.
Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed.
Able to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable. (For example, relation rules can be used only with nominal variables while neural networks can be used only with numerical variables.)
Uses a white box model. If a given situation is observable in a model the explanation for the condition is easily explained by Boolean logic. (An example of a black box model is an artificial neural network since the explanation for the results is difficult to understand.)
Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.
Robust. Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.
Performs well with large datasets. Large amounts of data can be analyzed using standard computing resources in reasonable time.
Limitations
The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts (Hyafil & Rivest, 1976). Consequently, practical decision-tree learning algorithms are based on heuristics such as the greedy algorithm where locally-optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally-optimal decision tree. To reduce the greedy effect of local-optimality some methods such as the dual information distance (DID) tree were proposed (I., A., N., & Singer, 2014). 
Decision-tree learners can create over-complex trees that do not generalize well from the training data. (This is known as overfitting (Bramer, 2007).) Mechanisms such as pruning are necessary to avoid this problem (with the exception of some algorithms such as the Conditional Inference approach, which does not require pruning (Strobl, Malley, & Tutz, 2009) (Hothorn, Hornik, & Zeileis, 2006)).
There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems. In such cases, the decision tree becomes prohibitively large. Approaches to solve the problem involve either changing the representation of the problem domain (known as propositionalization) (Horváth & Yamamoto, 2003) or using learning algorithms based on more expressive representations (such as statistical relational learning or inductive logic programming).
For data including categorical variables with different numbers of levels, information gain in decision trees is biased in favor of those attributes with more levels (Deng, Runger, & Tuv, 2011). However, the issue of biased predictor selection is avoided by the Conditional Inference approach.
Extensions
Decision graphs
In a decision tree, all paths from the root node to the leaf node proceed by way of conjunction, or AND. In a decision graph, it is possible to use disjunctions (ORs) to join two more paths together using Minimum message length (MML) (Tan & Dowe, 2004). Decision graphs have been further extended to allow for previously unstated new attributes to be learnt dynamically and used at different places within the graph. The more general coding scheme results in better predictive accuracy and log-loss probabilistic scoring.  In general, decision graphs infer models with fewer leaves than decision trees.
Alternative search methods
Evolutionary algorithms have been used to avoid local optimal decisions and search the decision tree space with little a priori bias (Papagelis, 2001) (Barros, Basgalupp, Carvalho, & Freitas, 2011).
It is also possible for a tree to be sampled using MCMC in a Bayesian paradigm (Chipman, George, & McCulloch, 1998).
The tree can be searched for in a bottom-up fashion (Barros, Cerri, Jaskowiak, & Carvalho, 2011).
Software
Many data mining software packages provide implementations of one or more decision tree algorithms. Open Source software packages for decision tree modeling include KNIME, Orange, R, scikit-learn, and Weka. 
Commercial packages include RapidMiner, SAS Enterprise Miner, and SPSS Modeler. CART, by Salford Systems, is the licensed proprietary code of the original CART authors.
Examples Using R
Classification Tree example 
Let’s use the data frame kyphosis to predict a type of deformation (kyphosis) after surgery, from age in months (Age), number of vertebrae involved (Number), and the highest vertebrae operated on (Start).
In R, call the rpart library. Recursive partitioning for classification, regression and survival trees. An implementation of most of the functionality of the 1984 book by Breiman, Friedman, Olshen and Stone (Breiman, Friedman, Olshen, & Stone, 1984). 
# Classification Tree with rpart
> library(rpart)

We will not grow the tree with the fit() and rpart functions.

 # grow tree 
> fit <- rpart(Kyphosis ~ Age + Number + Start,
+        method= "class", data=kyphosis)

where kyphosis is the response, with variables Age, Number, and Start. Class in the method and kyphosis is the data set. Next, we display the results.
> printcp(fit) # display the results

Classification tree:
rpart(formula = Kyphosis~Age + Number + Start, data = kyphosis, 
    method = “class”)

Variables actually used in tree construction:
[1] Age   Start

Root node error: 17/81 = 0.20988

n= 81 
        CP nsplit rel error xerror    xstd
1 0.176471      0   1.00000 1.0000 0.21559
2 0.019608      1   0.82353 1.2353 0.23200
3 0.010000      4   0.76471 1.2941 0.23548 

Now, we plot the tree to visualize the cross validation results as in Figure 9-2.
> plotcp(fit) # visualize cross-validation results 
 
Figure 9-2. Cross validation plot
Next, we display a summary of the model.
> summary(fit) # detailed summary of splits

Call:
rpart(formula = Kyphosis ~ Age + Number + Start, data = kyphosis, 
    method = “class”)
  n= 81 

          CP nsplit rel error   xerror      xstd
1 0.17647059      0 1.0000000 1.000000 0.2155872
2 0.01960784      1 0.8235294 1.117647 0.2243268
3 0.01000000      4 0.7647059 1.117647 0.2243268

Variable importance
 Start    Age Number 
    64     24     12 

Node number 1: 81 observations,    complexity param=0.1764706
  predicted class=absent   expected loss=0.2098765  P(node) =1
    class counts:    64    17
   probabilities: 0.790 0.210 
  left son=2 (62 obs) right son=3 (19 obs)
  Primary splits:
      Start  < 8.5  to the right, improve=6.762330, (0 missing)
      Number < 5.5  to the left,  improve=2.866795, (0 missing)
      Age    < 39.5 to the left,  improve=2.250212, (0 missing)
  Surrogate splits:
      Number < 6.5  to the left,  agree=0.802, adj=0.158, (0 split)

Node number 2: 62 observations,    complexity param=0.01960784
  predicted class=absent   expected loss=0.09677419  P(node) =0.7654321
    class counts:    56     6
   probabilities: 0.903 0.097 
  left son=4 (29 obs) right son=5 (33 obs)
  Primary splits:
      Start  < 14.5 to the right, improve=1.0205280, (0 missing)
      Age    < 55   to the left,  improve=0.6848635, (0 missing)
      Number < 4.5  to the left,  improve=0.2975332, (0 missing)
  Surrogate splits:
      Number < 3.5  to the left,  agree=0.645, adj=0.241, (0 split)
      Age    < 16   to the left,  agree=0.597, adj=0.138, (0 split)

Node number 3: 19 observations
  predicted class=present  expected loss=0.4210526  P(node) =0.2345679
    class counts:     8    11
   probabilities: 0.421 0.579 

Node number 4: 29 observations
  predicted class=absent   expected loss=0  P(node) =0.3580247
    class counts:    29     0
   probabilities: 1.000 0.000 

Node number 5: 33 observations,    complexity param=0.01960784
  predicted class=absent   expected loss=0.1818182  P(node) =0.4074074
    class counts:    27     6
   probabilities: 0.818 0.182 
  left son=10 (12 obs) right son=11 (21 obs)
  Primary splits:
      Age    < 55   to the left,  improve=1.2467530, (0 missing)
      Start  < 12.5 to the right, improve=0.2887701, (0 missing)
      Number < 3.5  to the right, improve=0.1753247, (0 missing)
  Surrogate splits:
      Start  < 9.5  to the left,  agree=0.758, adj=0.333, (0 split)
      Number < 5.5  to the right, agree=0.697, adj=0.167, (0 split)

Node number 10: 12 observations
  predicted class=absent   expected loss=0  P(node) =0.1481481
    class counts:    12     0
   probabilities: 1.000 0.000 

Node number 11: 21 observations,    complexity param=0.01960784
  predicted class=absent   expected loss=0.2857143  P(node) =0.2592593
    class counts:    15     6
   probabilities: 0.714 0.286 
  left son=22 (14 obs) right son=23 (7 obs)
  Primary splits:
      Age    < 111  to the right, improve=1.71428600, (0 missing)
      Start  < 12.5 to the right, improve=0.79365080, (0 missing)
      Number < 4.5  to the left,  improve=0.07142857, (0 missing)

Node number 22: 14 observations
  predicted class=absent   expected loss=0.1428571  P(node) =0.1728395
    class counts:    12     2
   probabilities: 0.857 0.143 

Node number 23: 7 observations
  predicted class=present  expected loss=0.4285714  P(node) =0.08641975
    class counts:     3     4
   probabilities: 0.429 0.571

We will now display a “rough” plot of the classification tree as in Figure 9-3.
 # plot tree 
> plot(fit, uniform=TRUE, 
+      main= "Classification Tree for Kyphosis")
> text(fit, use.n=TRUE, all=TRUE, cex=.8)

 Figure 9-3. “rough” plot of the classification tree
Now, we display a more refined plot of the tree as in Figure 9-4.
# create attractive postscript plot of tree 
> post(fit, file = "c:/tree.ps", 
+      title = "Classification Tree for Kyphosis")

 
Figure 9-4.Refined pot of the classification tree

We now “prune” the tree and display its plot as in Figures 9-5 and 9-6.
# prune the tree 
> pfit<- prune(fit, cp=0.01160389) # from cptable   
> 
> # plot the pruned tree 
> plot(pfit, uniform=TRUE, 
+      main="Pruned Regression Tree for Mileage")
> text(pfit, use.n=TRUE, all=TRUE, cex=.8)
> post(pfit, file = "c:/ptree2.ps", 
+      title = "Pruned Regression Tree for Mileage")
 
Figure 9-5. Pruned classification tree (version 1)
 
Figure 9-5. Pruned classification tree (version 1)
?
Regression Tree example
In this example we will predict car mileage from price, country, reliability, and car type. The data frame is cu.summary. We will only show the R inputs and plots here.
# Regression Tree Example
> library(rpart)

 # grow tree 
> fit <- rpart(Mileage~Price + Country + Reliability + 
+     Type, method=“anova”, data=cu.summary)

> printcp(fit) # display the results 
> plotcp(fit) # visualize cross-validation results 
> summary(fit) # detailed summary of splits

 # create additional plots 
> par(mfrow=c(1,2)) # two plots on one page 
> rsq.rpart(fit) # visualize cross-validation results   

 # plot tree 
> plot(fit, uniform=TRUE, 
+      main= "Regression Tree for Mileage")
> text(fit, use.n=TRUE, all=TRUE, cex=.8)

 # create attractive postcript plot of tree 
> post(fit, file = "c:/tree2.ps", 
+      title = "Regression Tree for Mileage")
 

 
 
 

# prune the tree 
> pfit<- prune(fit, cp=0.01160389) # from cptable   

 # plot the pruned tree 
> plot(pfit, uniform=TRUE, 
+      main= "Pruned Regression Tree for Mileage")
> text(pfit, use.n=TRUE, all=TRUE, cex=.8)
> post(pfit, file = "c:/ptree2.ps", 
+      title = "Pruned Regression Tree for Mileage")

It turns out that this produces the same tree as the original.
?
Exercises
Exercises may entail some data preparation in your Hadoop environment.
	Assume that a driver with a riskfactor greater than 196650 is unsafe and a driver with a riskfactor less than or equal to 196650 is safe. Using the risk factor data from Chapter 3, fit a Decision Tree model to driver classification (safe, unsafe) using explanatory variables from truck_mileage, driver_mileage, avg_mileage, geolocation and trucks as appropriate.
	Using the Baseball data from Chapter 3, construct a decision tree model that predicts the NLCS (National League Championship Series) Winner from 1965 to 2005, based on regular season Wins/Loss ratio, Runs/Run-Against ratio, and Earned Run Average (ERA). Based on actual NLCS winners, how accurate is your model based on NLCS winners from 2006 to 2015?
	Refer to the data set RefineDemoData.zip from Chapter 3. Use the table formed in part (d) by joining the omniture website log data to the CRM data (registered users) and CMS data (products). Construct a model to determine if males or females are more likely to buy products based on website activity. Compare your results with the Empirical Bayes model you built in Chapter 7.


 
Random Forests

Random forests are an ensemble learning method for classification (and regression) that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees. The algorithm for inducing a random forest was developed by Leo Breiman (Breiman L. , Random Forests, 2001) and Adele Cutler (Liaw, 2012), and “Random Forests” is their trademark. The term came from random decision forests that was first proposed by Tin Kam Ho of Bell Labs in 1995. The method combines Breiman’s “bagging“ idea and the random selection of features, introduced independently by Ho  (Ho T. , 1995)  (Ho T. , 1998) and Amit and Geman (Amit & Geman, 1997) in order to construct a collection of decision trees with controlled variance.
The selection of a random subset of features is an example of the random subspace method, which, in Ho’s formulation, is a way to implement classification proposed by Eugene Kleinberg (Kleinberg, 1996).
History
The early development of random forests was influenced by the work of Amit and Geman (Amit & Geman, 1997) which introduced the idea of searching over a random subset of the available decisions when splitting a node, in the context of growing a single tree. The idea of random subspace selection from Ho (Ho T. , 1998) was also influential in the design of random forests. In this method a forest of trees is grown, and variation among the trees is introduced by projecting the training data into a randomly chosen subspace before fitting each tree. Finally, the idea of randomized node optimization, where the decision at each node is selected by a randomized procedure, rather than a deterministic optimization was first introduced by Dietterich (Dietterich T. , 2000).
The introduction of random forests proper was first made in a paper by Leo Breiman (Breiman L. , Random Forests, 2001). This paper describes a method of building a forest of uncorrelated trees using a CART like procedure, combined with randomized node optimization and bagging. In addition, this paper combines several ingredients, some previously known and some novel, which form the basis of the modern practice of random forests, in particular:
1. Using out-of-bag error as an estimate of the generalization error.
2. Measuring variable importance through permutation.
The report also offers the first theoretical result for random forests in the form of a bound on the generalization error which depends on the strength of the trees in the forest and their correlation.
More recently several major advances in this area have come from Microsoft Research (Criminisi, Shotton, & Konukoglu, 2011), which incorporate and extend the earlier work from Breiman.
Algorithm
The training algorithm for random forests applies the general technique of bootstrap aggregating (see Bootstrap aggregating), or bagging, to tree learners. Given a training set X=x_1,…,x_n with responses Y=y_1 through y_n, bagging repeatedly selects a bootstrap sample of the training set and fits trees to these samples:
For b=1 through B:
1. Sample, with replacement, n training examples from X, Y; call these X_b,? Y?_b .
2. Train a decision or regression tree f_b on X_b,? Y?_b.
After training, predictions for unseen samples x' can be made by averaging the predictions from all the individual regression trees on x':
f ?=1/B ?_(b=1)^B??f ?_b (x') ?,
or by taking the majority vote in the case of decision trees.
In the above algorithm, B is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. Increasing the number of trees tends to decrease the variance of the model, without increasing the bias. As a result, the training and test error tend to level off after some number of trees have been fit. An optimal number of trees B can be found using cross-validation, or by observing the out-of-bag error: the mean prediction error on each training sample x?, using only the trees that did not have x? in their bootstrap sample (James, Witten, Hastie, & Tibshirani, 2013).
Bootstrap aggregating
Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.
Description of the technique
Given a standard training set D of size n, bagging generates m new training sets D_i, each of size n', by sampling from D uniformly and with replacement. By sampling with replacement, some observations may be repeated in each. If n'=n, then for large n the set is expected to have the fraction (1 - 1/e) (?63.2%) of the unique examples of D, the rest being duplicates (Aslam, Popa, & Rivest, 2007). This kind of sample is known as a bootstrap sample. The m models are fitted using the above m bootstrap samples and combined by averaging the output (for regression) or voting (for classification). Bagging leads to “improvements for unstable procedures” (Breiman L. , Random Forests, 2001), which include, for example, neural nets, classification and regression trees, and subset selection in linear regression  (Breiman L. , 1996). An interesting application of bagging showing improvement in preimage learning is provided here (Sahu, Runger, & Apley, 2011) (Shinde, Sahu, Apley, & Runger, 2014). On the other hand, it can mildly degrade the performance of stable methods such as K-nearest neighbors (Breiman, 1996).
Example: Ozone data
To illustrate the basic principles of bagging, below is an analysis on the relationship between ozone and temperature (data from Rousseeuw and Leroy (Rousseeuw & Leroy, 2003), available at classic data sets, analysis done in R).
The relationship between temperature and ozone in this data set is apparently non-linear, based on the scatter plot. To mathematically describe this relationship, LOESS smoothers (with span 0.5) are used. Instead of building a single smoother from the complete data set, 100 bootstrap samples of the data were drawn. Each sample is different from the original data set, yet resembles it in distribution and variability. For each bootstrap sample, a LOESS smoother was fit. Predictions from these 100 smoothers were then made across the range of the data. The first 10 predicted smooth fits appear as grey lines in the figure below. The lines are clearly very wiggly and they overfit the data - a result of the span being too low.
But taking the average of 100 smoothers, each fitted to a subset of the original data set, we arrive at one bagged predictor (red line). Clearly, the mean is more stable and there is less overfit (see Figure 10-1).
 
Figure 10-1. Bagging for nearest neighbor classifiers
It is well known that the risk of a 1 nearest neighbor (1NN) classifier is at most twice the risk of the Bayes classifier, but there are no guarantees that this classifier will be consistent. By careful choice of the size of the resamples, bagging can lead to substantial improvements of the performance of the 1NN classifier. By taking a large number of resamples of the data of size n’ , the bagged nearest neighbor classifier will be consistent provided n'?? diverges but n'?(n?0) as the sample size n??.
Under infinite simulation, the bagged nearest neighbor classifier can be viewed as a weighted nearest neighbor classifier. Suppose that the feature space is	d dimensional and denote by C_(n,n')^bnn the bagged nearest neighbor classifier based on a training set of size n, with resamples of size n'. In the infinite sampling case, under certain regularity conditions on the class distributions, the excess risk has the following asymptotic expansion
R_R (C_(n,n')^bnn )-R_R (C^bayes )=(B_1  n^'/n+B_2  1/(n^' )^(4?d) ){1+o(1)},
for some constants B_1 and B_2. The optimal choice of n^', that balances the two terms in the asymptotic expansion, is given by	n^'=Bn^(d?((d+4) )) for some constant B.
History
Bagging (Bootstrap aggregating) was proposed by Leo Breiman in 1994 to improve the classification by combining classifications of randomly generated training sets. See Breiman (Breiman L. , Bagging Predictors, 1994).
From bagging to random forests
The above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the B trees, causing them to become correlated.
Typically, for a dataset with p features, ?p features are used in each split.
Random subspace method
Random subspace method (or attribute bagging (Bryll, 2003)) is an ensemble classifier that consists of several classifiers and outputs the class based on the outputs of these individual classifiers. Random subspace method is a generalization of the random forest algorithm (Ho T. , 1998). Whereas random forests are composed of decision trees, a random subspace classifier can be composed from any underlying classifiers. Random subspace method has been used for linear classifiers (Skurichina, 2002), support vector machines (Tao, 2006), nearest neighbors (Tremblay, 2004) and other types of classifiers. This method is also applicable to one-class classifiers.
The algorithm is an attractive choice for classification problems where the number of features is much larger than the number of training objects, such as fMRI data or gene expression data (Kuncheva, Rodríguez, Plumpton, Linden, & Johnston, 2010).
Algorithm
The ensemble classifier is constructed using the following algorithm:
1. Let the number of training objects be N and the number of features in the training data be D.
2. Choose L to be the number of individual classifiers in the ensemble.
3. For each individual classifier, l, Choose d_l (d_l<D) to be the number of input variables for l. It is common to have only one value of d_l for all the individual classifiers
4. For each individual classifier, l, create a training set by choosing d_l features from D without replacement and train the classifier.
5. For classifying a new object, combine the outputs of the L individual classifiers by majority voting or by combining the posterior probabilities.
Relationship to Nearest Neighbors
Given a set of training data
D_n={(X_i,Y_i )}_(i=1)^n
a weighted neighborhood scheme makes a prediction for a query point X, by computing
Y ?=?_(i=1)^n??W_i (X) ? Y_i,
for some set of non-negative weights {W_i (X)}_(i=1)^n which sum to 1. The set of points X_i where W_i (X)>0 are called the neighbors of X. A common example of a weighted neighborhood scheme is the k-NN algorithm which sets W_i (X)=1?k if X_i is among the k closest points to X in D_n and 0 otherwise.
Random forests with constant leaf predictors can be interpreted as a weighted neighborhood scheme in the following way. Given a forest of M trees, the prediction that the	m-th tree makes for X can be written as
T_m (X)=?_(i=1)^n??W_im (X) Y_i,?
where W_im (X) is equal to 1?k_m  if X and X_i are in the same leaf in the m-th tree and 0 otherwise, and k_m is the number of training data which fall in the same leaf as X in the m-th tree. The prediction of the whole forest is
F(X)=?_(i=1)^n??T_m (X) ?=1/M ?_(m=1)^M???_(i=1)^n??W_im (X) ? Y_i ?=?_(i=1)^n??(1/M ?_(m=1)^M??W_im (X) ?) Y_i ?,
which shows that the random forest prediction is a weighted average of the Y_i’s, with weights
W_i (X)=1/M ?_(m=1)^M??W_im (X) ?.
The neighbors of X in this interpretation are the points X_i which fall in the same leaf as X in at least one tree of the forest. In this way, the neighborhood of X depends in a complex way on the structure of the trees, and thus on the structure of the training set.
This connection was first described by Lin and Jeon in a technical report from 2001 where they show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature (Lin & Jeon, 2001).
Variable importance
Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way. The following technique was described in Breiman’s original paper (Breiman L. , Random Forests, 2001) and is implemented in the R package randomForest (Liaw, 2012).
The first step in measuring the variable importance in a data set 
D_n={(X_i,Y_i )}_(i=1)^n is to fit a random forest to the data. During the fitting process the out-of-bag error for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).
To measure the importance of the j-th feature after training, the values of the j-th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set. The importance score for the j-th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees. The score is normalized by the standard deviation of these differences.
Features which produce large values for this score are ranked as more important than features which produce small values.
This method of determining variable importance has some drawbacks. For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as partial permutations can be used to solve the problem (Deng, Runger, & Tuv, 2011) (Altmann, Tolosi, Sander, & Lengauer, 2010). If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups (Tolosi & Lengauer, 2011).
Variants
Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and Naïve Bayes classifiers (Prinzie & Van den Poel, 2008).
Software
Open Source software package that have random forest functionality include R, and one specifically designed for this purpose:
	R – GNU R has several packages that perform random forest. Specifically, cforest() from library “party” or randomForest() from library “randomForest”.
	Random Forest™ - GNU Random Forests(tm) is a trademark of Leo Breiman and Adele Cutler. Runs can be set up with no knowledge of FORTRAN 77. The user is required only to set the right zero-one switches and give names to input and output files. This is done at the start of the program. It is licensed exclusively to Salford Systems for the commercial release of the software (see below). Their trademarks also include RF™, RandomForests™, RandomForest™ and Random Forest™.
Commercial software package that have random forest functionality include SPSS Modeler, and two specifically designed for this purpose:
	Random Forests – by Salford Systems, is a bagging tool that apply methods applied after the trees are grown and include new technology for identifying clusters or segments in data as well as new methods for ranking the importance of variables.
	RapidMiner – is a software platform developed by the company of the same name that provides an integrated environment for machine learning, data mining, text mining, predictive analytics and business analytics. It performs random forest.
Example Using R
Description
randomForest implements Breiman’s random forest algorithm (based on Breiman and Cutler’s original Fortran code) for classification and regression in R. It can also be used in unsupervised mode for assessing proximities among data points. 
We use the Forensic Glass data set was used in Chapter 12 of MASS4 (Venables and Ripley, 2002) to show how random forests work:
> library(randomForest)
> library(MASS)
> data(fgl)
> set.seed(17)
> fgl.rf <- randomForest(type ~ ., data = fgl,
+     mtry = 2, importance = TRUE,
+     do.trace = 100)
ntree      OOB      1      2      3      4      5      6
  100:   21.50% 14.29% 21.05% 64.71% 30.77% 11.11% 13.79%
  200:   19.63% 11.43% 21.05% 58.82% 23.08% 11.11% 13.79%
  300:   19.16% 11.43% 19.74% 58.82% 23.08% 11.11% 13.79%
  400:   18.69% 10.00% 18.42% 64.71% 23.08% 11.11% 13.79%
  500:   18.69% 10.00% 18.42% 64.71% 23.08% 11.11% 13.79%
> print(fgl.rf)
Call:
 randomForest(formula = type ~ ., data = fgl, mtry = 2, 
+	importance = TRUE,      do.trace = 100) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of error rate: 18.69%
Confusion matrix:
      WinF WinNF Veh Con Tabl Head class.error
WinF    63     6   1   0    0    0   0.1000000
WinNF   10    62   1   1    2    0   0.1842105
Veh      9     2   6   0    0    0   0.6470588
Con      0     2   0  10    0    1   0.2307692
Tabl     0     1   0   0    8    0   0.1111111
Head     1     3   0   0    0   25   0.1379310

Model Comparison
We can compare random forests with support vector machines by doing ten repetitions of 10-fold cross-validation, using the errorest functions in the ipred package:
> library(ipred)
> set.seed(131)
> error.RF <- numeric(10)
> for(i in 1:10) error.RF[i] <-
+     errorest(type ~ ., data = fgl,
+     model = randomForest, mtry = 2)$error
> summary(error.RF)
Min. 1st Qu. Median Mean 3rd Qu. Max.
0.1869 0.1974 0.2009 0.2009 0.2044 0.2103
> library(e1071)
> set.seed(563)
> error.SVM <- numeric(10)
> for (i in 1:10) error.SVM[i] <-
+     errorest(type ~ ., data = fgl,
+     model = svm, cost = 10, gamma = 1.5)$error
> summary(error.SVM)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.1822  0.1974  0.2079  0.2051  0.2138  0.2290

We see that the random forest compares quite favorably with SVM. We have found that the variable importance measures produced by random forests can sometimes be useful for model reduction (e.g., use the “important” variables to build simpler, more readily interpretable models). Figure 10-2 shows the variable importance of the Forensic Glass data set, based on the fgl.rf object created above. Roughly, it is created by
> par(mfrow = c(2, 2))
> for (i in 1:4)
+     plot(sort(fgl.rf$importance[,i], dec = TRUE),
+     type = “h”, main = paste(“Measure”, i))

We can see that measure 1 most clearly differentiates the variables. If we run random forest again dropping Na, K, and Fe from the model, the error rate remains below 20%.
 
Figure 10-2: Variable importance for the Forensic Glass data.
?
Example Using iPython
Introduction
In my last article, I presented Python programming using iPython. There, I used an example of logistic regression modeling for mothers with children having low birth weights. In this article, using the same example, I introduce Random Forest with iPython Notebook.
Random Forest is a machine learning algorithm used for classification, regression, and feature selection. It's an ensemble technique, meaning it combines the output of decision trees in order to get a stronger result.
In simplistic terms, Random Forest works by averaging decision tree output. It also ranks an individual tree’s output, by comparing it to the known output from the training data, which allows it to rank features. With Random Forest, some of the decision trees will perform better. Therefore, the features within those trees will be deemed more important. A Random Forest that generalizes well will have a higher accuracy by each tree, and higher diversity among its trees.
The Dataset
In this example, we are going to train a Random Forest classification algorithm to predict the class in the test data. The dataset I chose for this example in Longitudinal Low Birth Weight Study (CLSLOWBWT.DAT). [Hosmer and Lemeshow (2000) Applied Logistic Regression: Second Edition.] These data are copyrighted by John Wiley & Sons Inc. and must be acknowledged and used accordingly. I have split the data so each class is represented by a training set and testing set: train1 is the half of the set (245 rows) and test1 is the other half (245 rows).
Variable Description Codes/Values Name
	Identification Code ID Number ID
	Birth Number 1-4 BIRTH
	Smoking Status 0 = No, 1 = Yes SMOKE During Pregnancy
	Race 1 = White, 2 = Black RACE 3 = Other
	Age of Mother Years AGE
	Weight of Mother at Pounds LWT Last Menstrual Period
	Birth Weight Grams BWT
	Low Birth Weight 1 = BWT <=2500g, LOW 0 = BWT >2500g
Problem Statement
In this example, we want to predict Low Birth Weight using the remaining dataset variables. Low Birth Weight, the dependent variable, 1 = BWT <=2500g and 0 = BWT >2500g.
Import Modules
Note – you have to have  pandas (Panda.org, 2011), matplotlib (Whitaker, 2011), numPy (NumPy.org, 2016), scikit-learn, and sciPy  (SciPy.org, 2016) installed for this example. You can install them all easily using pip (‘pip install sciPy’, etc.). You could also download anacondas.
In [1]:
# First let's import required modules
import pandas as pd
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.cross_validation import cross_val_score
Import Datasets
Now let's import the dataset using Pandas or pd.
In [2]:
# Make sure you're in the right directory if using iPython
train = pd.read_csv("C:/Users/Strickland/Documents/Python Scripts/train1.csv")
test = pd.read_csv("C:/Users/Strickland/Documents/Python Scripts/test1.csv")
train.head()
Out[2]:
	ID	BIRTH	SMOKE	RACE	AGE	LWT	BWT	LOW
0	1	2	1	1	24	166	2457	1
1	2	1	1	1	27	124	2932	0
2	3	2	1	1	30	136	2092	1
3	4	1	1	1	28	215	3402	0
4	5	2	1	1	32	230	3538	0

Data Visualization
Before we delve into modeling, let's explore the data a little. We will use histograms to do this, and plot them within the Notebook.
In [3]:
# show plots in the notebook
%matplotlib inline
In [4]:
# histogram of birth number
train.BIRTH.hist()
plt.title('Histogram of Low Birth Weight')
plt.xlabel('Birth Number')
plt.ylabel('Frequency')
Out[4]:
<matplotlib.text.Text at 0x1d73a710>
 
In [5]:
# histogram of age of mother
train.AGE.hist()
plt.title('Histogram of Age of Mother')
plt.xlabel('Age')
plt.ylabel('Frequency')
Out[5]:
<matplotlib.text.Text at 0x1d6f2710>
 
Let's take a look at the distribution of smokers for those having children with low birth weights versus those who do not.
In [6]:
# Barplot of low birth weights grouped by smoker status (True or False)
pd.crosstab(train.SMOKE, train.LOW.astype(bool)).plot(kind='bar')
plt.title('Somker Distribution by Low Birth Weight')
plt.xlabel('Smoker')
plt.ylabel('Frequency')
Out[6]:
<matplotlib.text.Text at 0x1d9caba8>
 
Configure the Data
The data from the training set has to be put into numpy arrays in order for the Random Forest algorithm to accept it. Also, the dependent variable array must be a 1d, as opposed to a column vector. train.as.matrix() will execute the array and ravel() will convert vector array into a 1d array.
In [7]:
# The data have to be in a numpy array in order for
# the random forest algorithm to accept it!
# Also, output must be separated.
cols = ['BIRTH', 'SMOKE', 'RACE', 'AGE', 'LWT', 'BWT'] 
colsRes = ['LOW']
trainArr = train.as_matrix(cols) #training array
trainRes = np.ravel(train.as_matrix(colsRes)) # training results

Let's check our arrays.
In [8]:
trainArr
Out[8]:
array([[   2,    1,    1,   24,  166, 2457],
       [   1,    1,    1,   27,  124, 2932],
       [   2,    1,    1,   30,  136, 2092],
       ..., 
       [   1,    1,    1,   29,  140, 3238],
       [   2,    1,    1,   33,  161, 2966],
       [   1,    1,    1,   19,  138, 2591]], dtype=int64)
In [9]:
trainRes
Out[9]:
array([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
       1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
       1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
       0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0], dtype=int64)
Fit the Data
Now, we fit the data using Random Forest.
In [13]:
## Training!
rf = RandomForestClassifier(n_estimators=100) # initialize
rf.fit(trainArr, trainRes) # fit the data to the algorithm
Out[13]:
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)

Prepare the Testing Data
We prepare the testing data the way with did for the training data.
In [14]:
## Testing!
# put the test data in the same format!
testArr = test.as_matrix(cols)
results = rf.predict(testArr)

Predictions
Next, we add the predictions we obtained with the test data back to the data frame, so we can compare side-by-side
In [15]:
# Add predictions back to the data frame
test['predictions'] = results
In [16]:
test
Out[16]:
	ID	BIRTH	SMOKE	RACE	AGE	LWT	BWT	LOW	predictions
0	245	1	1	3	28	120	2865	0	0
1	246	2	1	3	33	141	2609	0	0
2	247	1	0	1	29	130	2613	0	0
3	248	2	0	1	34	151	3125	0	0
4	249	3	0	1	37	144	2481	1	1
5	250	1	1	2	31	187	1841	1	1
6	251	2	1	2	35	209	1598	1	1
7	252	3	1	2	41	217	2015	1	1
8	253	1	0	3	25	105	3489	0	0
9	254	2	0	3	30	129	3554	0	0
10	255	1	0	3	25	85	2719	0	0
11	256	2	0	3	30	106	2957	0	0
12	257	1	0	3	27	150	3226	0	0
13	258	2	0	3	33	172	3293	0	0
14	259	3	0	3	36	175	3091	0	0
15	260	1	0	3	23	97	3138	0	0
16	261	2	0	3	25	106	3247	0	0
17	262	3	0	3	31	128	3159	0	0
18	263	1	0	2	24	128	2796	0	0
19	264	2	0	2	29	152	2603	0	0
20	265	3	0	2	35	156	2884	0	0
...	...	...	...	...	...	...	...	...	...
240	485	2	1	1	26	107	1452	1	1
241	486	1	1	2	32	121	2907	0	0
242	487	2	1	2	35	143	2465	1	1
243	488	1	0	1	25	155	2944	0	0
244 rows × 9 columns
Predicting Probabilities
We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look.
In [17]:
predicted = rf.predict(testArr)
print predicted
[0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0
 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0
 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1
 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0
 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0]
In [18]:
# generate class probabilities
probs = rf.predict_proba(testArr)
print probs
[[ 0.91  0.09]
 [ 0.9   0.1 ]
 [ 1.    0.  ]
 [ 0.97  0.03]
 [ 0.14  0.86]
 [ 0.2   0.8 ]
 [ 0.29  0.71]
 [ 0.3   0.7 ]
 [ 0.98  0.02]
 [ 0.99  0.01]
 [ 0.97  0.03]
 [ 0.98  0.02]
 [ 0.99  0.01]
 [ 0.98  0.02]
 [ 1.    0.  ]
 [ 0.98  0.02]
 [ 0.97  0.03]
 [ 0.98  0.02]
 [ 0.98  0.02]
 …
 [ 1.    0.  ]
 [ 0.06  0.94]
 [ 0.02  0.98]
 [ 0.95  0.05]
 [ 0.17  0.83]
 [ 0.99  0.01]]

Predicting the Probability of Low Birth Weight Child
Just for fun, let's predict the probability of a low birth weight child for a random woman not present in the dataset. She's a 35-year-old Other race, has had 2 births,(has 2 children), is a smoker, and her weight is 132. [BIRTH SMOKE RACE AGE LWT BWT LOW ]
In [19]:
rf.predict_proba(np.array([0, 1, 1, 35, 192, 1]))
Out[19]:
array([[ 0.26,  0.74]])

Accuracy Check
Finally, we check the accuracy on the test set and generate evaluation metrics.
In [20]:
testRes = test.as_matrix(colsRes) # training results
# check the accuracy on the training set
rf.score(testArr,testRes)
Out[20]:
1.0
In [21]:
# generate evaluation metrics
print metrics.accuracy_score(testRes, predicted)
print metrics.roc_auc_score(testRes, probs[:, 1])
1.0
1.0

Though this will not always happen, our predictions appear to be perfect.
Conclusion
The Random Forest algorithm predicted class perfectly with this dataset. That is unlikely to happen with larger datasets, e.g., more records and more variables.
Sometimes in machine learning, models will be overfitted. That is, we may build our models too specific to the training data, and the model takes on the random gradations of the training data. This can cause problems when we try to generalize the model. As good practice, if your initial dataset is a large enough, we split the data into training and test data.

?
Exercises
Exercises may entail some data preparation in your Hadoop environment.
	Using the Baseball data from Chapter 3, construct a random forest model that predicts the NLCS (National League Championship Series) Winner from 1965 to 2005, based on regular season Wins/Loss ratio, Runs/Run-Against ratio, and Earned Run Average (ERA). 
	Based on actual NLCS winners, how accurate is your model based on NLCS winners from 2006 to 2015? 
	How does your model compare with the one you developed in Chapter 9?
	Download the Titanic survivor train and test data from https://www.kaggle.com/c/titanic/data, and use it to construct and test a random forest model with survival as the target outcome, where survival = 1 (yes).
 
	Clustering Models
Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, and bioinformatics.
Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances among the cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It will often be necessary to modify data preprocessing and model parameters until the result achieves the desired properties.
Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek ?????? “grape”) and typological analysis. The subtle differences are often in the usage of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest. This often leads to misunderstandings between researchers coming from the fields of data mining and machine learning, since they use the same terms and often the same algorithms, but have different goals.
Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Zubin in 1938 and Robert Tryon in 1939 (Bailey K. , 1994) (Tryon, 1939) and famously used by Cattell beginning in 1943 (Cattell, 1943) for trait theory classification in personality psychology.
Definition
According to Vladimir Estivill-Castro, the notion of a “cluster” cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms (Estivill-Castro, 2002). There is a common denominator: a group of data objects. However, different researchers employ different cluster models, and for each of these cluster models again different algorithms can be given. The notion of a cluster, as found by different algorithms, varies significantly in its properties. Understanding these “cluster models” is key to understanding the differences between the various algorithms. Typical cluster models include:
Connectivity models: for example hierarchical clustering builds models based on distance connectivity.
Centroid models: for example the k-means algorithm represents each cluster by a single mean vector.
Distribution models: clusters are modeled using statistical distributions, such as multivariate normal distributions used by the Expectation-maximization algorithm.
Density models: for example DBSCAN and OPTICS defines clusters as connected dense regions in the data space.
Subspace models: in Biclustering (also known as Co-clustering or two-mode-clustering), clusters are modeled with both cluster members and relevant attributes.
Group models: some algorithms do not provide a refined model for their results and just provide the grouping information.
Graph-based models: a clique, i.e., a subset of nodes in a graph such that every two nodes in the subset are connected by an edge can be considered as a prototypical form of cluster. Relaxations of the complete connectivity requirement (a fraction of the edges can be missing) are known as quasi-cliques.
A "clustering" is essentially a set of such clusters, usually containing all objects in the data set. Additionally, it may specify the relationship of the clusters to each other, for example a hierarchy of clusters embedded in each other. Clusterings can be roughly distinguished as:
	hard clustering: each object belongs to a cluster or not
	soft clustering (also: fuzzy clustering): each object belongs to each cluster to a certain degree (e.g. a likelihood of belonging to the cluster)
There are also finer distinctions possible, for example:
	strict partitioning clustering: here each object belongs to exactly one cluster
	strict partitioning clustering with outliers: objects can also belong to no cluster, and are considered outliers.
	overlapping clustering (also: alternative clustering, multi-view clustering): while usually a hard clustering, objects may belong to more than one cluster.
	hierarchical clustering: objects that belong to a child cluster also belong to the parent cluster
	subspace clustering: while an overlapping clustering, within a uniquely defined subspace, clusters are not expected to overlap.
Algorithms
Clustering algorithms can be categorized based on their cluster model, as listed above. The following overview will only list the most prominent examples of clustering algorithms, as there are possibly over 100 published clustering algorithms. Not all provide models for their clusters and can thus not easily be categorized. An overview of algorithms explained in Wikipedia can be found in the list of statistics algorithms.
There is no objectively “correct” clustering algorithm, but as it was noted, “clustering is in the eye of the beholder.” (Estivill-Castro, 2002) The most appropriate clustering algorithm for a particular problem often needs to be chosen experimentally, unless there is a mathematical reason to prefer one cluster model over another. It should be noted that an algorithm that is designed for one kind of model has no chance on a data set that contains a radically different kind of model. For example, k-means cannot find non-convex clusters (Estivill-Castro, 2002).
Connectivity based clustering (hierarchical clustering)
Connectivity based clustering, also known as hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. These algorithms connect “objects” to form "clusters" based on their distance. A cluster can be described largely by the maximum distance needed to connect parts of the cluster. At different distances, different clusters will form, which can be represented using a dendrogram, which explains where the common name “hierarchical clustering” comes from: these algorithms do not provide a single partitioning of the data set, but instead provide an extensive hierarchy of clusters that merge with each other at certain distances. In a dendrogram, the y-axis marks the distance at which the clusters merge, while the objects are placed along the x-axis such that the clusters don’t mix.
Strategies for hierarchical clustering generally fall into two types:
Agglomerative: This is a "bottom up" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
Divisive: This is a "top down" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.
Connectivity based clustering is a whole family of methods that differ by the way distances are computed. Apart from the usual choice of distance functions, the user also needs to decide on the linkage criterion (since a cluster consists of multiple objects, there are multiple candidates to compute the distance to) to use. Popular choices are known as single-linkage clustering (the minimum of object distances), complete linkage clustering (the maximum of object distances) or UPGMA (“Unweighted Pair Group Method with Arithmetic Mean”, also known as average linkage clustering). Furthermore, hierarchical clustering can be agglomerative (starting with single elements and aggregating them into clusters) or divisive (starting with the complete data set and dividing it into partitions).
Metric
The choice of an appropriate metric will influence the shape of the clusters, as some elements may be close to one another according to one distance and farther away according to another. For example, in a 2-dimensional space, the distance between the point (1,0) and the origin (0,0) is always 1 according to the usual norms, but the distance between the point (1,1) and the origin (0,0) can be 2 under Manhattan distance, ?2 under Euclidean distance, or 1 under maximum distance.
Some commonly used metrics for hierarchical clustering are (The DISTANCE Procedure: Proximity Measures):
Names	Formula
Euclidean distance
?a-b?_2=?(?_i?(a_i-b_i )^2 )
Squared Euclidean distance

?a-b?_2^2=?_i?(a_i-b_i )^2 
Manhattan distance	?a-b?_1=?_i?|a_i-b_i | 
maximum distance	?a-b?_?=max?i?|a_i-b_i |
Mahalanobis distance	?((a-b)^"T"  S^(-1) (a-b) ) where S is the Covariance matrix

For text or other non-numeric data, metrics such as the Hamming distance or Levenshtein distance are often used. A review of cluster analysis in health psychology research found that the most common distance measure in published studies in that research area is the Euclidean distance or the squared Euclidean distance
Linkage criteria
The linkage criterion determines the distance between sets of observations as a function of the pairwise distances between observations.
Some commonly used linkage criteria between two sets of observations A and B are (The CLUSTER Procedure: Clustering Methods):
Names	Formula
Maximum or complete linkage clustering
"max" {d(a,b):a?A,b?B}
Minimum or single-linkage clustering
"
" "min" {d(a,b):a?A,b?B}
Mean or average linkage clustering, or UPGMA
1/|A||B|  ?_(a?A)??_(b?B)?d(a,b) 
Centroid linkage clustering, or UPGMC
?c_s-c_t ? where c_s and c_t are the centroids of clusters s and t, respectively.
Minimum energy clustering
2/nm ?_(i,j=1)^(n,m)??a_i-b_j ?_2 -1/n^2  ?_(i,j=1)^n??a_i-a_j ?_2 -1/m^2  ?_(i,j=1)^m??b_i-b_j ?_2 

where d is the chosen metric. Other linkage criteria include:
	The sum of all intra-cluster variance.
	The decrease in variance for the cluster being merged (Ward's criterion) (Ward, 1963).
	The probability that candidate clusters spawn from the same distribution function (V-linkage).
	The product of in-degree and out-degree on a k-nearest-neighbor graph (graph degree linkage) (Zhang e. a., October 7–13, 2012).
	The increment of some cluster descriptor (i.e., a quantity defined for measuring the quality of a cluster) after merging two clusters (Zhang e. a., Agglomerative clustering via maximum incremental path integral, 2013).
These methods will not produce a unique partitioning of the data set, but a hierarchy from which the user still needs to choose appropriate clusters. They are not very robust towards outliers, which will either show up as additional clusters or even cause other clusters to merge (known as “chaining phenomenon”, in particular with single-linkage clustering). In the general case, the complexity is O(n^3 ), which makes them too slow for large data sets. For some special cases, optimal efficient methods (of complexity O(n^2 )) are known: SLINK (Sibson, 1973) for single-linkage and CLINK (Defays, 1977) for complete-linkage clustering. In the data mining community these methods are recognized as a theoretical foundation of cluster analysis, but often considered obsolete. They did however provide inspiration for many later methods such as density based clustering.
Examples Using R
The ‘cluster’ package provides several useful functions for clustering analysis. We will use one here called ‘agnes’, which performs agglomerative hierarchical clustering of a dataset. The dataset we will use, ‘votes.repub’ is included in the package.
## First load the package.
> library(cluster)
> data(votes.repub)
> agn1 <- agnes(votes.repub, metric = "manhattan", 
+	stand = TRUE)
agn1
Call: agnes(x = votes.repub, metric = "manhattan", stand = TRUE) 
Agglomerative coefficient:  0.7977555 
Order of objects:
 [1] Alabama        Georgia        Arkansas       Louisiana      Mississippi    South Carolina
 [7] Alaska         Vermont        Arizona        Montana        Nevada         Colorado      
[13] Idaho          Wyoming        Utah           California     Oregon         Washington    
[19] Minnesota      Connecticut    New York       New Jersey     Illinois       Ohio          
[25] Indiana        Michigan       Pennsylvania   New Hampshire  Wisconsin      Delaware      
[31] Kentucky       Maryland       Missouri       New Mexico     West Virginia  Iowa          
[37] South Dakota   North Dakota   Kansas         Nebraska       Maine          Massachusetts 
[43] Rhode Island   Florida        North Carolina Tennessee      Virginia       Oklahoma      
[49] Hawaii         Texas         
Height (summary):
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  8.382  12.800  18.530  23.120  28.410  87.460 

Available components:
[1] "order"     "height"    "ac"        "merge"     "diss"      "call"      "method"    "order.lab"
[9] "data"

> plot(agn1)
 
 

> op <- par(mfrow=c(2,2))
> agn2 <- agnes(daisy(votes.repub), diss = TRUE, 
+	method = "complete")
> plot(agn2)
## alpha = 0.625 ==> beta = -1/4 is "recommended" by some
 

> agnS <- agnes(votes.repub, method = "flexible", 
+	par.meth = 0.625)
> plot(agnS)
 

par(op)
## "show" equivalence of three "flexible" special cases
> d.vr <- daisy(votes.repub)
> a.wgt <- agnes(d.vr, method = "weighted")
> a.sing <- agnes(d.vr, method = "single")
> a.comp <- agnes(d.vr, method = "complete")
> iC <- -(6:7) # not using 'call' and 'method' for
+	comparisons
> stopifnot(all.equal(a.wgt [iC], agnes(d.vr, method="flexible",par.method = 0.5)[iC]) ,
+	all.equal(a.sing[iC], agnes(d.vr, method="flex",
+	par.method= c(.5,.5,0, -.5))[iC]),
+	all.equal(a.comp[iC], agnes(d.vr, method="flex",
+	par.method= c(.5,.5,0, +.5))[iC]))

If you choose any height along the y-axis of the dendrogram, and move across the dendrogram counting the number of lines that you cross, each line represents a group that was identified when objects were joined together into clusters. The observations in that group are represented by the branches of the dendrogram that spread out below the line. For example, if we look at a height of 60, and move across the x-axis at that height, we'll cross two lines. That defines a two-cluster solution; by following the line down through all its branches, we can see the names of the states that are included in these two clusters. Since the y-axis represents how close together observations were when they were merged into clusters, clusters whose branches are very close together (in terms of the heights at which they were merged) probably aren’t very reliable. But if there is a big difference along the y-axis between the last merged cluster and the currently merged one, which indicates that the clusters formed are probably doing a good job in showing us the structure of the data. Looking at the dendrogram for the voting data, there are (maybe not clearly) five distinct groups at the 20-level; at the 0-level there seems to be nine distinct groups. 
For this data set, it looks like either five or six groups might be an interesting place to start investigating. This is not to imply that looking at solutions with more clusters would be meaningless, but the data seems to suggest that five or six clusters might be a good start. For a problem of this size, we can see the names of the states, so we could start interpreting the results immediately from the dendrogram, but when there are larger numbers of observations, this won't be possible.



## Exploring the dendrogram structure

> (d2 <- as.dendrogram(agn2)) # two main branches
'dendrogram' with 2 branches and 50 members total, at height 281.9508

> d2[[1]] # the first branch
'dendrogram' with 2 branches and 8 members total, at height 116.7048 

> d2[[2]] # the 2nd one { 8 + 42 = 50 }
'dendrogram' with 2 branches and 42 members total, at height 178.4119 

> d2[[1]][[1]]# first sub-branch of branch 1 .. and shorter form
'dendrogram' with 2 branches and 6 members total, at height 72.92212

> identical(d2[[c(1,1)]], d2[[1]][[1]])
[1] TRUE

## a "textual picture" of the dendrogram :
str(d2)
?
--[dendrogram w/ 2 branches and 50 members at h = 282]
  |--[dendrogram w/ 2 branches and 8 members at h = 117]
  |  |--[dendrogram w/ 2 branches and 6 members at h = 72.9]
  |  |  |--[dendrogram w/ 2 branches and 3 members at h = 60.9]
  |  |  |  |--[dendrogram w/ 2 branches and 2 members at h = 48.2]
  |  |  |  |  |--leaf "Alabama" 
  |  |  |  |  `--leaf "Georgia" 
  |  |  |  `--leaf "Louisiana" 
  |  |  `--[dendrogram w/ 2 branches and 3 members at h = 58.8]
  |  |     |--[dendrogram w/ 2 branches and 2 members at h = 56.1]
  |  |     |  |--leaf "Arkansas" 
  |  |     |  `--leaf "Florida" 
  |  |     `--leaf "Texas" 
  |  `--[dendrogram w/ 2 branches and 2 members at h = 63.1]
  |     |--leaf "Mississippi" 
  |     `--leaf "South Carolina" 
  `--[dendrogram w/ 2 branches and 42 members at h = 178]
     |--[dendrogram w/ 2 branches and 37 members at h = 121]
     |  |--[dendrogram w/ 2 branches and 31 members at h = 80.5]
     |  |  |--[dendrogram w/ 2 branches and 17 members at h = 64.5]
     |  |  |  |--[dendrogram w/ 2 branches and 13 members at h = 56.4]
     |  |  |  |  |--[dendrogram w/ 2 branches and 10 members at h = 47.2]
     |  |  |  |  |  |--[dendrogram w/ 2 branches and 2 members at h = 28.1]
     |  |  |  |  |  |  |--leaf "Alaska" 
     |  |  |  |  |  |  `--leaf "Michigan" 
     |  |  |  |  |  `--[dendrogram w/ 2 branches and 8 members at h = 39.2]
     |  |  |  |  |     |--[dendrogram w/ 2 branches and 5 members at h = 36.8]
     |  |  |  |  |     |  |--[dendrogram w/ 2 branches and 3 members at h = 32.9]
     |  |  |  |  |     |  |  |--[dendrogram w/ 2 branches and 2 members at h = 19.4]
     |  |  |  |  |     |  |  |  |--leaf "Connecticut" 
     |  |  |  |  |     |  |  |  `--leaf "New York" 
     |  |  |  |  |     |  |  `--leaf "New Hampshire" 
     |  |  |  |  |     |  `--[dendrogram w/ 2 branches and 2 members at h = 20.2]
     |  |  |  |  |     |     |--leaf "Indiana" 
     |  |  |  |  |     |     `--leaf "Ohio" 
     |  |  |  |  |     `--[dendrogram w/ 2 branches and 3 members at h = 25.3]
     |  |  |  |  |        |--[dendrogram w/ 2 branches and 2 members at h = 20.9]
     |  |  |  |  |        |  |--leaf "Illinois" 
     |  |  |  |  |        |  `--leaf "New Jersey" 
     |  |  |  |  |        `--leaf "Pennsylvania" 
     |  |  |  |  `--[dendrogram w/ 2 branches and 3 members at h = 42.2]
     |  |  |  |     |--leaf "Minnesota" 
     |  |  |  |     `--[dendrogram w/ 2 branches and 2 members at h = 33.7]
     |  |  |  |        |--leaf "North Dakota" 
     |  |  |  |        `--leaf "Wisconsin" 
     |  |  |  `--[dendrogram w/ 2 branches and 4 members at h = 37.5]
     |  |  |     |--[dendrogram w/ 2 branches and 2 members at h = 26.2]
     |  |  |     |  |--leaf "Iowa" 
     |  |  |     |  `--leaf "South Dakota" 
     |  |  |     `--[dendrogram w/ 2 branches and 2 members at h = 25.9]
     |  |  |        |--leaf "Kansas" 
     |  |  |        `--leaf "Nebraska" 
     |  |  `--[dendrogram w/ 2 branches and 14 members at h = 70.5]
     |  |     |--[dendrogram w/ 2 branches and 8 members at h = 48]
     |  |     |  |--[dendrogram w/ 2 branches and 4 members at h = 43.4]
     |  |     |  |  |--[dendrogram w/ 2 branches and 3 members at h = 27.8]
     |  |     |  |  |  |--[dendrogram w/ 2 branches and 2 members at h = 23.4]
     |  |     |  |  |  |  |--leaf "Arizona" 
     |  |     |  |  |  |  `--leaf "Nevada" 
     |  |     |  |  |  `--leaf "Montana" 
     |  |     |  |  `--leaf "Oklahoma" 
     |  |     |  `--[dendrogram w/ 2 branches and 4 members at h = 43.7]
     |  |     |     |--leaf "Colorado" 
     |  |     |     `--[dendrogram w/ 2 branches and 3 members at h = 31.2]
     |  |     |        |--[dendrogram w/ 2 branches and 2 members at h = 17.2]
     |  |     |        |  |--leaf "Idaho" 
     |  |     |        |  `--leaf "Wyoming" 
     |  |     |        `--leaf "Utah" 
     |  |     `--[dendrogram w/ 2 branches and 6 members at h = 54.3]
     |  |        |--[dendrogram w/ 2 branches and 3 members at h = 33.2]
     |  |        |  |--leaf "California" 
     |  |        |  `--[dendrogram w/ 2 branches and 2 members at h = 22.2]
     |  |        |     |--leaf "Oregon" 
     |  |        |     `--leaf "Washington" 
     |  |        `--[dendrogram w/ 2 branches and 3 members at h = 35.1]
     |  |           |--[dendrogram w/ 2 branches and 2 members at h = 21.1]
     |  |           |  |--leaf "Missouri" 
     |  |           |  `--leaf "New Mexico" 
     |  |           `--leaf "West Virginia" 
     |  `--[dendrogram w/ 2 branches and 6 members at h = 66.8]
     |     |--[dendrogram w/ 2 branches and 3 members at h = 43.4]
     |     |  |--leaf "Delaware" 
     |     |  `--[dendrogram w/ 2 branches and 2 members at h = 33.5]
     |     |     |--leaf "Kentucky" 
     |     |     `--leaf "Maryland" 
     |     `--[dendrogram w/ 2 branches and 3 members at h = 30.2]
     |        |--[dendrogram w/ 2 branches and 2 members at h = 29.5]
     |        |  |--leaf "North Carolina" 
     |        |  `--leaf "Tennessee" 
     |        `--leaf "Virginia" 
     `--[dendrogram w/ 2 branches and 5 members at h = 83.1]
        |--[dendrogram w/ 2 branches and 4 members at h = 55.4]
        |  |--[dendrogram w/ 2 branches and 2 members at h = 32.8]
        |  |  |--leaf "Hawaii" 
        |  |  `--leaf "Maine" 
        |  `--[dendrogram w/ 2 branches and 2 members at h = 22.6]
        |     |--leaf "Massachusetts" 
        |     `--leaf "Rhode Island" 
        `--leaf "Vermont" 

Now, we need to interpret the results of this analysis. From the dendrogram we can see some logical clustering at the 0-level. For instance, California, Oregon and Washington are clustered together as we would expect. Also, at the 40-level Georgia, Alabama, Mississippi, Arkansas, South Carolina and Louisiana are grouped together. What other clusters make sense?
The next 
plot(agnes(agriculture), ask = TRUE)
> data(animals)
> aa.a <- agnes(animals) # default method = "average"
> aa.ga <- agnes(animals, method = "gaverage")
> op <- par(mfcol=1:2, mgp=c(1.5, 0.6, 0), 
+	mar=c(.1+ > c(4,3,2,1)),cex.main=0.8)
> plot(aa.a, which.plot = 2)
 
 

plot(agnes(agriculture), ask = TRUE)
Make a plot selection (or 0 to exit):
 
1: plot  All
2: plot  Banner
3: plot  Clustering Tree

Selection: 
Enter an item from the menu, or 0 to exit
Selection: 2

 


> plot(aa.ga, which.plot = 2)

 

> par(op)
## Show how "gaverage" is a "generalized average":
> aa.ga.0 <- agnes(animals, method = "gaverage", 
+	par.method = 0)
> stopifnot(all.equal(aa.ga.0[iC], aa.a[iC]))
> plot(aa.ga.0, which.plot=2) plot(aa.ga.0, which.plot=2)

 

We next introduce another popular clustering technique, k-means. The format of the k-means function in R is kmeans(x,centers) where x is a numeric dataset (matrix or data frame) and centers is the number of clusters to extract. The function returns the cluster memberships, centroids, sums of squares (within, between, total), and cluster sizes.
K-means cluster analysis starts with k randomly chosen centroids, a different solution can be obtained each time the function is invoked. Use the set.seed() function to guarantee that the results are reproducible. Additionally, this clustering approach can be sensitive to the initial selection of centroids. The kmeans() function has an nstart option that attempts multiple initial configurations and reports on the best one. For example, adding nstart=25 will generate 25 initial configurations. This approach is often recommended.
Unlike hierarchical clustering, k-means clustering requires that the number of clusters to extract be specified in advance. Here, a dataset containing 13 chemical measurements on 178 Italian wine samples is analyzed. The data originally come from the UCI Machine Learning Repository (http://www.ics.uci.edu/~mlearn/MLRepository.html) but we will access it via the rattle package.
> library(rattle)
> data(wine)
> head(wine)
 
  Type Alcohol Malic  Ash Alcalinity Magnesium Phenols Flavanoids
1    1   14.23  1.71 2.43       15.6       127    2.80       3.06
2    1   13.20  1.78 2.14       11.2       100    2.65       2.76
3    1   13.16  2.36 2.67       18.6       101    2.80       3.24
4    1   14.37  1.95 2.50       16.8       113    3.85       3.49
5    1   13.24  2.59 2.87       21.0       118    2.80       2.69
6    1   14.20  1.76 2.45       15.2       112    3.27       3.39

It looks like the variables are measured on different scales, so we will likely want to standardize the data before proceeding. The ‘scale’ function will do this. Additionally, a plot of the total within-groups sums of squares against the number of clusters in a k-means solution can be helpful. A bend in the graph can suggest the appropriate number of clusters. The graph can be produced by the following function. We also use the NbClust package here. NbClust package provides 30 indices for determining the number of clusters and proposes to user the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.

> df <- scale(wine[-1])
> wssplot(df)
 
> library(NbClust)
> set.seed(1234)
> nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
*** : The Hubert index is a graphical method of determining the number of clusters.
In the plot of Hubert index, we seek a significant knee that corresponds to a significant increase of the value of the measure i.e. the significant peak in Hubert index second differences plot. 
 
*** : The D index is a graphical method of determining the number of clusters. 
In the plot of D index, we seek a significant knee (the significant peak in Dindex second differences plot) that corresponds to a significant increase of the value of the measure. 
 
All 178 observations were used. 
 
*********************************************************** 
* Among all indices:                                                
* 4 proposed 2 as the best number of clusters 
* 16 proposed 3 as the best number of clusters 
* 1 proposed 11 as the best number of clusters 
* 2 proposed 15 as the best number of clusters 

                ***** Conclusion *****                            
 
* According to the majority rule, the best number of clusters is 3 
 
*********************************************************** 
 
 


> table(nc$Best.n[1,])

 0  1  2  3 11 15 
 2  1  4 16  1  2 
> barplot(table(nc$Best.n[1,]), 
+      xlab="Numer of Clusters", ylab="Number of Criteria",
+      main="Number of Clusters Chosen by 26 Criteria")
 

> set.seed(1234)
> fit.km <- kmeans(df, 3, nstart=25)

> fit.km$size
[1] 61 68 49
 
> fit.km$centers
 
           V1         V2         V3         V4         V5          V6          V7          V8
1 -1.16822514  0.8756272 -0.3037196  0.3180446 -0.6626544  0.56329925  0.87403990  0.94098462
2  0.07973544 -0.9195318 -0.3778231 -0.4643776  0.1750133 -0.46892793 -0.07372644  0.04416309
3  1.34366784  0.1860184  0.9024258  0.2485092  0.5820616 -0.05049296 -0.98577624 -1.23271740
            V9         V10        V11        V12        V13        V14
1 -0.583942581  0.58014642  0.1667181  0.4823674  0.7648958  1.1550888
2  0.008736157  0.01821349 -0.8598525  0.4233092  0.2490794 -0.7630972
3  0.714825281 -0.74749896  0.9857177 -1.1879477 -1.2978785 -0.3789756


> aggregate(wine[-1], by=list(cluster=fit.km$cluster), mean)

  cluster       V2       V3       V4       V5        V6       V7        V8        V9      V10      V11
1       1 13.71148 1.997049 2.453770 17.28197 107.78689 2.842131 2.9691803 0.2891803 1.922951 5.444590
2       2 12.25412 1.914265 2.239118 20.07941  93.04412 2.248971 2.0733824 0.3629412 1.601324 3.064706
3       3 13.15163 3.344490 2.434694 21.43878  99.02041 1.678163 0.7979592 0.4508163 1.163061 7.343265
        V12      V13       V14
1 1.0677049 3.154754 1110.6393
2 1.0542059 2.788529  506.5882
3 0.6859184 1.690204  627.5510

Two additional cluster plot may be useful in your analysis.

> clusplot(wine, fit.km$cluster, color=TRUE, shade=TRUE, 
+          labels=2, lines=0)
 

> plotcluster(wine, fit.km$cluster) 

 
?
Example Using iPython
Introduction
Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups (clusters). Clustering is a method used for exploratory data analysis, which is used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, and bioinformatics.
Clustering can be used to discover structures in data without providing an explanation or interpretation. Clustering or cluster analysis is not a specific algorithm, rather it is a task. First used by Tryon, 1939, it encompasses a number of different algorithms and methods for grouping objects of similar kind into respective categories.
An example where clustering might be used is in the field of psychiatry, where the characterization of patients on the basis of clusters of symptoms can be useful in the identification of an appropriate form of therapy. In marketing, it may be useful to identify distinct groups of potential customers so that, for example, advertising can be appropriately targeted.
Clustering Example
We will use the example in the field of psychiatry here. The Ginzberg dataset is comprised of data for psychiatric patients hospitalized for depression. It is part of the R car-package consisting of a data frame with 82 rows and 6 columns. It can be downloaded at:
	CSV Format: https://vincentarelbundock.github.io/Rdatasets/csv/car/Ginzberg.csv
	TXT FormatL http://socserv.mcmaster.ca/jfox/Books/Applied-Regression-2E/datasets/Greene.txt
Dataset
The data are for psychiatric patients hospitalized for depression.
	simplicity: Measures subject's need to see the world in black and white.
	fatalism: Fatalism scale.
	depression: Beck self-report depression scale.
	adjsimp: Adjusted Simplicity - Simplicity adjusted (by regression) for other variables thought to influence depression.
	adjfatal: Adjusted Fatalism.
	adjdep: Adjusted Depression.
Import Nodules
Before we begin writing and executing Python code, we need to ensure we have the necessary external modules loaded or we will have a frustrating day. We will use Numpy, Pandas, Scipy and Matplotlib (Whitaker, 2011). Something I will show you here that I have not in other labs is how to call a function from Numpy without the prefix np. For example, if we want to put our data into an array (and we do) we would call np.array when Numpy is loaded. I always forget the np prefix. Here we will use "from numpy import array" and then be able to just call array. We will also do this for kmeans and vq from scipy.cluster.vq.
In [1]:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from numpy import array
from scipy.cluster.vq import kmeans,vq

Load the Dataset
We will load the Ginzberg dataset using Pandos with its read_csv function. Then we will look at the shape to verify the number of rows and columns.
In [2]:
data = pd.read_csv("C:/Users/Strickland/Documents/Python Scripts/Ginzberg.csv")
print(data.shape)
(82, 6)

Configure the Data
We need to place the data from Ginzberg.csv into a np.array or simple array in order to perform clustering.
In [3]:
# put the data into an np.array with floating pint for the datatype
X = array(data, dtype='f')

Perform k-Means Clustering
We will use kmeans from scipy.clsuter.vq to perform k-Means cluster analysis.
In [4]:
# computing K-Means with K = 2 (2 clusters)
centroids,_ = kmeans(X,2)
# assign each sample to a cluster
idx,_ = vq(X,centroids)

Plotting the Clusters
We now plot the clsuters derived from kmeans. We want to plot these within the Notebook.
In [5]:
%matplotlib notebook 
# some plotting using numpy's logical indexing
plt.plot(X[idx==0,0],X[idx==0,1],'ob',
     X[idx==1,0],X[idx==1,1],'or')
plt.plot(centroids[:,0],centroids[:,1],'sg',markersize=8)
plt.show()
 
Now we use KMeans algorithm of scikit-learn to perform the clustering.
In [6]:
from sklearn.cluster import KMeans 
kmeans = KMeans(n_clusters=4, max_iter=100, verbose=1) 
fit = kmeans.fit(data)
fit
Initialization complete
Iteration  0, inertia 59.312
Iteration  1, inertia 45.691
Iteration  2, inertia 43.667
Iteration  3, inertia 42.638
Iteration  4, inertia 41.927
Iteration  5, inertia 40.452
Iteration  6, inertia 40.003
Iteration  7, inertia 39.922
Iteration  8, inertia 39.879
Converged at iteration 8
Initialization complete
Iteration  0, inertia 62.946
Iteration  1, inertia 46.804
Iteration  2, inertia 44.352
Iteration  3, inertia 42.961
Iteration  4, inertia 40.846
Iteration  5, inertia 40.060
Iteration  6, inertia 39.960
Converged at iteration 6
Initialization complete
Iteration  0, inertia 74.151
Iteration  1, inertia 41.510
Iteration  2, inertia 39.813
Iteration  3, inertia 39.582
Converged at iteration 3
Initialization complete
Iteration  0, inertia 69.728
Iteration  1, inertia 48.501
Iteration  2, inertia 42.942
Iteration  3, inertia 39.691
Iteration  4, inertia 39.581
Converged at iteration 4
Initialization complete
Iteration  0, inertia 55.485
Iteration  1, inertia 45.239
Iteration  2, inertia 44.147
Iteration  3, inertia 43.656
Converged at iteration 3
Initialization complete
Iteration  0, inertia 62.807
Iteration  1, inertia 45.101
Iteration  2, inertia 42.066
Iteration  3, inertia 40.387
Iteration  4, inertia 39.873
Converged at iteration 4
Initialization complete
Iteration  0, inertia 65.600
Iteration  1, inertia 48.717
Iteration  2, inertia 45.755
Iteration  3, inertia 42.944
Iteration  4, inertia 40.915
Iteration  5, inertia 40.044
Iteration  6, inertia 39.690
Converged at iteration 6
Initialization complete
Iteration  0, inertia 62.923
Iteration  1, inertia 43.852
Iteration  2, inertia 41.044
Iteration  3, inertia 40.593
Iteration  4, inertia 39.937
Iteration  5, inertia 39.654
Converged at iteration 5
Initialization complete
Iteration  0, inertia 58.230
Iteration  1, inertia 43.226
Iteration  2, inertia 40.636
Iteration  3, inertia 39.854
Iteration  4, inertia 39.816
Converged at iteration 4
Initialization complete
Iteration  0, inertia 60.062
Iteration  1, inertia 43.732
Iteration  2, inertia 41.498
Iteration  3, inertia 40.323
Iteration  4, inertia 39.824
Converged at iteration 4
Out[6]:
KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=4, n_init=10,
    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,
    verbose=1)
In [7]:
clusters = kmeans.predict(data)
clusters.shape
Out[7]:
(82L,)

The centroids provide an aggregate representation and a characterization of each cluster.
In [8]:
t = kmeans.cluster_centers_
print t
[[ 1.030895    1.14000964  1.32298679  0.89626929  1.0340825   1.25243143]
 [ 0.82607188  1.067885    0.60879125  0.92260688  1.19103812  0.66781938]
 [ 1.85712923  1.64644615  1.69169769  1.87569692  1.63098385  1.67284692]
 [ 0.631006    0.463592    0.5289476   0.7103472   0.5114508   0.5799924 ]]

Let's also convert the target labels to integers:
In [9]:
target = np.genfromtxt('C:/Users/Strickland/Documents/Python Scripts/Ginzberg.csv',delimiter=',',usecols=None,names=True,dtype=str)
t = np.zeros(len(target),dtype=int)
t[target == 'simplicity'] = 1
t[target == 'fatalism'] = 2
t[target == 'depression'] = 3
t[target == 'adjsimp'] = 4
t[target == 'adjfatal'] = 5
t[target == 'adjdep'] = 6
t.shape
Out[9]:
(82L,)

Homogeneity: each cluster contains only members of a single class. Completeness: all members of a given class are assigned to the same cluster.
In [10]:
from sklearn.metrics import completeness_score, homogeneity_score
print completeness_score(t,clusters)
0.0109399800015
In [11]:
print homogeneity_score(t,clusters)
0.222566710701

The completeness score approaches 1 when most of the data points that are members of a given class are elements of the same cluster while the homogeneity score approaches 1 when all the clusters contain almost only data points that are member of a single class.
In [12]:
print kmeans.cluster_centers_
[[ 1.030895    1.14000964  1.32298679  0.89626929  1.0340825   1.25243143]
 [ 0.82607188  1.067885    0.60879125  0.92260688  1.19103812  0.66781938]
 [ 1.85712923  1.64644615  1.69169769  1.87569692  1.63098385  1.67284692]
 [ 0.631006    0.463592    0.5289476   0.7103472   0.5114508   0.5799924 ]]

Now, let's look at the Iris Data set
In [13]:
data = np.genfromtxt('C:/Users/Strickland/Documents/Python Scripts/iris.csv',delimiter=',',usecols=(0,1,2,3), dtype='f')
In [14]:
data.shape
Out[14]:
(150L, 4L)
In [15]:
target = np.genfromtxt('C:/Users/Strickland/Documents/Python Scripts/iris.csv',delimiter=',',usecols=(4),dtype=str)
In [16]:
print set(target) # build a collection of unique elements
set(['setosa', 'versicolor', 'virginica'])

This snippet uses the first and the third dimension (sepal length and sepal width) and the result is shown in the following figure:
In [17]:
%matplotlib notebook 
plt.plot(data[target=='setosa',0],data[target=='setosa',2],'bo')
plt.plot(data[target=='versicolor',0],data[target=='versicolor',2],'ro')
plt.plot(data[target=='virginica',0],data[target=='virginica',2],'go')
plt.show()
 
In the graph we have 150 points and their color represents the class; the blue points represent the samples that belong to the specie setosa, the red ones represent versicolor and the green ones represent virginica. Next let's see if through clustering we can obtain the correct classes.
In [18]:
from sklearn.cluster import KMeans 
iris_kmeans = KMeans(n_clusters=3, max_iter=500, verbose=1, n_init=5) # initialization
iris_kmeans.fit(data)
Initialization complete
Iteration  0, inertia 114.560
Iteration  1, inertia 81.961
Iteration  2, inertia 79.394
Iteration  3, inertia 78.910
Iteration  4, inertia 78.851
Converged at iteration 4
Initialization complete
Iteration  0, inertia 107.740
Iteration  1, inertia 83.128
Iteration  2, inertia 82.004
Iteration  3, inertia 81.081
Iteration  4, inertia 79.874
Iteration  5, inertia 79.344
Iteration  6, inertia 78.921
Iteration  7, inertia 78.856
Converged at iteration 7
Initialization complete
Iteration  0, inertia 114.610
Iteration  1, inertia 78.851
Converged at iteration 1
Initialization complete
Iteration  0, inertia 106.370
Iteration  1, inertia 82.870
Iteration  2, inertia 81.761
Iteration  3, inertia 80.806
Iteration  4, inertia 79.874
Iteration  5, inertia 79.344
Iteration  6, inertia 78.921
Iteration  7, inertia 78.856
Converged at iteration 7
Initialization complete
Iteration  0, inertia 96.550
Iteration  1, inertia 82.607
Iteration  2, inertia 81.544
Iteration  3, inertia 80.806
Iteration  4, inertia 79.874
Iteration  5, inertia 79.344
Iteration  6, inertia 78.921
Iteration  7, inertia 78.856
Converged at iteration 7
Out[18]:
KMeans(copy_x=True, init='k-means++', max_iter=500, n_clusters=3, n_init=5,
    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,
    verbose=1)
In [19]:
c = iris_kmeans.predict(data)
c.shape
Out[19]:
(150L,)
In [20]:
tgt = kmeans.cluster_centers_
print tgt
[[ 1.030895    1.14000964  1.32298679  0.89626929  1.0340825   1.25243143]
 [ 0.82607188  1.067885    0.60879125  0.92260688  1.19103812  0.66781938]
 [ 1.85712923  1.64644615  1.69169769  1.87569692  1.63098385  1.67284692]
 [ 0.631006    0.463592    0.5289476   0.7103472   0.5114508   0.5799924 ]]

Let's also convert the target labels to integers:
In [21]:
t = np.zeros(len(target),dtype=int)
t[target == 'setosa'] = 1
t[target == 'versicolor'] = 0
t[target == 'virginica'] = 2
In [22]:
print set(target)
set(['setosa', 'versicolor', 'virginica'])
In [23]:
%matplotlib notebook 
plt.plot(data[c==1,0],data[c==1,2],'ro')
plt.plot(data[c==0,0],data[c==0,2],'bo')
plt.plot(data[c==2,0],data[c==2,2],'go')
plt.show()
 
In [24]:
from pylab import plot,show
from numpy import vstack,array
from numpy.random import rand
from scipy.cluster.vq import kmeans,vq
%matplotlib notebook
# data generation
data = vstack((rand(150,3)+array([.5,.5,.5]),rand(150,3)))

# computing K-Means with K = 2 (2 clusters)
centroids,_ = kmeans(data,3)
# assign each sample to a cluster
idx,_ = vq(data,centroids)

# some plotting using numpy's logical indexing
plot(data[idx==0,0],data[idx==0,1],'ob',
     data[idx==1,0],data[idx==1,1],'or')
plot(centroids[:,0],centroids[:,1],'sg',markersize=8)
show()
 
In [25]:
X = array(data, dtype='f')
X
Out[25]:
array([[ 0.7606225 ,  1.00396752,  0.91713339],
       [ 0.86362851,  1.49719131,  1.29835129],
       [ 1.0373323 ,  0.68821973,  1.2527262 ],
       [ 1.20430529,  0.54477304,  0.57451302],
       [ 0.62177759,  0.97743171,  0.9849503 ],
       [ 1.44316018,  1.1278609 ,  0.81270504],
       [ 0.54448342,  1.25596237,  0.75031245],
       [ 0.98808038,  0.56099194,  1.21160471],
       [ 0.57638019,  1.19905102,  1.10228312],
       [ 0.97257513,  1.01299822,  1.09131849],
…
       [ 0.0722592 ,  0.47804308,  0.73814231],
       [ 0.75996631,  0.46455863,  0.05692657],
       [ 0.61511505,  0.31706649,  0.93511796],
       [ 0.85528266,  0.03656619,  0.30144855],
       [ 0.10326847,  0.88172561,  0.43077478]], dtype=float32)

?
Exercises
	In the R data set mtcars, run the distance matrix with hclust, and plot a dendrogram that displays a hierarchical relationship among the vehicles.
	Considering the dataset SensorFiles.zip (HVAC and Building data) from Chapter 4,
	Create a cluster model for hvac system Cost Base Optimization (CBO)
	Create a cluster model that displays a hierarchical relationship among hvac systems
	Considering the Baseball Data from Chapter 3,
	Create a cluster models that shows the relationship between team ERA and team Wins.
	Create a cluster model that shows the relationship between team Runs Against and team Wins.?
Support vector machines
In machine learning, support vector machines (SVMs, also support vector networks (Cortes & Vapnik, 1995)) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
Definition
More formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training data point of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.
Whereas the original problem may be stated in a finite dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function K(x,y) selected to suit the problem (Press, Teukolsky, Vetterling, & Flannery, 2007). The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters ?_i of images of feature vectors that occur in the data base. With this choice of a hyperplane, the points in the feature space that are mapped into the hyperplane are defined by the relation: ?_i???_i K(x_i,x)="constant" ?. Note that if K(x,y) becomes small as grows further away from, each term in the sum measures the degree of closeness of the test point to the corresponding data base point x_i. In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. Note the fact that the set of points mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets which are not convex at all in the original space.
History
The original SVM algorithm was invented by Vladimir N. Vapnik and the current standard incarnation (soft margin) was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995 (Cortes & Vapnik, 1995).
Motivation
Classifying data is a common task in machine learning. Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in (see Figure 12-1). In the case of support vector machines, a data point is viewed as a p-dimensional vector (a list of p numbers), and we want to know whether we can separate such points with a (p-1)-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier; or equivalently, the perceptron of optimal stability. 
 
Figure 12-1. H_1 does not separate the classes. H_2 does, but small margin. H_3 separates them with the maximum margin.
Linear SVM
Given some training data D a set of n points of the form only with a 
D={(x_i,y_i )?|x_i?R^p,y_i?{-1,1}?}_(i=1)^n
where the y_i is either 1 or ?1, indicating the class to which the point x_i belongs. Each x_i is a p-dimensional real vector. We want to find the maximum-margin hyperplane that divides the points having y_i=1 from those having y_i=-1. Any hyperplane can be written as the set of points x satisfying
w?x-b=0,
where ? denotes the dot product and w the (not necessarily normalized) normal vector to the hyperplane. The parameter b/?w?  determines the offset of the from the origin along the normal vector w.
If the training data are linearly separable, we can select two hyperplanes in a way that they separate the data and there are no points between them, and then try to maximize their distance. The region bounded by them is called “the margin” (see Figure 12-2). These hyperplanes can be described by the equations
w?x-b=1,
and
w?x-b=-1,
 
Figure 12-2. Maximum-margin hyperplane and margins for an SVM trained with samples from two classes. Samples on the margin are called the support vectors.
By using geometry, we find the distance between these two hyperplanes is 2/?w? , so we want to minimize ?w?. As we also have to prevent data points from falling into the margin, we add the following constraint: for each i either
w?x-b?1," for" x_i  "of the first class",
or
This can be rewritten as:
w?x-b?1," for" x_i  "of the second class." 
This can be written as:
	y_i (w?x_i-b)?1," for all " 1?i?n.  	(1)
We can put this together to get the optimization problem: 
Minimize (in w,b)
?w?
subject to (for any i=1,…,n )
y_i (w?x_i-b)?1.
Primal form
The optimization problem presented in the preceding section is difficult to solve because it depends on ||w||, the norm of w, which involves a square root. Fortunately it is possible to alter the equation by substituting ?w? with 1/2 ?w?^2 (the factor of 1/2 being used for mathematical convenience) without changing the solution (the minimum of the original and the modified equation have the same w and b). This is a quadratic programming optimization problem. More clearly:
arg?min?((w,b) )??1/2 ?w?^2 ? ,
subject to (for any i=1,…,n)
y_i (w?x_i-b)?1.
By introducing Lagrange multipliers ?, the previous constrained problem can be expressed as
arg?min?(w,b)?max?(??0)?{1/2 ?w?^2-?_(i=1)^n???_i [y_i (w?x_i-b)-1] ?}  ,
that is, we look for a saddle point. In doing so all the points which can be separated as y_i (w?x_i-b)>1 do not matter since we must set the corresponding ?_i to zero.
This problem can now be solved by standard quadratic programming techniques and programs. The “stationary” Karush–Kuhn–Tucker condition implies that the solution can be expressed as a linear combination of the training vectors
w=?_(i=1)^n???_i y_i x_i ?.
Only a few ?_i will be greater than zero. The corresponding x_i are exactly the support vectors, which lie on the margin and satisfy 
y_i (w?x_i-b)=1. From this one can derive that the support vectors also satisfy
(w?x_i-b=1)?(y_i=y_i?b=w?x_i-y_i ),
which allows one to define the offset b. In practice, it is more robust to average over all support vectors:
b=1/N_SV  ?_(i=1)^(N_SV)??(w?x_i-y_i ).?
Dual form
Writing the classification rule in its unconstrained dual form reveals that the maximum-margin hyperplane and therefore the classification task is only a function of the support vectors, the subset of the training data that lie on the margin.
Using the fact that ?w?^2=w?w and substituting 
w=?_(i=1)^n???_i y_i x_i ?, one can show that the dual of the SVM reduces to the following optimization problem: Maximize (in ?_i)
L ?(?)=?_(i=1)^n??_i -1/2 ?_(i,j)???_i ?_j y_i y_j x_i^T x_j ?=?_(i=1)^n??_i -1/2 ?_(i,j)???_i ?_j y_i y_j k(x_i x_j ) ?,
subject to (for any i=1,…,n)
?_i?0,
and to the constraint from the minimization in b
?_(i=1)^n???_i y_i=0?.
Here the kernel is defined by k(x_i,x_j )=x_i?x_j. W can be computed thanks to the ?	terms:
w=?_i???_i y_i x_i ?.
Biased and unbiased hyperplanes
For simplicity reasons, sometimes it is required that the hyperplane pass through the origin of the coordinate system. Such hyperplanes are called unbiased, whereas general hyperplanes not necessarily passing through the origin are called biased. An unbiased hyperplane can be enforced by setting b=0 in the primal optimization problem. The corresponding dual is identical to the dual given above without the equality constraint
?_(i=1)^n???_i y_i=0?.
Soft margin
In 1995, Corinna Cortes and Vladimir N. Vapnik suggested a modified maximum margin idea that allows for mislabeled examples (Cortes & Vapnik, 1995). If there exists no hyperplane that can split the “yes” and “no” examples, the Soft Margin method will choose a hyperplane that splits the examples as cleanly as possible, while still maximizing the distance to the nearest cleanly split examples. The method introduces non-negative slack variables, ?_i, which measure the degree of misclassification of the data x_i
	y_i (w?x_i-b)?1-?_i,1?i?n.	(2)
The objective function is then increased by a function which penalizes non-zero ?_i, and the optimization becomes a trade-off between a large margin and a small error penalty. If the penalty function is linear, the optimization problem becomes:
arg?min?(w,?,b)?{1/2 ?w?^2+C?_(i=1)^n??_i } ,
subject to (for any i=1,…,n)
y_i (w?x_i-b)?1-?_i,?_i?0.
This constraint along with the objective of minimizing ?w? can be solved using Lagrange multipliers as done above. One has then to solve the following problem:
arg?min?(w,?,b)?max?(?,?)?{1/2 ?w?^2+C?_(i=1)^n??_i -?_(i=1)^n???_i [y_i (w?x_i-b)-1+?_i ]-?_(i=1)^n???_i ?_i ??}  ,
with ?_i,?_i?0. .
Dual form
Maximize (in ?_i)
L ?(?)=?_(i=1)^n??_i -1/2 ?_(i,j)???_i ?_j y_i y_j k(x_i x_j ) ?,
subject to (for any i=1,…,n)
0??_i?C,
and
?_(i=1)^n???_i y_i=0?.
The key advantage of a linear penalty function is that the slack variables vanish from the dual problem, with the constant C appearing only as an additional constraint on the Lagrange multipliers. For the above formulation and its huge impact in practice, Cortes and Vapnik received the 2008 ACM Paris Kanellakis Award. Nonlinear penalty functions have been used, particularly to reduce the effect of outliers on the classifier, but unless care is taken the problem becomes non-convex, and thus it is considerably more difficult to find a global solution.
Nonlinear classification
The original optimal hyperplane algorithm proposed by Vapnik in 1963 was a linear classifier. However, in 1992, Bernhard E. Boser, Isabelle M. Guyon and Vladimir N. Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al. (Aizerman, Braverman, & Rozonoer, 1964)) to maximum-margin hyperplanes (Boser, Guyon, & Vapnik, 1992). The resulting algorithm is formally similar, except that every dot product is replaced by a nonlinear kernel function. This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high dimensional; thus though the classifier is a hyperplane in the high-dimensional feature space, it may be nonlinear in the original input space (see Figure 12-3).
 
Figure 12-3. Kernel machine
If the kernel used is a Gaussian radial basis function, the corresponding feature space is a Hilbert space of infinite dimensions. Maximum margin classifiers are well regularized, so the infinite dimensions do not spoil the results. Some common kernels include:
	Polynomial (homogeneous): k(x_i,x_j )=(x_i?x_j )^d
	Polynomial (inhomogeneous): k(x_i,x_j )=(x_i?x_j+1)^d
	Gaussian radial basis function: k(x_i,x_j )="exp" (-??x_i-x_j ?^2 ) , for ?>0. Sometimes parametrized using ?=1?(2?^2 )
	Hyperbolic tangent:, k(x_i,x_j )=tanh?(?x_i?x_j+c) for some (not every) ?>0 and c>0.
The kernel is related to the transform ?(x_i ) by the equation k(x_i,x_j )=?(x_i )??(x_j ). The value w is also in the transformed space, with w=?_i???_i y_i ?(x_i ) ?. Dot products with w for classification can again be computed by the kernel trick, i.e. w??(x_i )=?_i???_i y_i k(x_i,x) ? . However, there does not in general exist a value w’ such that w??(x_i )=k(w^',w) .
Properties
SVMs belong to a family of generalized linear classifiers and can be interpreted as an extension of the perceptron. They can also be considered a special case of Tikhonov regularization. A special property is that they simultaneously minimize the empirical classification error and maximize the geometric margin; hence they are also known as maximum margin classifiers.
A comparison of the SVM to other classifiers has been made by Meyer, Leisch and Hornik (Meyer, Leisch, & Hornik, 2003).
Parameter selection
The effectiveness of SVM depends on the selection of kernel, the kernel’s parameters, and soft margin parameter C. A common choice is a Gaussian kernel, which has a single parameter ?. The best combination of C and ? is often selected by  a  grid  search with  exponentially growing  sequences  of  C  and  ?,  for  example, C?{2^(-5),2^(-3),…,2^13,2^15 }; ??{2^(-15),2^(-13),…,2^1,2^3 }. Typically, each combination of parameter choices is checked using cross validation, and the parameters with best cross-validation accuracy are picked. The final model, which is used for testing and for classifying new data, is then trained on the whole training set using the selected parameters  (Hsu, Chang, & Lin, 2010).
Issues
Potential drawbacks of the SVM are the following three aspects:
	Uncalibrated class membership probabilities
	The SVM is only directly applicable for two-class tasks. Therefore, algorithms that reduce the multi-class task to several binary problems have to be applied; see the multi-class SVM section.
	Parameters of a solved model are difficult to interpret.
Extensions
Multiclass SVM
Multiclass SVM aims to assign labels to instances by using support vector machines, where the labels are drawn from a finite set of several elements.
The dominant approach for doing so is to reduce the single multiclass problem into multiple binary classification problems (Duan & Keerthi, 2005). Common methods for such reduction include (Hsu, Chang, & Lin, 2010):
	Building binary classifiers which distinguish between (i) one of the labels and the rest (one-versus-all) or (ii) between every pair of classes (one-versus-one). Classification of new instances for the one-versus-all case is done by a winner-takes-all strategy, in which the classifier with the highest output function assigns the class (it is important that the output functions be calibrated to produce comparable scores). For the one-versus-one approach, classification is done by a max-wins voting strategy, in which every classifier assigns the instance to one of the two classes, then the vote for the assigned class is increased by one vote, and finally the class with the most votes determines the instance classification.
	Directed acyclic graph SVM (DAGSVM) (Platt, Cristianini, & Shawe-Taylor, 2000).
	Error-correcting output codes (Dietterich & Bakiri, 1995).
Crammer and Singer proposed a multiclass SVM method which casts the multiclass classification problem into a single optimization problem, rather than decomposing it into multiple binary classification problems (Crammer & Singer, 2001). See also Lee, Lin and Wahba (Lee, Lin, & Wahba, 2001).
Transductive support vector machines
Transductive support vector machines extend SVMs in that they could also treat partially labeled data in semi-supervised learning by following the principles of transduction (Joachims, 1999). Here, in addition to the training set D, the learner is also given a set
D^*={x_i^*?x_i^*?R^p }_(i=1)^k
of test examples to be classified. Formally, a transductive support vector machine is defined by the following primal optimization problem:[2]
Minimize (in w,b,y^*)
1/2 ?w?^2
subject to (for any  i=1,…,n and anyj=1,…,k)
y_i (w?x_i-b)?1,
y_j^* (w?x_j^*-b)?1,
and
y_j^*?{-1,1}.
Transductive support vector machines were introduced by Vladimir N. Vapnik in 1998.
Structured SVM
SVMs have been generalized to structured SVMs, where the label space is structured and of possibly infinite size.
Regression
A version of SVM for regression was proposed in 1996 by Vladimir N. Vapnik, Harris Drucker, Christopher J. C. Burges, Linda Kaufman and Alexander J. Smola (Drucker, Burges, Kaufman, Smola, & Vapnik, 1996). This method is called support vector regression (SVR). The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by SVR depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction (within a threshold). Another SVM version known as least squares support vector machine (LS-SVM) has been proposed by Suykens and Vandewalle (Suykens & Vandewalle, 1999).
Interpreting SVM models
The SVM algorithm has been widely applied in the biological and other sciences. Permutation tests based on SVM weights have been suggested as a mechanism for interpretation of SVM models (Cuingnet, et al., 2011). Support vector machine weights have also been used to interpret SVM models in the past. Posthoc interpretation of support vector machine models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences.
Implementation
The parameters of the maximum-margin hyperplane are derived by solving the optimization. There exist several specialized algorithms for quickly solving the QP problem that arises from SVMs, mostly relying on heuristics for breaking the problem down into smaller, more-manageable chunks. A common method is Platt’s sequential minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that may be solved analytically, eliminating the need for a numerical optimization algorithm (Platt J. , 1999).
Another approach is to use an interior point method that uses Newton-like iterations to find a solution of the Karush–Kuhn–Tucker conditions of the primal and dual problems (Ferris & Munson, 2002). Instead of solving a sequence of broken down problems, this approach directly solves the problem as a whole. To avoid solving a linear system involving the large kernel matrix, a low rank approximation to the matrix is often used in the kernel trick.
The special case of linear support vector machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS (Shalev-Shwartz, Singer, & Srebro, 2007)) and coordinate descent (e.g., LIBLINEAR (Fan, Chang, Hsieh, Wang, & C.J., 2008)). The general kernel SVMs can also be solved more efficiently using sub-gradient descent (e.g. P-packSVM (Zhu, Chen, Wang, Zhu, & Chen, 2009)), especially when parallelization is allowed.
Kernel SVMs are available in many machine learning toolkits, including LIBSVM, MATLAB, SVMlight, scikit-learn, Shogun, Weka, Shark [8], JKernelMachines [9] and others.
Applications
SVMs can be used to solve various real world problems:
	SVMs are helpful in text and hypertext categorization as their application can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.
	Classification of images can also be performed using SVMs. Experimental results show that SVMs achieve significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback.
	SVMs are also useful in medical science to classify proteins with up to 90% of the compounds classified correctly.
	Hand-written characters can be recognized using SVM
Pros and Cons associated with SVM
Pros:
	It works really well with clear margin of separation
	It is effective in high dimensional spaces.
	It is effective in cases where number of dimensions is greater than the number of samples.
	It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.
Cons:
	It doesn’t perform well, when we have large data set because the required training time is higher
	It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping
	SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. It is related SVC method of Python scikit-learn library.
Software
Software for SVM modeling include Gini-SVM in Octave, kernlab in R, SVMlight, SPSS Modeler, and MATLAB.
?
Example Using R
Ksvm in kernlab
Package kernlab (Karatzoglou, Smola, Hornik, & Zeileis, 2004) aims to provide the R user with basic kernel functionality (e.g., like computing a kernel matrix using a particular kernel), along with some utility functions commonly used in kernel-based methods like a quadratic programming solver, and modern kernel-based algorithms based on the functionality that the package provides.  It also takes advantage of the inherent modularity of kernel-based methods, aiming to allow the user to switch between kernels on an existing algorithm and even create and use own kernel functions for the various kernel methods provided in the package.
kernlab uses R’s new object model described in “Programming with Data” (Chambers, 1998) which is known as the S4 class system  and is implemented in package methods.  In contrast to the older S3 model for objects in R, classes, slots, and methods relationships must be declared explicitly when using the S4 system. The number and types of slots in an instance of a class have to be established at the time the class is defined. The objects from the class are validated against this definition and have to comply with it at any time.  S4 also requires formal declarations of methods, unlike the informal system of using function names to identify a certain method in S3. Package kernlab is available from CRAN (http://CRAN.R-project. org/) under the GPL license.
The ksvm() function, kernlab’s implementation of SVMs, provides a standard formula interface along with  a matrix  interface.   ksvm()  is  mostly programmed in R but  uses,  through the .Call interface, the optimizers found in bsvm and libsvm (Chang & Lin, 2011) which provide  a very efficient  C++  version of the  Sequential  Minimization  Optimization  (SMO). The SMO algorithm solves the SVM quadratic problem (QP) without using any numerical QP optimization steps. Instead, it chooses to solve the smallest possible optimization problem involving two elements of ?_1 because the must obey one linear equality constraint.  At every step, SMO chooses two ?_1 to jointly optimize and finds the optimal values for these ?_1 analytically, thus avoiding numerical QP optimization, and updates the SVM to reflect the new optimal values.
The SVM implementations available in ksvm() include the C-SVM classification algorithm along with the ?-SVM classification. Also included is a bound constraint version of C classification (C-BSVM) which solves a slightly different QP problem ( (Mangasarian & Musicant, 1999), including the offset ? in the objective function) using a modified version of the TRON (Lin & More, 1999)  optimization software. For regression, ksvm() includes the E-SVM regression algorithm along with the ?-SVM regression formulation. In addition, a bound constraint version (E-BSVM) is provided, and novelty detection (one-class classification) is supported.
For classification problems which include more than two classes (multi-class case) two options are available: a one-against-one (pairwise) classification method or the native multi-class formulation of the SVM (spocsvc). The optimization problem of the native multi-class SVM implementation is solved by a decomposition method proposed in Hsu and Lin (Hsu & Lin, 2002) where optimal working sets are found (that is, sets of ?_1 values which have a high probability of being non-zero). The QP sub-problems are then solved by a modified version of the TRON optimization software.
The ksvm() implementation can also compute class-probability output by using Platt’s probability methods along with the multi-class extension of the method in Wu et al. (Wu, Lin, & Weng, 2003). The prediction method can also return the raw decision values of the support vector model:
> library(“kernlab”)
> data(“iris”)
> irismodel <- ksvm(Species ~ ., data = iris,
+ type = “C-bsvc”, kernel = “rbfdot”,
+ kpar = list(sigma = 0.1), C = 10,
+ prob.model = TRUE)
> irismodel
Support Vector Machine object of class “ksvm” 

SV type: C-bsvc  (classification) 
 parameter : cost C = 10 

Gaussian Radial Basis kernel function. 
 Hyperparameter : sigma =  0.1 

Number of Support Vectors : 32 

Objective Function Value : -5.8442 -3.0652 -136.9786 
Training error : 0.02 
Probability model included. 

> predict(irismodel, iris[c(3, 10, 56, 68, 107, 120), -5]
+	, type = “probabilities”)
          setosa  versicolor   virginica
[1,] 0.985040508 0.007941886 0.007017606
[2,] 0.981634222 0.010962854 0.007402924
[3,] 0.004004929 0.970726183 0.025268888
[4,] 0.007777874 0.990465191 0.001756935
[5,] 0.012370962 0.103044282 0.884584756
[6,] 0.010869688 0.205778364 0.783351948

> predict(irismodel, iris[c(3, 10, 56, 68, 107, 120), -5]
+	, type = “decision”)
           [,1]      [,2]       [,3]
[1,] -1.460398 -1.1910251 -3.8868836
[2,] -1.357355 -1.1749491 -4.2107843
[3,]  1.647272  0.7655001 -1.3205306
[4,]  1.412721  0.4736201 -2.7521640
[5,]  1.844763  1.0000000  1.0000019
[6,]  1.848985  1.0069010  0.6742889

ksvm allows for the use of any valid user defined kernel function by just defining a function which takes two vector arguments and returns its Hilbert Space dot product in scalar form.
> k <- function(x, y) { (sum(x * y) + 1) * exp(0.001 
+	* sum((x - y)^2))}
> class(k) <- “kernel“
> data(“promotergene”)
> gene <- ksvm(Class ~ ., data = promotergene,
+ kernel = k, C = 10, cross = 5)
> gene
Support Vector Machine object of class “ksvm” 

SV type: C-svc  (classification) 
 parameter : cost C = 10 

Number of Support Vectors : 44 

Objective Function Value : -23.3052 
Training error : 0.084906 
Cross validation error : 0.151082

The implementation also includes the following computationally efficiently implemented kernels: Gaussian RBF, polynomial, linear, sigmoid, Laplace, Bessel RBF, spline, and ANOVA RBF.
N -fold cross-validation of an SVM model is also supported by ksvm, and the training error is reported by default.
The problem of model selection is partially addressed by an empirical observation for the popular Gaussian RBF kernel (Caputo, Sim, Furesjo, & Smola), where the optimal values of the width hyper-parameter ? are shown to lie in between the 0.1 and 0.9 quantile of the   ?x-x'?^2 statistics. The sigest() function uses a sample of the training set to estimate the quantiles and returns a vector containing the values of the quantiles.  Pretty much any value within this interval leads to good performance.
The object returned by the ksvm() function is an S4 object of class ksvm with slots containing the  coefficients  of the  model (support  vectors), the  parameters  used (C , ?, etc.), test  and cross-validation error, the kernel function, information on the problem type, the data scaling parameters, etc. There are accessor functions for the information contained in the slots of the ksvm object.
The decision values of binary classification problems can also be visualized via a contour plot with the plot() method for the ksvm objects.  This function is mainly for simple problems. An example is shown in Figure 12-4.
> x <- rbind(matrix(rnorm(120), , 2), matrix(rnorm(120,
+ mean = 3), , 2))
> y <- matrix(c(rep(1, 60), rep(-1, 60)))
> svp <- ksvm(x, y, type = “C-svc”, kernel = “rbfdot”,
+ kpar = list(sigma = 2))
> plot(svp)
 
Figure 12-4: A contour plot of the fitted decision values for a simple binary classification problem.
svm in e1071
Package e1071 provides an interface to libsvm (Chang and Lin 2001, current version: 2.8 (Chang & Lin, 2011)), complemented by visualization and tuning functions. libsvm is a fast and easy-to-use implementation of the most popular SVM formulations (C and ? classification, E and ? regression, and novelty detection). It includes the most common kernels (linear, polynomial, RBF, and sigmoid), only extensible by changing the C++ source code of libsvm.  Multi-class classification is provided using the one-against-one voting scheme. Other features include the computation of decision and probability values for predictions (for both classification and regression), shrinking heuristics during the fitting process, class weighting in the classification mode, handling of sparse data, and the computation of the training error using cross-validation. libsvm is distributed under a very permissive, BSD-like license.
The R implementation is based on the S3 class mechanisms.  It basically provides a training function with standard and formula interfaces, and a predict()  method.  In addition, a plot() method visualizing data, support vectors, and decision boundaries if provided. Hyper- parameter tuning is done using the tune() framework in e1071 performing a grid search over specified parameter ranges.
The sample session starts with a C classification task on the iris data, using the radial basis function kernel with fixed hyperparameters C and ?:
> library(“e1071”)
> model  <-  svm(Species  ~ ., data  = iris_train,
+        method  = “C-classification”,  kernel  = “radial”,
+        cost  = 10,  gamma  = 0.1)
> summary(model)

Call:
 
svm(formula  = Species  ~ ., data  = iris_train,  method  = +	“C-classification”, kernel  = “radial”,  cost  = 10,  +	gamma  = 0.1)

Parameters:
SVM-Type:	C-classification
SVM-Kernel:		radial cost:	10 gamma:		0.1

Number  of  Support  Vectors:	27 ( 12 12 3 )

Number  of  Classes:	3

Levels:
setosa  versicolor  virginica

We can visualize a 2-dimensional projection of the data with highlighting classes and support vectors (see Figure 12-5.)
> plot(model,  iris_train,  Petal.Width  ~  Petal.Length
+    ,  slice  = list(Sepal.Width  = 3, Sepal.Length  = 4))

Predictions from the model, as well as decision values from the binary classifiers, are obtained using the predict() method:
> (pred  <-  predict(model,  head(iris),  decision.values  = TRUE))

[1]  setosa  setosa  setosa  setosa  setosa  setosa
Levels:  setosa  versicolor  virginica

> attr(pred,  “decision.values”)

	setosa/versicolor	setosa/virginica	versicolor/virginica
1	1.401032	1.185293	4.242854
2	1.27702	1.149088	4.240565
3	1.466205	1.191251	3.904643
4	1.376632	1.149488	3.799572
5	1.437044	1.191163	3.953461
6	1.168084	1.091338	3.86014
Levels: setosa versicolor virginica
 
Figure 12-5:  SVM plot visualizing the iris data.  Support vectors are shown  as ‘X’,  true classes are  highlighted through  symbol   color,  predicted class  regions  are  visualized  using  colored background.
Probability values can be obtained in a similar way.
Example Using Python
In this example we use the famous “iris” dataset, which contains the petal lengths, petal width, sepal length, an sepal width of three species of iris.
The first thing we do is import the packages we will need.
In [1]:
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
Import Data
Next, we import the iris data
In [2]:
# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2] # we only take the first two features. # We could avoid this ugly slicing by using a two-dim dataset
y = iris.target
Model the Data
Next, we create an instance of SVM and fit out data. We do not scale our data since we want to plot the support vector.
In [3]:
# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0 # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=1,gamma=0).fit(X, y)
Creat a Mesh and Plot
Before we plot the SVM, we want to create a mesh to plot in. Then we create the plot (see Figure 12-6).
In [30]:
h=.01
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
 np.arange(y_min, y_max, h))
In [31]:
%matplotlib inline
plt.subplot(1, 1, 1)
Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.title('SVC with linear kernel')
plt.show()
 
Figure 12-6. SVM with linear kernel of the iris data
Now, we redefine the SVM using a Radial Basis Function (RBF) and plot the result (see Figure 12-7).
In [32]:
svc = svm.SVC(kernel='rbf', C=1,gamma=0).fit(X, y)
In [33]:
%matplotlib inline
plt.subplot(1, 1, 1)
Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.title('SVC with linear kernel')
plt.show()
 
Figure 12-7. SVM with RBF kernal with parametes C=1 an gamma=0
Now, we redefine the SVM by changing the value of gamma from 0, to 10 and 100. Then we plot and compare our results (see figures 12-8 and 12-9).
In [34]:
svc = svm.SVC(kernel='rbf', C=1,gamma=10).fit(X, y)
In [35]:
%matplotlib inline
plt.subplot(1, 1, 1)
Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.title('SVC with linear kernel')
plt.show()
 
Figure 12-8. SVM with RBF kernal with parametes C=1 an gamma=10
In [36]:
svc = svm.SVC(kernel='rbf', C=1,gamma=100).fit(X, y)
In [37]:
%matplotlib inline
plt.subplot(1, 1, 1)
Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.title('SVC with linear kernel')
plt.show()
 
Figure 12-9. SVM with RBF kernal with parametes C=1 an gamma=100
Now we redefine our SVM by changing C to 10 and 100. Then we plot and compare the results (see figures 12-10 and 12-11).
In [44]:
svc = svm.SVC(kernel='rbf', C=10,gamma=0).fit(X, y)
C:\Anaconda\lib\site-packages\sklearn\svm\base.py:85: DeprecationWarning: gamma=0.0 has been deprecated in favor of gamma='auto' as of 0.17. Backward compatibility for gamma=0.0 will be removed in 0.18
  DeprecationWarning)
In [45]:
%matplotlib inline
plt.subplot(1, 1, 1)
Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.title('SVC with linear kernel')
plt.show()
 
Figure 12-10. SVM with RBF kernal with parametes C=10 an gamma=0
In [40]:
svc = svm.SVC(kernel='rbf', C=100,gamma=0).fit(X, y)
In [41]:
%matplotlib inline
plt.subplot(1, 1, 1)
Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.title('SVC with linear kernel')
plt.show()
 
Figure 12-7. SVM with RBF kernal with parametes C=100 an gamma=0


?
Exercises
	Hill, LaPan, Li and Haney (2007) develop models to predict which cells in a high content screen were well segmented. The data consists of 119 imaging measurements on 2019. The original analysis used 1009 for training and 1010 as a test set (see the column called Case).
The outcome class is contained in a factor variable called Class with levels "PS" for poorly segmented and "WS" for well segmented.
The raw data used in the paper can be found at the Biomedcentral website. Versions of caret < 4.98 contained the original data. The version now contained in segmentationData is modified. First, several discrete versions of some of the predictors (with the suffix "Status") were removed. Second, there are several skewed predictors with minimum values of zero (that would benefit from some transformation, such as the log). A constant value of 1 was added to these fields: AvgIntenCh2, FiberAlign2Ch3, FiberAlign2Ch4, SpotFiberCountCh4 and TotalIntenCh2.
A binary version of the original data is at http://topepo.github.io/caret/segmentationOriginal.RData.
	Using the  caret-package createDataPartition() function to produce training and test data sets and summarize the data. (The dplyr –package is use by caret.)
	Using the kernlab-package, construct a support vector machine.
	Using the pROC-package, plot ROC ccurves for your SVM
	Using the x and y as defined below, generate an C-cllassification SVM using the kernlab-package and plot the results.
	x <- rbind(matrix(rnorm(120), , 2), matrix(rnorm(120, mean = 3), , 2))
	y <- matrix(c(rep(1, 60), rep(-1, 60)))?
Gradient boosting
Gradient boosting is a machine learning technique for regression problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. The gradient boosting method can also be used for classification problems by reducing them to regression with a suitable loss function.
The method was invented by Jerome H. Friedman in 1999 and was published in a series of two papers, the first of which (Friedman, Greedy Function Approximation: A Gradient Boosting Machine, 1995) introduced the method, and the second one (Friedman, Stochastic Gradient Boosting, 1999) described an important tweak to the algorithm, which improves its accuracy and performance.
Algorithm
In many supervised learning problems one has an output variable y and a vector of input variables x connected together via a joint probability distribution P(x,y). Using a training set of known values of x and corresponding values of y, the goal is to find an approximation F ?(x) to a function F^* (x) that minimizes the expected value of some specified loss function L(y,F(x)):
F^*=arg?min?F??E_(x,y) [L(y,F(x))]? 
Gradient boosting method assumes a real-valued y and seeks an approximation F ?(x) in the form of a weighted sum of functions h?(x) from some class H, called base (or weak) learners:
F(x)=?_(i=1)^M???_i h_i (x)+"constant" ?.
In accordance with the empirical risk minimization principle, the method tries to find an approximation F ?(x) that minimizes the average value of the loss function on the training set. It does so by starting with a model, consisting of a constant function F_0 (x), and incrementally expanding it in a greedy fashion:
F_0 (x)=(arg min)????_(i=1)^n?L(y_i,?) ,
F_m (x)=F_(m-1) (x)+(arg min)?(f?H)??_(i=1)^n??L(y_i,F_(m-1) (x_i )+f(x_i )),?
where f is restricted to be a function from the class H of base learner functions.
However, the problem of choosing at each step the best f for an arbitrary loss function L is a hard optimization problem in general, and so we will “cheat” by solving a much easier problem instead.
The idea is to apply a steepest descent step to this minimization problem. If we only cared about predictions at the points of the training set, and f were unrestricted, we’d update the model per the following equation, where we view L(y,f) not as a functional of f, but as a function of a vector of values f(x_1 ),…,f(x_n ):
F_m (x)=F_(m-1) (x)-?_m ?_(i=1)^n???_f L? (y_i,F_(m-1) (x_i )),
?_m=(arg min)????_(i=1)^n?L(y_i,F_(m-1) (x_i )-? ?L(y_i,F_(m-1) (x_i ))/?f(x_i ) ) .
But as f must come from a restricted class of functions (that’s what allows us to generalize), we will just choose the one that most closely approximates the gradient of L. Having chosen f, the multiplier ? is then selected using line search just as shown in the second equation above.
In pseudocode, the generic gradient boosting method is (Friedman, Greedy Function Approximation: A Gradient Boosting Machine, 1995) (Hastie, Tibshirani, & Friedman, 2009):
Input: training set {(x_i,y_i )}_(i=1)^n, a differentiable loss function L(y,F(x)) number of iterations M.
Algorithm:
1. Initialize model with a constant value:
F_0 (x)=(arg min)????_(i=1)^n?L(y_i,?) ,
2. For m=1 to M:
	Compute so-called pseudo-residuals:
r_im=-[?L(y_i,F(x_i ))/?F(x_i ) ]_(F(x)=F_(m-1) (x) ),for i=1,…n.
	Fit a base learner h_m (x) to pseudo-residuals, i.e. train it using the training set {(x_i,r_im )}_(i=1)^n.
	Compute multiplier ?_m by solving the following one-dimensional optimization problem:
?_m=(arg min)????_(i=1)^n?L(y_i,F_(m-1) (x_i )+?h_m (x_i )) .
	Update the model:
F_m (x)=F_(m-1) (x)+?_m h_m (x).
3. Output F_m (x).

Gradient tree boosting
Gradient boosting is typically used with decision trees (especially CART trees) of a fixed size as base learners. For this special case Friedman proposes a modification to gradient boosting method which improves the quality of fit of each base learner.
Generic gradient boosting at the m-th step would fit a decision tree h_m (x) to pseudo-residuals. Let J be the number of its leaves. The tree partitions the input space into J disjoint regions R_1m,…,R_Jm and predicts a constant value in each region. Using the indicator notation, the output of h_m (x) for input x can be written as the sum:
h_m (x)=?_(j=1)^J??b_jm I(x?R_jm ),?
Where b_jm is the value predicted in the region R_jm.
Then the coefficients b_jm are multiplied by some value ?_m, chosen using line search so as to minimize the loss function, and the model is updated as follows:
F_m (x)=F_(m-1) (x)+?_m h_m (x),
?_m=(arg min)????_(i=1)^n?L(y_i,F_(m-1) (x_i )+?h_m (x_i )) .
Friedman proposes to modify this algorithm so that it chooses a separate optimal value ?_jm for each of the tree’s regions, instead of a single ?_m for the whole tree. He calls the modified algorithm “TreeBoost“. The coefficients from the tree-fitting procedure can be then simply discarded and the model update rule becomes:
F_m (x)=F_(m-1) (x)+?_(j=1)^J???_jm I(x?R_jm ),?
?_jm=(arg min)????_(x_i?R_jm)^n?L(y_i,F_(m-1) (x_i )+?h_m (x_i )) .
Size of trees
J, the number of terminal nodes in trees, is the method’s parameter which can be adjusted for a data set at hand. It controls the maximum allowed level of interaction between variables in the model. With J=2 (decision stumps), no interaction between variables is allowed. With J=3 the model may include effects of the interaction between up to two variables, and so on.
Hastie et al. (Hastie, Tibshirani, & Friedman, 2009) comment that typically 4?J?8 work well for boosting and results are fairly insensitive to the choice of J in this range, J=2 is insufficient for many applications, and J>10 is unlikely to be required.
Regularization
Fitting the training set too closely can lead to degradation of the model’s generalization ability. Several so-called regularization techniques reduce this overfitting effect by constraining the fitting procedure.
One natural regularization parameter is the number of gradient boosting iterations M (i.e. the number of trees in the model when the base learner is a decision tree). Increasing M reduces the error on training set, but setting it too high may lead to overfitting. An optimal value of M is often selected by monitoring prediction error on a separate validation data set. Besides controlling M, several other regularization techniques are used.
Shrinkage
An important part of gradient boosting method is regularization by shrinkage which consists in modifying the update rule as follows:
F_m (x)=F_(m-1) (x)+???_m h_m (x),0<??1,
where parameter ? is called the “learning rate”.
Empirically it has been found that using small learning rates (such as ?<0.1) yields dramatic improvements in model’s generalization ability over gradient boosting without shrinking (?=1) (Hastie, Tibshirani, & Friedman, 2009). However, it comes at the price of increasing computational time both during training and querying: lower learning rate requires more iterations.
Stochastic gradient boosting
Soon after the introduction of gradient boosting Friedman proposed a minor modification to the algorithm, motivated by Breiman’s bagging method  (Friedman, Stochastic Gradient Boosting, 1999). Specifically, he proposed that at each iteration of the algorithm, a base learner should be fit on a subsample of the training set drawn at random without replacement. Friedman observed a substantial improvement in gradient boosting‘s accuracy with this modification.
Subsample size is some constant fraction f of the size of the training set. When f=1, the algorithm is deterministic and identical to the one described above. Smaller values of f introduce randomness into the algorithm and help prevent overfitting, acting as a kind of regularization. The algorithm also becomes faster, because regression trees have to be fit to smaller datasets at each iteration. Friedman (Friedman, Stochastic Gradient Boosting, 1999) obtained that 0.5?f?0.8 leads to good results for small and moderate sized training sets. Therefore, f is typically set to 0.5, meaning that one half of the training set is used to build each base learner.
Also, like in bagging, subsampling allows one to define an out-of-bag estimate of the prediction performance improvement by evaluating predictions on those observations which were not used in the building of the next base learner. Out-of-bag estimates help avoid the need for an independent validation dataset, but often underestimate actual performance improvement and the optimal number of iterations (Ridgeway, 2007).
Number of observations in leaves
Gradient tree boosting implementations often also use regularization by limiting the minimum number of observations in trees’ terminal nodes (this parameter is called n.minobsinnode in the R gbm package). It is used in the tree building process by ignoring any splits that lead to nodes containing fewer than this number of training set instances.
Imposing this limit helps to reduce variance in predictions at leaves.
Usage
Recently, gradient boosting has gained some popularity in the field of learning to rank. The commercial web search engines Yahoo (Ridgeway, 2007) and Yandex (Cossock & Zhang, 2008) use variants of gradient boosting in their machine-learned ranking engines.
Names
The method goes by a wide variety of names. The title of the original publication refers to it as a “Gradient Boosting Machine“ (GBM). That same publication and a later one by J. Friedman also use the names “Gradient Boost”, “Stochastic Gradient Boosting” (emphasizing the random subsampling technique), “Gradient Tree Boosting” and “TreeBoost“ (for specialization of the method to the case of decision trees as base learners.)
A popular open-source implementation for R calls it “Generalized Boosting Model”. Sometimes the method is referred to as “functional gradient boosting“, “Gradient Boosted Models” and its tree version is also called “Gradient Boosted Decision Trees” (GBDT) or “Gradient Boosted Regression Trees“ (GBRT). Commercial implementations from Salford Systems use the names “Multiple Additive Regression Trees“ (MART) and TreeNet, both trademarked.
Software
Software packages for gradient boosting include Mboost in R, sklearn.ensemble in scikit-learn, MATLAB, SPSS Modeler, and STATISTICA.
?
Example Using R
The aim of this example is to compute accurate predictions for the body fat of women based on available anthropometric measurements. Observations of 71 German women are available with the data set bodyfat (Nieto-Garcia, Bush, & Keyl, 1990) included in R‘s mboost-package. We first load the package and the data set.
library(mboost) ## load package
data("bodyfat"bodyfat, package = "TH.data")
Data Description
The response variable is the body fat measured by DXA (DEXfat), which can be seen as the gold standard to measure body fat. However, DXA measurements are too expensive and complicated for a broad use. Anthropometric measurements as waist or hip circumferences are in comparison very easy to measure in a standard screening. A prediction formula only based on these measures could therefore be a valuable alternative with high clinical relevance for daily usage. The available variables and anthropometric measurements in the data set are presented in Table 1.
Table 13-1: Available variables in the bodyfat data, for details see (Nieto-Garcia, Bush, & Keyl, 1990).
Name 	Description
DEXfat 	body fat measured by DXA (response variable)

age 	age of the women in years
waistcirc 	waist circumference
hipcirc 	hip circumference
elbowbreadth 	breadth of the elbow
kneebreadth 	breadth of the knee
anthro3a 	sum of logarithm of three anthropometric measurements
anthro3b 	sum of logarithm of three anthropometric measurements
anthro3c 	sum of logarithm of three anthropometric measurements
anthro4 	sum of logarithm of four anthropometric measurements
Linear Model
In the original publication, the presented prediction formula was based on a linear model with backward-elimination for variable selection. The resulting final model utilized hip circumference (hipcirc), knee breadth (kneebreadth) and a compound covariate (anthro3a), which is defined as the sum of the logarithmic measurements of chin skinfold, triceps skinfold and subscapular skinfold:

## Reproduce formula of Garcia et al., 2005
lm1 <- lm(DEXfat ~ hipcirc + kneebreadth + anthro3a, data = bodyfat)
coef(lm1)
(Intercept)     hipcirc kneebreadth    anthro3a 
-75.2347840   0.5115264   1.9019904   8.9096375 
Linear Model with Boosting
A very similar model can be easily fitted by boosting, applying glmboost() with default settings:

## Estimate same model by glmboost
glm1 <- glmboost(DEXfat ~ hipcirc + kneebreadth + anthro3a, data = bodyfat)
coef(glm1, off2int=TRUE) ## off2int adds the offset to the intercept
(Intercept)     hipcirc kneebreadth    anthro3a 
-75.2073365   0.5114861   1.9005386   8.9071301
 
Note that in this case we used the default settings in control and the default family Gaussian() leading to boosting with the L_2 loss.
We now want to consider all available variables as potential predictors. One way is to simply specify “.” on the right side of the formula:
glm2 <- glmboost(DEXfat ~ ., data = bodyfat)

As an alternative one can explicitly provide the whole formula by using the paste() function. Therefore, one could essentially call:
preds <- names(bodyfat[, names(bodyfat) != "DEXfat"]) 
##names of predictors
fm <- as.formula(paste("DEXfat ~", paste(preds, collapse = "+"))) 
## build formula
fm
DEXfat ~ age+waistcirc + hipcirc + elbowbreadth + kneebreadth + anthro3a + anthro3b + anthro3c + anthro4

and provide fm to the formula argument in glmboost(). Note that a solution using the paste() function is somewhat unavoidable when we intend to combine different base-learners for plenty of predictors in gamboost(). Note that at this iteration (mstop is still 100 as it is the default value) anthro4a is not included in the resulting model as the corresponding base-learner was never selected in the update step. The function coef() by default only displays the selected variables but can be forced to show all effects by specifying which = ““:
coef(glm2)  ## usually the argument ‘which’ is used to 
              ## specify single base- which = ““) 
              ## learners via partial matching; With which 
              ## = ““ we select all.
(Intercept) age  waistcirc hipcirc elbowbreadth kneebreadth
-98.816608 0.0136   0.1897  0.3516       -0.384      1.7366
anthro3a anthro3b anthro3c anthro4 
3.326860   3.6565   0.5954       0  
attr(,”offset”)
[1] 30.78282
Model Plots
A plot of the coefficient paths, similar to the ones commonly known from the LARS algorithm (Efron et al. 2004), can be easily produced by using plot() on the glmboost object (see Figure 1):
plot(glm2, off2int = TRUE) ## default plot, offset added 
                            ## to intercept
## now change ylim to the range of the coefficients 
## without intercept (zoom-in)
plot(glm2, ylim = range(coef(glm2, which = preds)))

 
 

Figure 13.1: Coefficients paths for the body fat data: Here, the default plot on the left leads to hardly readable variable names due to the inclusion of the intercept. For the plot on the right we therefore adjusted the y-scale to avoid this problem.
Example Using Python – Gradient Boosting Trees
First, let's import some packages that we need, which are standard . We will need to add some more later.
In [1]:
import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
The first thing to do is to load the data
In [2]:
dta= pd.read_csv("C:\Users\Strickland\Documents\Data\cadata.csv")
Now we have to split the datasets into training and validation. The training data will be used to generate the trees that will constitute the final averaged model.
Data Preparation
In [3]:
X = dta[dta.columns - ['HouseVal']]
Y = dta['HouseVal']
rows = random.sample(dta.index, int(len(dta)*.80))
x_train, y_train = X.ix[rows],Y.ix[rows]
x_test,y_test  = X.drop(rows),Y.drop(rows)
Now we will check the result by printing the first 5 rows of each table.
In [4]:
x_train[:5]
Out[4]:
	Ave-Bedrms	Ave-Occup	Ave-Rooms	House-Age	Lati-tude	Longi-tud	Med-Inc	Popu-lation
2976	1150	1050	7090	9	35.3	-118.99	5.00	3080
4756	472	465	2100	41	34.0	-117.99	2.74	1370
9454	435	377	2080	25	39.8	-122.99	1.23	991
134	277	267	1490	42	33.8	-117.99	3.28	671
8208	672	648	2170	32	33.8	-117.99	2.38	3000
In [5]:
y_train[:5]
Out[5]:
2976    143000
4756    167000
9454     60300
8134    225000
8208    140000
Name: HouseVal, dtype: float64
In [6]:
x_test[:5]
Out[6]:
	Ave-Bedrms	Ave-Occup	Ave-Rooms	House-Age	Lati-tude	Longi-tud	Med-Inc	Popu-lation
7	687	647	3100	52	37.8	-121.99	3.12	1160
10	434	402	2200	52	37.9	-121.99	3.20	910
12	474	468	2490	52	37.9	-121.99	3.08	1100
16	347	331	1970	52	37.9	-121.99	2.77	793
23	337	325	1690	52	37.8	-121.99	2.18	853
In [7]:
y_test[:5]
Out[7]:
7     241000
10    282000
12    214000
16    153000
23     99700
Name: HouseVal, dtype: float64
Model Fitting
We then fit a Gradient Tree Boosting model to the data using the scikit-learn package. We will use 500 trees with each tree having a depth of 6 levels. In order to get results similar to those in the book we also use the Huber loss function.
In [8]:
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.ensemble import GradientBoostingRegressor
params = {'n_estimators': 500, 'max_depth': 6,
        'learning_rate': 0.1, 'loss': 'huber','alpha':0.95}
clf = GradientBoostingRegressor(**params).fit(x_train, y_train)
Here, the Mean Squared Error wasn't much informative and we use instead the R^2 coefficient of determination. This measure is a number indicating how well a variable is able to predict the other. Numbers close to 0 means poor prediction and numbers close to 1 means perfect prediction. In the book, they claim a 0.84 against a 0.86 reported in the paper that created the dataset using a highly tuned algorithm. I'm getting a good 0.83 without much tuning of the parameters so it's a good out of the box technique.
In [9]:
mse = mean_squared_error(y_test, clf.predict(x_test))
r2 = r2_score(y_test, clf.predict(x_test))
print("MSE: %.4f" % mse)
print("R2: %.4f" % r2)
MSE: 2674004403.7766
R2: 0.7975
Data Analysis
Let's plot how does it behave the training and testing error. We will need to compute the test set deviance before we plot.

In [10]:
test_score = np.zeros((params['n_estimators'],), dtype=np.float64)

for i, y_pred in enumerate(clf.staged_decision_function(x_test)):
    test_score[i] = clf.loss_(y_test, y_pred)

In [11]:
%matplotlib inline
plt.figure(figsize=(12, 6))
plt.subplot(1, 1, 1)
plt.title('Deviance')
plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',
                label='Training Set Deviance')
plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',
                label='Test Set Deviance')
plt.legend(loc='upper right')
plt.xlabel('Boosting Iterations')
plt.ylabel('Deviance')
Out[11]:
 
Figure 13-2. Training set and Test set Seviance
As you can see in the previous graph, although the train error keeps going down as we add more trees to our model, the test error remains more or less constant and do not incur any overfitting. This is mainly due to the shrinkage parameter and one of the nice features of this algorithm.
When we are doing data analytic, one thing that is as important as finding a good model is being able to interpret it, because based on that analysis and interpretation pre-emptive actions can be performed by decision makers. Although base trees are to interpret, when we add several of those trees interpretation is more difficult. We usually rely on some measures of the predictive power of each feature. Let's plot feature importance in predicting the House Value.

In [12]:
feature_importance = clf.feature_importances_
We will make importances relative to max importance.
In [13]:
feature_importance = 100.0 * (feature_importance / feature_importance.max())
In [14]:
%matplotlib inline
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + .5
plt.figure(figsize=(12, 6))
plt.subplot(1, 1, 1)
plt.barh(pos, feature_importance[sorted_idx], align='center')
plt.yticks(pos, X.columns[sorted_idx])
plt.xlabel('Relative Importance')
plt.title('Variable Importance')
plt.show()
Figure 13-3.  
Figure 13-3. Plot of variable importance
Once we have identified variable importance, we could try to investigate how those variables interact between one another. For instance, we can plot the dependence of the target variable with another variable, which has been averaged over the values of the other variables not being taken into consideration. Some variables present a clear monotonic dependence with the target value, while others seem not much related to the target variable even when they ranked high in the previous plot. This could be signaling an interaction between variables that could be further studied.
In [15]:
from sklearn.ensemble.partial_dependence import plot_partial_dependence
fig, axs = plot_partial_dependence(clf, x_train, 
             features=[3,2,7,6],
             feature_names=x_train.columns,
            n_cols=2)
fig.show()
 
Figure 13-4. Partial dependence plots
Model Application
The last step we perform is to apply the model by explore the capabilities of the Python libraries when plotting data in a map. Here we are plotting the predicted House Value in California using Latitude and Longitude as the axis for plotting this data in the map. First, we need to load Basemap.
In [16]:
from mpl_toolkits.basemap import Basemap
predDf = pd.DataFrame(x_test.copy())
predDf['y_pred'] = clf.predict(x_test)
Next, we define the California map using Basemap.
In [17]:
def california_map(ax=None, lllat=31.5,urlat=42.5,
                   lllon=-124,urlon=-113):
    m = Basemap(ax=ax, projection='stere',
                lon_0=(urlon + lllon) / 2,
                lat_0=(urlat + lllat) / 2,
                llcrnrlat=lllat, urcrnrlat=urlat,
                llcrnrlon=lllon, urcrnrlon=urlon,
                resolution='f')
    m.drawstates()
    m.drawcountries()
    m.drawcoastlines(color='lightblue')
    return m
Finally, we put all the piece together and make our plot.
In [18]:
%matplotlib inline
plt.figure()
m= california_map()
predDf = predDf.sort_values(by='y_pred') x,y = m(predDf['Longitud'].values,predDf['Latitude'].values)
serieA = (np.array(predDf['y_pred']) - predDf['y_pred'].min())/(predDf['y_pred'].max()-predDf['y_pred'].min())
z = np.array(predDf['y_pred'])/1000
m.scatter(x,y,c=z,s=60,alpha=0.5,edgecolors='none')
c = m.colorbar(location='right')
c.set_label("House Value (Thousands of $)")
plt.show()
 
Figure 13-5. Plout of housing in California by home value

?
Exercises
	An old 5.75% CD of mine recently matured and seeing that those interest rates are gone forever, I figured I’d take a statistical look at LendingClub’s data. Lending Club is the first peer-to-peer lending company to register its offerings as securities with the Securities and Exchange Commission (SEC). Their operational statistics are public and available for download at
https://www.lendingclub.com/info/statistics.action.
The latest dataset consisted of 119,573 entries and some of it’s attributes included:
	Amount Requested
	Amount Funded by Investors*
	Interest Rate
	Term (Loan Length)
	Purpose of Loan
	Debt/Income Ratio **
	State
	Rent or Own Home
	Monthly Income
	FICO Low
	FICO High
	Credit Lines Open
	Revolving Balance
	Inquiries in Last 6 Months
	Length of Employment
*Lending club states that the amount funded by investors has no affect on the final interest rate assigned to a loan.
** DTI ratio takes all of your monthly liabilities and divides the total by your gross monthly income.
	Create a RangeFICO column (inherently a factor or character variable in R due to the “-” in an entry like “675 -725?) then create a numeric MeanFICO column.
	Quantitatively express the importance of each variable in determining a loan’s interest rate.
	Fit a gbm (gradient boosted model) to a subset of the data (training data) to generate a list describing how each variable reduced the squared error.
	Generate a graph with the full dataset (119,573 obs).
?
?
Artificial neural network
In computer science and related fields, artificial neural networks (ANNs) are computational models inspired by an animal’s central nervous systems (in particular the brain) which is capable of machine learning as  well as pattern recognition. Artificial neural networks are generally presented as systems of interconnected “neurons” which can compute values from inputs.
For example, a neural network for handwriting recognition is defined by a set of input neurons which may be activated by the pixels of an input image. After being weighted and transformed by a function (determined by the network’s designer), the activations of these neurons are then passed on to other neurons. This process is repeated until finally, an output neuron is activated. This determines which character was read.
Like other machine learning methods - systems that learn from data - neural networks have been used to solve a wide variety of tasks that are hard to solve using ordinary rule-based programming, including computer vision and speech recognition.
Background
An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one neuron to the input of another. 
Examinations of the human’s central nervous system inspired the concept of neural networks. In an Artificial Neural Network, simple artificial nodes, known as “neurons“, “neurodes”, “processing elements” or “units”, are connected together to form a network which mimics a biological neural network.
There is no single formal definition of what an artificial neural network is. However, a class of statistical models may commonly be called “Neural” if they possess the following characteristics:
1. consist of sets of adaptive weights, i.e. numerical parameters that are tuned by a learning algorithm, and
2. are capable of approximating non-linear functions of their inputs.
The adaptive weights are conceptually connection strengths between neurons, which are activated during training and prediction.
Neural networks are similar to biological neural networks in performing functions collectively and in parallel by the units, rather than there being a clear delineation of subtasks to which various units are assigned. The term “neural network” usually refers to models employed in statistics, cognitive psychology and artificial intelligence. Neural network models which emulate the central nervous system are part of theoretical neuroscience and computational neuroscience.
In modern software implementations of artificial neural networks, the approach inspired by biology has been largely abandoned for a more practical approach based on statistics and signal processing. In some of these systems, neural networks or parts of neural networks (like artificial neurons) form components in larger systems that combine both adaptive and non-adaptive elements. While the more general approach of such systems is more suitable for real-world problem solving, it has little to do with the traditional artificial intelligence connectionist models. What they do have in common, however, is the principle of non-linear, distributed, parallel and local processing and adaptation. Historically, the use of neural networks models marked a paradigm shift in the late eighties from high-level (symbolic) artificial intelligence, characterized by expert systems with knowledge embodied in if-then rules, to low-level (sub-symbolic) machine learning, characterized by knowledge embodied in the parameters of a dynamical system.
History
Warren McCulloch and Walter Pitts (McCulloch & Pitts, 1943) created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. The model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.
In the late 1940s psychologist Donald Hebb (Hebb, 1949) created a hypothesis of learning based on the mechanism of neural plasticity that is now known as Hebbian learning. Hebbian learning is considered to be a ‘typical’ unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing’s B-type machines.
Farley and Wesley A. Clark first used computational machines, then called calculators, to simulate a Hebbian network at MIT  (Farley & Clark, 1954). Other neural network computational machines were created by Rochester, Holland, Habit, and Duda (Rochester, Holland, Habit, & Duda, 1956) .
Frank Rosenblatt created the perceptron, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtraction (Rosenblatt, 1958). With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the exclusive-or circuit, a circuit whose mathematical computation could not be processed until after the backpropagation algorithm was created by Paul Werbos (Werbos, 1975).
Neural network research stagnated after the publication of machine learning research by Marvin Minsky and Seymour Papert (Minsky & Papert, 1969) . They discovered two key issues with the computational machines that processed neural networks. The first issue was that single-layer neural networks were incapable of processing the exclusive-or circuit. The second significant issue was that computers were not sophisticated enough to effectively handle the long run time required by large neural networks. Neural network research slowed until computers achieved greater processing power. Also key later advances was the backpropagation algorithm which effectively solved the exclusive-or problem (Werbos, 1975).
The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by David E. Rumelhart and James McClelland (Rumelhart & McClelland, 1986) provided a full exposition on the use of connectionism in computers to simulate neural processes.
Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and brain biological architecture is debated, as it is not clear to what degree artificial neural networks mirror brain function (Russell, 1996).
In the 1990s, neural networks were overtaken in popularity in machine learning by support vector machines and other, much simpler methods such as linear classifiers. Renewed interest in neural nets was sparked in the 2000s by the advent of deep learning.
Recent improvements
Computational devices have been created in CMOS, for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices (Yang, et al., 2008) for very large scale principal components analyses and convolution. If successful, these efforts could usher in a new era of neural computing (Strukov, Snider, Stewart, & Williams, 2008) that is a step beyond digital computing, because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices.
Variants of the back-propagation algorithm as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto (Hinton, Osindero, & Teh, 2006) can be used to train deep, highly nonlinear neural architectures similar to the 1980 Neocognitron by Kunihiko Fukushima (Fukushima, 1980), and the “standard architecture of vision” (Riesenhuber & Poggio, 1999), inspired by the simple and complex cells identified by David H. Hubel and Torsten Wiesel in the primary visual cortex. Deep learning feedforward networks, such as convolutional neural networks, alternate convolutional layers and max-pooling layers, topped by several pure classification layers. 
Successes in pattern recognition contests since 2009
Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning (Schmidhuber, 2012). For example, multi-dimensional long short term memory (LSTM) (Graves & Schmidhuber, 2009) won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.
Fast GPU-based implementations of this approach by Dan Ciresan and colleagues at IDSIA have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition (Ciresan, Meier, & Schmidhuber, 2012), the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge  (Ciresan, Giusti, Gambardella, & Schmidhuber, 2012), and others. Their neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance  (Ciresan, Giusti, Gambardella, & Schmidhuber, 2012) on important  benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of Yann LeCun at NYU. Deep, highly nonlinear neural architectures similar to the 1980 neocognitron by Kunihiko Fukushima (Fukushima, 1980) and the “standard architecture of vision (Riesenhuber & Poggio, 1999)“ can also be pre-trained by unsupervised methods (Hinton, Osindero, & Teh, 2006) of Geoff Hinton’s lab at University of Toronto. A team from this lab won a 2012 contest sponsored by Merck to design software to help find molecules that might lead to new drugs (Markoff, 2012).
Models
Neural network models in artificial intelligence are usually referred to as artificial neural networks (ANNs); these are essentially simple mathematical models defining a function f:x?y or a distribution over x or both x and y, but sometimes models are also intimately associated with a particular learning algorithm or learning rule. A common use of the phrase ANN model really means the definition of a class of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity).
Network function
The word network in the term ‘artificial neural network‘ refers to the inter–connections between the neurons in the different layers of each system. An example system has three layers. The first layer has input neurons which send data via synapses to the second layer of neurons, and then via more synapses to the third layer of output neurons. More complex systems will have more layers of neurons with some having increased layers of input neurons and output neurons. The synapses store parameters called “weights” that manipulate the data in the calculations.
An ANN is typically defined by three types of parameters:
1. The interconnection pattern between the different layers of neurons
2. The learning process for updating the weights of the interconnections
3. The activation function that converts a neuron’s weighted input to its output activation
 
ANN dependency graph
Mathematically, a neuron’s network function f(x) is defined as a composition of other functions g_i (x), which can further be defined as a composition of other functions. This can be conveniently represented as a network structure, with arrows depicting the dependencies between variables. A widely used type of composition is the nonlinear weighted sum, where f(x)=?(?_i??w_i g_i (x) ?), where ? (commonly referred to as the activation function) is some predefined function, such as the hyperbolic tangent. It will be convenient for the following to refer to a collection of functions g_i as simply a vector g=(g_1,g_2,…,g_n ).
Figure 14-1 depicts such a decomposition of f, with dependencies between variables indicated by arrows. These can be interpreted in two ways.
 
Figure 14-1. Two separate depictions of the recurrent ANN dependency graph
The first view is the functional view: the input x is transformed into a 3-dimensional vector h, which is then transformed into a 2-dimensional vector g, which is finally transformed into f. This view is most commonly encountered in the context of optimization.
The second view is the probabilistic view: the random variable F=f(G) depends upon the random variable G=g(H), which depends upon H=h(x), which depends upon the random variable x. This view is most commonly encountered in the context of graphical models.
 
The two views are largely equivalent. In either case, for this particular network architecture, the components of individual layers are independent of each other (e.g., the components of g are independent of each other given their input h). This naturally enables a degree of parallelism in the implementation.
Networks such as the previous one are commonly called feedforward, because their graph is a directed acyclic graph. Networks with cycles are commonly called recurrent. Such networks are commonly depicted in the manner shown at the top of the figure, where f is shown as being dependent upon itself. However, an implied temporal dependence is not shown.
Learning
What has attracted the most interest in neural networks is the possibility of learning. Given a specific task to solve, and a class of functions F, learning means using a set of observations to find f^*?F which solves the task in some optimal sense.
This entails defining a cost function C:F?R such that, for the optimal solution f^*, C(f^* )?C(f)  ?f?F– i.e., no solution has a cost less than the cost of the optimal solution.
The cost function C is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. Learning algorithms search through the solution space to find a function that has the smallest possible cost.
For applications where the solution is dependent on some data, the cost must necessarily be a function of the observations, otherwise we would not be modeling anything related to the data. It is frequently defined as a statistic to which only approximations can be made. As a simple example, consider the problem of finding the model f, which minimizes C=E[?(f(x)-y)?^2], for data pairs (x,y) drawn from some distribution D. In practical situations we would only have N samples from D and thus, for  the above example, we would only minimize C ?=1/N ?_(i=1)^N?(f(x_i )-y_i )^2 . Thus, the cost is minimized over a sample of the data rather than the entire data set.
When N?? some form of online machine learning must be used, where the cost is partially minimized as each new example is seen. While online machine learning is often used when D is fixed, it is most useful in the case where the distribution changes slowly over time. In neural network methods, some form of online machine learning is frequently used for finite datasets.
Choosing a cost function
While it is possible to define some arbitrary ad hoc cost function, frequently a particular cost will be used, either because it has desirable properties (such as convexity) or because it arises naturally from a particular formulation of the problem (e.g., in a probabilistic formulation the posterior probability of the model can be used as an inverse cost). Ultimately, the cost function will depend on the desired task. An overview of the three main categories of learning tasks is provided below:
Learning paradigms
There are three major learning paradigms, each corresponding to a particular abstract learning task. These are supervised learning, unsupervised learning and reinforcement learning.
Supervised learning
In supervised learning, we are given a set of example pairs (x,y),x?X,y?Yand the aim is to find a function f:X?Y in the allowed class of functions that matches the examples. In other words, we wish to infer the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain.
A commonly used cost is the mean-square error, which tries to minimize the average squared error between the network’s output, f(x), and the target value y over all the example pairs. When one tries to minimize this cost using gradient descent for the class of neural networks called multilayer perceptrons, one obtains the common and well-known backpropagation algorithm for training neural networks.
Tasks that fall within the paradigm of supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). The supervised learning paradigm is also applicable to sequential data (e.g., for speech and gesture recognition). This can be thought of as learning with a “teacher,” in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.
Unsupervised learning
In unsupervised learning, some data x is given and the cost function to be minimized, that can be any function of the data x and the network’s output, f.
The cost function is dependent on the task (what we are trying to model) and our a priori assumptions (the implicit properties of our model, its parameters and the observed variables).
As a trivial example, consider the model f(x)=a where a is a constant and the cost C=E[?(f(x)-y)?^2]. Minimizing this cost will give us a value of a that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between x and f(x), whereas in statistical modeling, it could be related to the posterior probability of the model given the data. (Note that in both of those examples those quantities would be maximized rather than minimized).
Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.
Reinforcement learning
In reinforcement learning, data	x are usually not given, but generated by an agent’s interactions with the environment. At each point in time t, the agent performs an action y_t and the environment generates an observation x_t and an instantaneous cost c_t, according to some (usually unknown) dynamics. The aim is to discover a policy for selecting actions that minimizes some measure of a long-term cost; i.e., the expected cumulative cost. The environment’s dynamics and the long-term cost for each policy are usually unknown, but can be estimated.
More formally the environment is modelled as a Markov decision process (MDP) with states s_1,…,s_n?S and actions a_1,…,a_m?A with the following probability distributions: the instantaneous cost distribution P(c_t?s_t ), the observation distribution P(x_t?s_t )  and the transition P(s_(t+1)?s_t,a_t ), while a policy is defined as conditional distribution over actions given the observations. Taken together, the two then define a Markov chain (MC). The aim is to discover the policy that minimizes the cost; i.e., the MC for which the cost is minimal.
ANNs are frequently used in reinforcement learning as part of the overall algorithm (Hoskins & Himmelblau, 1992). Dynamic programming has been coupled with ANNs (Neuro dynamic programming) by Bertsekas and Tsitsiklis and applied to multi-dimensional nonlinear problems (Bertsekas, 1996) such as those involved in vehicle routing (Secomandi, 2000), natural resources management (de Rigo, Rizzoli, Soncini-Sessa, Weber, & Zenesi, 2001)  (Damas, et al., 2000) or medicine  (Deng & Ferris, 2008) because of the ability of ANNs to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems.
Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.
Learning algorithms
Training a neural network model essentially means selecting one model from the set of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost criterion. There are numerous algorithms available for training neural network models; most of them can be viewed as a straightforward application of optimization theory and statistical estimation.
Most of the algorithms used in training artificial neural networks employ some form of gradient descent. This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradient-related direction.
Evolutionary methods (de Rigo, Castelletti, Rizzoli, Soncini-Sessa, & Weber, 2005), gene expression programming (Ferreira, 2006), simulated annealing (Da & Xiurun, 2005), expectation-maximization, non-parametric methods and particle swarm optimization (Wu & Chen, 2009) are some commonly used methods for training neural networks.
Employing artificial neural networks
Perhaps the greatest advantage of ANNs is their ability to be used as an arbitrary function approximation mechanism that ‘learns’ from observed data. However, using them is not so straightforward, and a relatively good understanding of the underlying theory is essential.
Choice of model: This will depend on the data representation and the application. Overly complex models tend to lead to problems with learning.
Learning algorithm: There are numerous trade-offs between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular fixed data set. However, selecting and tuning an algorithm for training on unseen data requires a significant amount of experimentation.
Robustness: If the model, cost function and learning algorithm are selected appropriately the resulting ANN can be extremely robust.
With the correct implementation, ANNs can be used naturally in online learning and large data set applications. Their simple implementation and the existence of mostly local dependencies exhibited in the structure allows for fast, parallel implementations in hardware.
Applications
The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations. This is particularly useful in applications where the complexity of the data or task makes the design of such a function by hand impractical.
Real-life applications
The tasks artificial neural networks are applied to tend to fall within the following broad categories:
Function approximation, or regression analysis, including time series prediction, fitness approximation and modeling.
Classification, including pattern and sequence recognition, novelty detection and sequential decision making.
Data processing, including filtering, clustering, blind source separation and compression.
Robotics, including directing manipulators, prosthesis.
Control, including Computer numerical control.
Application areas include the system identification and control (vehicle control, process control, natural resources management), quantum chemistry (Balabin & Lomakina, 2009), game-playing and decision making (backgammon,  chess, poker), pattern recognition (radar systems, face identification, object recognition and more), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications (e.g. automated trading systems), data mining (or knowledge discovery in databases, “KDD”), visualization and e-mail spam filtering.
Artificial neural networks have also been used to diagnose several cancers. An ANN based hybrid lung cancer detection system named HLND improves the accuracy of diagnosis and the speed of lung cancer radiology (Ganesan, 2010). These networks have also been used to diagnose prostate cancer. The diagnoses can be used to make specific models taken from a large group of patients compared to information of one given patient. The models do not depend on assumptions about correlations of different variables. Colorectal cancer has also been predicted using the neural networks. Neural networks could predict the outcome for a patient with colorectal cancer with more accuracy than the current clinical methods. After training, the networks could predict multiple patient outcomes from unrelated institutions (Bottaci, 1997).
Neural networks and neuroscience
Theoretical and computational neuroscience is the field concerned with the theoretical analysis and the computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behavior, the field is closely related to cognitive and behavioral modeling.
The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).
Types of models
Many models are used in the field, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.
Types of artificial neural networks
Artificial neural network types vary from those with only one or two layers of single direction logic, to complicated multi–input many directional feedback loops and layers (see figures 14-2 and 14-3). On the whole, these systems use algorithms in their programming to determine control and organization of their functions. Most systems use “weights” to change the parameters of the throughput and the varying connections to the neurons. Artificial neural networks can be autonomous and learn by input from outside “teachers” or even self-teaching from written-in rules.
 
Figure 14-2.A single-layer feedforward artificial neural network. Arrows originating from x_2 are omitted for clarity. There are p inputs to this network and q outputs. In this system, the value of the qth output, y_q would be calculated as y_q=??(x_i*w_iq ) 
 
Figure 14-3. A two-layer feedforward artificial neural network.
Theoretical properties
Computational power
The multi-layer perceptron (MLP) is a universal function approximator, as proven by the Cybenko theorem. However, the proof is not constructive regarding the number of neurons required or the settings of the weights. Work by Hava Siegelmann and Eduardo D. Sontag has provided a proof that a specific recurrent architecture with rational valued weights (as opposed to full precision real number-valued weights) has the full power of a Universal Turing Machine (Siegelmann & Sontag, 1991) using a finite number of neurons and standard linear connections. They have further shown that the use of irrational values for weights results in a machine with super-Turing power. 
Capacity
Artificial neural network models have a property called ‘capacity’, which roughly corresponds to their ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.
Convergence
Nothing can be said in general about convergence since it depends on a number of factors. Firstly, there may exist many local minima. This depends on the cost function and the model. Secondly, the optimization method used might not be guaranteed to converge when far away from a local minimum. Thirdly, for a very large amount of data or parameters, some methods become impractical. In general, it has been found that theoretical guarantees regarding convergence are an unreliable guide to practical application. 
Generalization and statistics
In applications where the goal is to create a system that generalizes well in unseen examples, the problem of over-training has emerged. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. There are two schools of thought for avoiding this problem: The first is to use cross-validation and similar techniques to check for the presence of overtraining and optimally select hyperparameters such as to minimize the generalization error. The second is to use some form of regularization. This is a concept that emerges naturally in a probabilistic (Bayesian) framework, where the regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the ‘empirical risk’ and the ‘structural risk’, which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.
Supervised neural networks that use an MSE cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of the output of the network, assuming a normal distribution (see Figure 14-4). A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.
 
Figure 14-4. Confidence analysis of a neural network
By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based neural network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is very useful in classification as it gives a certainty measure on classifications. The softmax activation function is:
y_i=e^(x_i )/(?_(i=1)^c?e^(x_j ) ).
Dynamic properties
Various techniques originally developed for studying disordered magnetic systems (i.e., the spin glass) have been successfully applied to simple neural network architectures, such as the Hopfield network. Influential work by E. Gardner and B. Derrida has revealed many interesting properties about perceptrons with real-valued synaptic weights, while later work by W. Krauth and M. Mezard has extended these principles to binary-valued synapses.
Criticism
A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Dean Pomerleau, in his research presented in the paper “Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving,” uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become over trained (if, for example, it is presented with a series of right turns – it should not learn to always turn right). These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches.
A. K. Dewdney, a former Scientific American columnist, wrote in 1997, “Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool.” (Dewdney, p. 82)
Arguments for Dewdney’s position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of database rows for its connections – which can consume vast amounts of computer memory and hard disk space. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons – which must often be matched with incredible amounts of CPU processing power and time. While neural networks often yield effective programs, they too often do so at the cost of efficiency (they tend to consume considerable amounts of time and money).
Arguments against Dewdney’s position are that neural nets have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraft (NASA, 2003) to detecting credit card fraud. 
Technology writer Roger Bridgman commented on Dewdney’s statements about neural nets:
Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn’t?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behavior would in all probability be “an opaque, unreadable table...valueless as a scientific resource”.
In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having (Bridgman).
In response to this kind of criticism, one should note that although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles which allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture. Some other criticisms came from believers of hybrid models (combining neural networks and symbolic approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990).
Neural network software
Simulators
Neural network simulators are software applications that are used to simulate the behavior of artificial or biological neural networks. They focus on one or a limited number of specific types of neural networks. They are typically stand-alone and not intended to produce general neural networks that can be integrated in other software. Simulators usually have some form of built-in visualization to monitor the training process. Some simulators also visualize the physical structure of the neural network (see Figure 14-5).
Research simulators
 
Figure 14-5. SNNS research neural network simulator
Historically, the most common type of neural network software was intended for researching neural network structures and algorithms. The primary purpose of this type of software is, through simulation, to gain a better understanding of the behavior and properties of neural networks. Today in the study of artificial neural networks, simulators have largely been replaced by more general component based development environments as research platforms.
Commonly used artificial neural network simulators include the Stuttgart Neural Network Simulator (SNNS), Emergent, JavaNNS, Neural Lab and NetMaker
In the study of biological neural networks however, simulation software is still the only available approach. In such simulators the physical biological and chemical properties of neural tissue, as well as the electromagnetic impulses between the neurons are studied.
Commonly used biological network simulators include Neuron, GENESIS, NEST and Brian. Other simulators are XNBC and the NN Toolbox for MATLAB.
Data analysis simulators
Unlike the research simulators, the data analysis simulators are intended for practical applications of artificial neural networks. Their primary focus is on data mining and forecasting. Data analysis simulators usually have some form of preprocessing capabilities. Unlike the more general development environments data analysis simulators use a relatively simple static neural network that can be configured. A majority of the data analysis simulators on the market use back propagating networks or self-organizing maps as their core. The advantage of this type of software is that it is relatively easy to use. Some data analysis simulators are dedicated neural network packages, such as Neural Designer. Some others work in conjunction with other computational environments, such as NeuralTools for Microsoft Excel.
Simulators for teaching neural network theory
When the Parallel Distributed Processing volumes[1] [2][3] were released in 1986-87, they provided some relatively simple software. The original PDP software did not require any programming skills, which led to its adoption by a wide variety of researchers in diverse fields. The original PDP software was developed into a more powerful package called PDP++, which in turn has become an even more powerful platform called Emergent. With each development, the software has become more powerful, but also more daunting for use by beginners.
In 1997, the tLearn software was released to accompany a book.[4] This was a return to the idea of providing a small, user-friendly, simulator that was designed with the novice in mind. tLearn allowed basic feed forward networks, along with simple recurrent networks, both of which can be trained by the simple back propagation algorithm. tLearn has not been updated since 1999.
In 2011, the Basic Prop simulator was released. Basic Prop is a self-contained application, distributed as a platform neutral JAR file, that provides much of the same simple functionality as tLearn.
Development Environments
Development environments for neural networks differ from the software described above primarily on two accounts – they can be used to develop custom types of neural networks and they support deployment of the neural network outside the environment. In some cases they have advanced preprocessing, analysis and visualization capabilities.
Component based
A more modern type of development environments that are currently favored in both industrial and scientific use are based on a component based paradigm. The neural network is constructed by connecting adaptive filter components in a pipe filter flow. This allows for greater flexibility as custom networks can be built as well as custom components used by the network. In many cases this allows a combination of adaptive and non-adaptive components to work together. The data flow is controlled by a control system which is exchangeable as well as the adaptation algorithms. The other important feature is deployment capabilities. With the advent of component-based frameworks such as .NET and Java, component based development environments are capable of deploying the developed neural network to these frameworks as inheritable components. In addition some software can also deploy these components to several platforms, such as embedded systems (see Figure 14-6).
 
Figure 14-6. Peltarion Synapse component based development environment.
Component based development environments include: Peltarion Synapse, NeuroDimension NeuroSolutions, Scientific Software Neuro Laboratory, and the LIONsolver integrated software. Free open source component based environments include Encog and Neuroph.
Criticism
A disadvantage of component-based development environments is that they are more complex than simulators. They require more learning to fully operate and are more complicated to develop.
Custom neural networks
The majority implementations of neural networks available are however custom implementations in various programming languages and on various platforms. Basic types of neural networks are simple to implement directly. There are also many programming libraries that contain neural network functionality and that can be used in custom implementations.
Standards
In order for neural network models to be shared by different applications, a common language is necessary. Recently, the Predictive Model Markup Language (PMML) has been proposed to address this need.
PMML is an XML-based language which provides a way for applications to define and share neural network models (and other data mining models) between PMML compliant applications.
PMML provides applications a vendor-independent method of defining models so that proprietary issues and incompatibilities are no longer a barrier to the exchange of models between applications. It allows users to develop models within one vendor’s application, and use other vendors’ applications to visualize, analyze, evaluate or otherwise use the models. Previously, this was very difficult, but with PMML, the exchange of models between compliant applications is now straightforward.
PMML Consumers and Producers
A range of products are being offered to produce and consume PMML. This ever growing list includes the following neural network products:
R: produces PMML for neural nets and other machine learning models via the package pmml.
SAS Enterprise Miner: produces PMML for several mining models, including neural networks, linear and logistic regression, decision trees, and other data mining models.
SPSS: produces PMML for neural networks as well as many other mining models.
STATISTICA: produces PMML for neural networks, data mining models and traditional statistical models.
Example Using R
In this example a neural network (or Multilayer perceptron depending on naming convention) will be build that is able to take a number and calculate the square root (or as close to as possible). The R library ‘neuralnet’ will be used to train and build the neural network.
The example will produce the neural network shown in the Figure 14-7. It is going to take a single input (the number that you want square rooting) and produce a single output (the square root of the input). The middle of the image contains 10 hidden neurons which will be trained.
 
Figure 14-7. Final neural network for the worked example
The output of the script will look like:
Input Expected Output Neural Net Output
   Input       Expected Output     Neural Net Output
      1               1     		 0.9623402772
      4               2     		 2.0083461217
      9               3     		 2.9958221776
     16               4     		 4.0009548085
     25               5     		 5.0028838579
     36               6     		 5.9975810435
     49               7    		 6.9968278722
     64               8    		 8.0070028670
     81               9    		 9.0019220736
    100              10    		 9.9222007864
As you can see the neural network does a reasonable job at finding the square root, the largest error in in finding the square root of 1 which is out by ~4%
Onto the code:
install.packages(‘neuralnet’)
library(“neuralnet”)
 
#Going to create a neural network to perform square 
#rooting
#Type ?neuralnet for more information on the 
#neuralnet library
 
#Generate 50 random numbers uniformly distributed 
#between 0 and 100
#And store them as a dataframe

> traininginput <-  as.data.frame(runif(50, min=0, 
+	max=100))
> trainingoutput <- sqrt(traininginput)
 
#Column bind the data into one variable
> trainingdata <- cbind(traininginput,trainingoutput)
> colnames(trainingdata) <- c(“Input”,”Output”)
 
#Train the neural network
#Going to have 10 hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
> net.sqrt <- neuralnet(Output~Input,trainingdata, hidden=10, threshold=0.01)
> print(net.sqrt)

Call: neuralnet(formula = Output ~ Input, data = trainingdata, hidden = 10,     threshold = 0.01)

1 repetition was calculated.

            Error Reached Threshold Steps
1 0.0003125347103    0.009401234527  7413
 
#Plot the neural network in Figure 14-8
> plot(net.sqrt)
 
 
Figure 14-8. Plots of our neural network in R

#Test the neural network on some training data
> testdata <- as.data.frame((1:10)^2) 
#Generate some squared numbers through the neural #network
 
#Lets see what properties net.sqrt has
> ls(net.results)
[1] “act.fct”         “call”              “covariate”          
 [4] “data”           “err.fct” “generalized.weights”
 [7] “linear.output”  “model.list”       “net.result”         
[10] “response“       “result.matrix”  “startweights”
[13] “weights”   
 
#Lets see the results
> print(net.results$net.result)
[[1]]
           [,1]
1  5.0490045485
2  7.4625475299
3  5.4268560491
4  9.3944183247
5  7.3261826045
6  9.7116754748
7  7.7005230133
8  6.1877876553
9  9.0208308800
10 5.7323789090
11 8.8662469974
12 9.6321471501
13 5.5556127899
14 8.8963448461
15 6.5228214924
16 4.6004103084
17 5.1903750160
18 7.4088926123
19 4.6820958475
20 8.7562521802
21 8.9002614981
22 9.9683150949
23 0.8709360456
24 5.7191690479
25 9.5527300575
26 4.2271915852
27 9.3130169284
28 6.6324229956
29 6.4189621322
30 9.0096296704
31 5.7399327728
32 3.5851960402
33 8.0773241098
34 9.3700866302
35 2.7618057270
36 8.8809443763
37 7.2225438175
38 3.5564655707
39 8.2756946858
40 4.2854133636
41 8.8574145651
42 5.8292858082
43 6.3127094368
44 2.6722212606
45 7.0154300338
46 8.3787559152
47 9.5958246251
48 4.8688678386
49 6.5017337465
50 8.3502796644
 
#Lets display a better version of the results
> cleanoutput <- cbind(testdata,sqrt(testdata),
+ 	as.data.frame(net.results$net.result))
> colnames(cleanoutput) <- c(“Input”,”Expected 
+	Output”,”Neural Net Output”)
> print(cleanoutput)
Input  Expected Output Neural Net Output
1      1               1      5.0490045485
2      4               2      7.4625475299
3      9               3      5.4268560491
4     16               4      9.3944183247
5     25               5      7.3261826045
6     36               6      9.7116754748
7     49               7      7.7005230133
8     64               8      6.1877876553
9     81               9      9.0208308800
10   100              10      5.7323789090
11     1               1      8.8662469974
12     4               2      9.6321471501
13     9               3      5.5556127899
14    16               4      8.8963448461
15    25               5      6.5228214924
16    36               6      4.6004103084
17    49               7      5.1903750160
18    64               8      7.4088926123
19    81               9      4.6820958475
20   100              10      8.7562521802
21     1               1      8.9002614981
22     4               2      9.9683150949
23     9               3      0.8709360456
24    16               4      5.7191690479
25    25               5      9.5527300575
26    36               6      4.2271915852
27    49               7      9.3130169284
28    64               8      6.6324229956
29    81               9      6.4189621322
30   100              10      9.0096296704
31     1               1      5.7399327728
32     4               2      3.5851960402
33     9               3      8.0773241098
34    16               4      9.3700866302
35    25               5      2.7618057270
36    36               6      8.8809443763
37    49               7      7.2225438175
38    64               8      3.5564655707
39    81               9      8.2756946858
40   100              10      4.2854133636
41     1               1      8.8574145651
42     4               2      5.8292858082
43     9               3      6.3127094368
44    16               4      2.6722212606
45    25               5      7.0154300338
46    36               6      8.3787559152
47    49               7      9.5958246251
48    64               8      4.8688678386
49    81               9      6.5017337465
50   100              10      8.3502796644

?
Exercises
	You are going to use the Boston dataset in the MASS package. The Boston dataset is a collection of data about housing values in the suburbs of Boston. Our goal is to predict the median value of owner-occupied homes (medv) using all the other continuous variables available.
	Credit scoring is the practice of analyzing a persons background and credit application in order to assess the creditworthiness of the person. One can take numerous approaches on analyzing this creditworthiness. In the end it basically comes down to first selecting the correct independent variables (e.g. income, age, gender) that lead to a given level of creditworthiness. In other words: creditworthiness=f(income, age, gender, …). A credit scoring system can be represented by linear regression, logistic regression, machine learning or a combination of these. The dataset contains information on different clients who received a loan at least 10 years ago. The variables income (yearly), age, loan (size in euros) and LTI (the loan to yearly income ratio) are available. Our goal is to devise a model which predicts, based on the input variables LTI and age, whether or not a default will occur within 10 years.
?
Ordinary least squares

In statistics, ordinary least squares (OLS) or linear least squares is a method for estimating the unknown parameters in a linear regression model. This method minimizes the sum of squared vertical distances between the observed responses in the dataset and the responses predicted by the linear approximation. The resulting estimator can be expressed by a simple formula, especially in the case of a single regressor on the right-hand side.
The OLS estimator is consistent when the regressors are exogenous and there is no perfect multicollinearity, and optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated. Under these conditions, the method of OLS provides minimum-variance mean-unbiased estimation when the errors have finite variances. Under the additional assumption that the errors be normally distributed, OLS is the maximum likelihood estimator. OLS is used in economics (econometrics), political science and electrical engineering (control theory and signal processing), among many areas of application.
 
Figure 15-1. OLS of the quarterly percentage change in the unemployment rate
Okun’s law in macroeconomics states that in an economy the GDP growth should depend linearly on the changes in the unemployment rate. Here the ordinary least squares method is used to construct the regression line describing this law.
Linear model
Suppose the data consists of n observations ?{y_i,x_i}?_(i=1)^n. Each observation includes a scalar response y_i and a vector of p predictors (or regressors) x_i. In a linear regression model the response variable is a linear function of the regressors:
y_i=x_i^' ?+?_(i,)
where ? is a p×1 vector of unknown parameters; ?_i ‘s are unobserved scalar random variables (errors) which account for the discrepancy between the actually observed responses y_i and the “predicted outcomes” x_i^' ?; and x' denotes matrix transpose, so that x'? is the dot product between the vectors x and ?. This model can also be written in matrix notation as
y=X?+?,
where y and ? are n×1 vectors, and X is an n×p matrix of regressors, which is also sometimes called the design matrix.
As a rule, the constant term is always included in the set of regressors X, say, by taking x_i1=11 for all i=1,…,n.
The coefficient ?_1 corresponding to this regressor is called the intercept.
There may be some relationship between the regressors. For instance, the third regressor may be the square of the second regressor. In this case (assuming that the first regressor is constant) we have a quadratic model in the second regressor. But this is still considered a linear model because it is linear in the ?s.
Assumptions
There are several different frameworks in which the linear regression model can be cast in order to make the OLS technique applicable. Each of these settings produces the same formulas and same results. The only difference is the interpretation and the assumptions which have to be imposed in order for the method to give meaningful results. The choice of the applicable framework depends mostly on the nature of data in hand, and on the inference task which has to be performed.
One of the lines of difference in interpretation is whether to treat the regressors as random variables, or as predefined constants. In the first case (random design) the regressors x_i are random and sampled together with the y_i ‘s from some population,  as in an observational study. This approach allows for more natural study of the asymptotic properties of the estimators. In the other interpretation (fixed design), the regressors X are treated as known constants set by a design, and y is sampled conditionally on the values of X as in an experiment. For practical purposes, this distinction is often unimportant, since estimation and inference is carried out while conditioning on X. All results stated in this article are within the random design framework.
The primary assumption of OLS is that there is zero or negligible errors in the independent variable, since this method only attempts to minimize the mean squared error in the dependent variable.
Classical linear regression model
The classical model focuses on the “finite sample” estimation and inference, meaning that the number of observations n is fixed. This contrasts with the other approaches, which study the asymptotic behavior of OLS, and in which the number of observations is allowed to grow to infinity.
Correct specification. The linear functional form is correctly specified.
Strict exogeneity. The errors in the regression should have conditional mean zero (Hayashi, 2000):
E[? ??|X]=0.
The immediate consequence of the exogeneity assumption is that the errors have mean zero: E[?]=0, and that the regressors are uncorrelated with the errors: E[X'?]=0.
The exogeneity assumption is critical for the OLS theory. If it holds then the regressor variables are called exogenous (Hayashi, 2000). If it doesn’t, then those regressors that are correlated with the error term are called endogenous, and then the OLS estimates become invalid. In such case the method of instrumental variables may be used to carry out inference.
No linear dependence. The regressors in X must all be linearly independent. Mathematically it means that the matrix X must have full column rank almost surely (Hayashi, 2000):
Pr["rank"(X)=p]=1
Usually, it is also assumed that the regressors have finite moments up to at least second. In such case the matrix Q_xx=E[X'X/n] will be finite and positive semi-definite. When this assumption is violated the regressors are called linearly dependent or perfectly multicollinear. In such case the value of the regression coefficient ? cannot be learned, although prediction of y values is still possible for new values of the regressors that lie in the same linearly dependent subspace.
Spherical errors:
Var[? ??|X]=?^2 I_n
where I_n is an n×n identity matrix, and ?^2 is a parameter which determines the variance of each observation. This ?^2 is considered a nuisance parameter in the model, although usually it is also estimated. If this assumption is violated then the OLS estimates are still valid, but no longer efficient. It is customary to split this assumption into two parts:
Homoscedasticity: E[?_i^2 |X]=?^2, which means that the error term has the same variance ?^2 in each i observation. When this requirement is violated this is called heteroscedasticity, in such case a more efficient estimator would be weighted least squares. If the errors have infinite variance then the OLS estimates will also have infinite variance (although by the law of large numbers they will nonetheless tend toward the true values so long as the errors have zero mean). In this case, robust estimation techniques are recommended.
Nonautocorrelation: the errors are uncorrelated between observations: E[?_i ?_j |X]=0 for i?j. This assumption may be violated in the context of time series data, panel data, cluster samples, hierarchical data, repeated measures data, longitudinal data, and other data with dependencies. In such cases generalized least squares provides a better alternative than the OLS.
Normality. It is sometimes additionally assumed that the errors have normal distribution conditional on the regressors (Hayashi, 2000):
? ??|X~N(0,?^2 I_n ).
This assumption is not needed for the validity of the OLS method, although certain additional finite-sample properties can be established in case when it does (especially in the area of hypotheses testing). Also when the errors are normal, the OLS estimator is equivalent to the maximum likelihood estimator (MLE), and therefore it is asymptotically efficient in the class of all regular estimators.
Independent and identically distributed
In some applications, especially with cross-sectional data, an additional assumption is imposed — that all observations are independent and identically distributed (iid). This means that all observations are taken from a random sample which makes all the assumptions listed earlier simpler and easier to interpret. Also this framework allows one to state asymptotic results (as the sample size n ? ?), which are understood as a theoretical possibility of fetching new independent observations from the data generating process. The list of assumptions in this case is:
iid observations: (x_i,y_i) is independent from, and has the same distribution as, (x_j,y_j) for all i?j;
no perfect multicollinearity: Q_xx=E[x_i x_j^']is a positive-definite matrix;
exogeneity: E[?_i |x_i]=0
homoscedasticity: Var[?_i |x_i]=?^2
Time series model
The stochastic process {x_i,y_i} is stationary and ergodic;
The regressors are predetermined: E[x_i ?_i]=0 for all i=1,…,n;
The p×p matrix Q_xx=E[x_i x_j^'] is of full rank, and hence positive-definite;
{x_i ?_i} is a martingale difference sequence, with a finite matrix of second moments Q_(xx?^2 )=E[?_i^2 x_i x_j^']
Estimation
Suppose b is a “candidate” value for the parameter ?. The quantity y_i-x_i^' b is called the residual for the i-th observation, it measures the vertical distance between the data point (x_i,y_i) and the hyperplane y=x'b, and thus assesses the degree of fit between the actual data and the model. The sum of squared residuals (SSR) (also called the error sum of squares (ESS) or residual sum of squares (RSS)) (Hayashi, 2000) is a measure of the overall model fit:
S(b)=?_(i=1)^n?(y_i-x_i^' b)^2 =(y-Xb)^T (y-Xb),
where T denotes the matrix transpose. The value of b, which minimizes this sum is called the OLS estimator for ?. The function S(b) is quadratic in b with positive-definite Hessian, and therefore this function possesses a unique global minimum at b=? ?, which can be given by the explicit formula (Hayashi, 2000):
? ?=arg?min?(b?R^p )?S(b) =(1/n ?_(i=1)^n??x_i x_i^' ?)^(-1)?1/n ?_(i=1)^n??x_i y_i ?
or equivalently in matrix form,
? ?=(X^T X)^(-1) X^T y.
After we have estimated ?, the fitted values (or predicted values) from the regression will be
y ?=X? ?=Py,

where P=X?(X^T X)?^(-1) X^T is the projection matrix onto the space spanned by the columns of X. This matrix P is also sometimes called the hat matrix because it “puts a hat” onto the variable y. Another matrix, closely related to P is the annihilator matrix M=I_n-P, this is a projection matrix onto the space orthogonal to X. Both matrices P and M are symmetric and idempotent (meaning that P^2=P), and relate to the data matrix X via identities PX=X and MX=0 (Hayashi, 2000).
Matrix M creates the residuals from the regression: 
? ?=y-X? ?=My=M?
Using these residuals we can estimate the value of ?^2:
s^2=(? ?'? ?)/(n-p)=y'My/(n-p)=S(? ? )/(n-p),? ?^2=(n-p)/n s^2
The numerator, n-p, is the statistical degrees of freedom. The first quantity, s^2, is the OLS estimate for ?^2, whereas the second, ? ?^2, is the MLE estimate for ?^2. The two estimators are quite similar in large samples; the first one is always unbiased, while the second is biased but minimizes the mean squared error of the estimator. In practice s^2 is used more often, since it is more convenient for the hypothesis testing. The square root of s^2 is called the standard error of the regression (SER), or standard error of the equation (SEE).
It is common to assess the goodness-of-fit of the OLS regression by comparing how much the initial variation in the sample can be reduced by regressing onto X. The coefficient of determination R^2 is defined as a ratio of “explained” variance to the “total” variance of the dependent variable y (Hayashi, 2000):
R^2=(??(y ?_i-y ? )^2 )/(??(y_i-y ? )^2 )=y'P'LPy/y'Ly=1-y'My/y'Ly=1-SSR/TSS
where TSS is the total sum of squares for the dependent variable, L=I_n-11'/n, and 1 is an n×1 vector of ones. (L is a “centering matrix” which is equivalent to regression on a constant; it simply subtracts the mean from a variable.)
In order for R^2 to be meaningful, the matrix X of data on regressors must contain a column vector of ones to represent the constant whose coefficient is the regression intercept. In that case, R^2 will always be a number between 0 and 1, with values close to 1 indicating a good degree of fit.
Simple regression model
If the data matrix X contains only two variables: a constant, and a scalar regressor x_i, then this is called the “simple regression model” (Hayashi, 2000). This case is often considered in the beginner statistics classes, as it provides much simpler formulas even suitable for manual calculation. The vectors of parameters in such model is 2-dimensional, and is commonly denoted as (?,?):
y_i=?+?x_i+?_i.
The least squares estimates in this case are given by simple formulas
? ?=(???x_i y_i ?-1/n ??x_i  ??y_i )/(??x_i^2 -1/n (??x_i )^2 )="Cov" [x,y]/"Var" [x] ,? ?=y ?-? ?x ?
Alternative derivations
In the previous section the least squares estimator ? ? was obtained as a value that minimizes the sum of squared residuals of the model. However it is also possible to derive the same estimator from other approaches. In all cases the formula for OLS estimator remains the same: ? ?=?(X'X)?^(-1) X'y, the only difference is in how we interpret this result.
Geometric approach
For mathematicians, OLS is an approximate solution to an overdetermined system of linear equations X??y, where ? is the unknown. Assuming the system cannot be solved exactly (the number of equations n is much larger than the number of unknowns p), we are looking for a solution that could provide the smallest discrepancy between the right- and left- hand sides. In other words, we are looking for the solution that satisfies
? ?=arg?min????y-X?? 
where ||·|| is the standard L^2 norm in the n-dimensional Euclidean space R^n. The predicted quantity X? is just a certain linear combination of the vectors of regressors. Thus, the residual vector y-X? will have the smallest length when y is projected orthogonally onto the linear subspace spanned by the columns of X. The OLS estimator in this case can be interpreted as the coefficients of vector decomposition of y ?=Py along the basis of X (see Figure 15-3).
 
Figure 15-3. Depiction of OLS concept in three-space
OLS estimation can be viewed as a projection onto the linear space spanned by the regressors.
Another way of looking at it is to consider the regression line to be a weighted average of the lines passing through the combination of any two points in the dataset (Akbarzadeh, 2013). Although this way of calculation is more computationally expensive, it provides a better intuition on OLS.
Maximum likelihood
The OLS estimator is identical to the maximum likelihood estimator (MLE) under the normality assumption for the error terms (Hayashi, 2000). This normality assumption has historical importance, as it provided the basis for the early work in linear regression analysis by Yule and Pearson. From the properties of MLE, we can infer that the OLS estimator is asymptotically efficient (in the sense of attaining the Cramér-Rao bound for variance) if the normality assumption is satisfied (Hayashi, 2000).
Generalized method of moments
In iid case the OLS estimator can also be viewed as a GMM estimator arising from the moment conditions
E[x_i (y_i-x_i^' ?)]=0.
These moment conditions state that the regressors should be uncorrelated with the errors. Since x_i is a p-vector, the number of moment conditions is equal to the dimension of the parameter vector ?, and thus the system is exactly identified. This is the so-called classical GMM case, when the estimator does not depend on the choice of the weighting matrix.
Note that the original strict exogeneity assumption E[?_i |x]=0 implies a far richer set of moment conditions than stated above. In particular, this assumption implies that for any vector-function ƒ, the moment condition E[ƒ(x_i)·?_i]=0 will hold. However it can be shown using the Gauss–Markov theorem that the optimal choice of function ƒ is to take ƒ(x)=x, which results in the moment equation posted above.
Finite sample properties
First of all, under the strict exogeneity assumption the OLS estimators ? ? and s^2 are unbiased, meaning that their expected values coincide with the true values of the parameters (Hayashi, 2000):
E[? ? ? ?|X]=?,E[? s^2 ?|X]=?^2.
If the strict exogeneity does not hold (as is the case with many time series models, where exogeneity is assumed only with respect to the past shocks but not the future ones), then these estimators will be biased in finite samples.
The variance-covariance matrix of ? ? is equal to (Hayashi, 2000)
"Var" [? ? ? ?|X]=?^2 (X'X)^(-1).
In particular, the standard error of each coefficient ? ?_j is equal to square root of the j-th diagonal element of this matrix. The estimate of this standard error is obtained by replacing the unknown quantity ?^2 with its estimate s^2.
Thus,
(se) ?(? ?_j )=?(s^2 (X'X)_jj^(-1) )
It can also be easily shown that the estimator is ? ? uncorrelated with the residuals from the model (Hayashi, 2000):
"Cov" [? ? ?(,?) ? ?|X]=0.
The Gauss–Markov theorem states that under the spherical errors assumption (that is, the errors should be uncorrelated and homoscedastic) the estimator is efficient in the class of linear unbiased estimators. This is called the best linear unbiased estimator (BLUE). Efficiency should be understood as if we were to find some other estimator ? ? which would be linear in y and unbiased, then (Hayashi, 2000)
"Var" [? ? ? ?|X]-"Var" [? ? ? ?|X]
in the sense that this is a nonnegative-definite matrix. This theorem establishes optimality only in the class of linear unbiased estimators, which is quite restrictive. Depending on the distribution of the error terms ?, other, non-linear estimators may provide better results than OLS.
Assuming normality
The properties listed so far are all valid regardless of the underlying distribution of the error terms. However if you are willing to assume that the normality assumption holds (that is, that ?~N(0,?^2 I_n)), then additional properties of the OLS estimators can be stated.
The estimator ? ? is normally distributed, with mean and variance as given before (Amemiya, 1985):
? ?~N(?,?^2 (X'X)^(-1) )
This estimator reaches the Cramér–Rao bound for the model, and thus is optimal in the class of all unbiased estimators. Note that unlike the Gauss–Markov theorem, this result establishes optimality among both linear and non-linear estimators, but only in the case of normally distributed error terms.
The estimator s^2 will be proportional to the chi-squared distribution (Amemiya, 1985):
s^2~?^2/(n-p)??_(n-p)^2
The variance of this estimator is equal to 2?^4/(n - p), which does not attain the Cramér–Rao bound of 2?^4/n. However it was shown that there are no unbiased estimators of ?^2 with variance smaller than that of the estimator s^2 (Rao, 1973). If we are willing to allow biased estimators, and consider the class of estimators that are proportional to the sum of squared residuals (SSR) of the model, then the best (in the sense of the mean squared error) estimator in this class will be ? ?2=SSR/(n-p+2), which even beats the Cramér–Rao bound in case when there is only one regressor (p=1) (Amemiya, 1985).
Moreover, the ? ? estimators and s^2 are independent (Amemiya, 1985), the fact which comes in useful when constructing the t- and F-tests for the regression.
Influential observations
As was mentioned before, the estimator ? ? is linear in y, meaning that it represents a linear combination of the dependent variables y_i’s. The weights in this linear combination are functions of the regressors X, and generally are unequal. The observations with high weights are called influential because they have a more pronounced effect on the value of the estimator.
To analyze which observations are influential we remove a specific j-th observation and consider how much the estimated quantities are going to change (similarly to the jackknife method). It can be shown that the change in the OLS estimator for ? will be equal to (Davidson & Mackinnon, 1993)
? ?^((j) )-? ?=-1/(1-h_j ) (X^' X)^(-1) x_j^' ?_j,
where h=x_j ?(X'X)?^(-1) x_j is the j-th diagonal element of the hat matrix P, and x_j is the vector of regressors corresponding to the j-th observation. Similarly, the change in the predicted value for j-th observation resulting from omitting that observation from the dataset will be equal to (Davidson & Mackinnon, 1993)
y ?_j^((j) )-y ?_j=x_j^' ? ?^((j) )-x_j^' ? ?=-h_j/(1-h_j ) ? ?_j
From the properties of the hat matrix, 0?h_j?1, and they sum up to p, so that on average h_j? p/n. These quantities h_j are called the leverages, and observations with high h_j’s — leverage points (Davidson & Mackinnon, 1993). Usually the observations with high leverage ought to be scrutinized more carefully, in case they are erroneous, or outliers, or in some other way atypical of the rest of the dataset.
Partitioned regression
Sometimes the variables and corresponding parameters in the regression can be logically split into two groups, so that the regression takes form
y=X_1 ?_1+X_2 ?_2+?,
where X_1 and X_2 have dimensions n×p , n×p , and ?_1, ?_2 are p×1 and p×1 vectors, with p_1+p_2=p.
The Frisch–Waugh–Lovell theorem states that in this regression the residuals ? ? and the OLS estimate ? ?_2 will be numerically identical to the residuals and the OLS estimate for ?_2 in the following regression (Davidson & Mackinnon, 1993):
M_1 y=M_1 X_2 ?_2+?,
where M_1 is the annihilator matrix for regressors X_1 .
The theorem can be used to establish a number of theoretical results. For example, having a regression with a constant and another regressor is equivalent to subtracting the means from the dependent variable and the regressor and then running the regression for the demeaned variables but without the constant term.
Constrained estimation
Suppose it is known that the coefficients in the regression satisfy a system of linear equations
H_0:Q^' ?=c
where Q is a p×q matrix of full rank, and c is a q×1 vector of known constants, where q<p. In this case least squares estimation is equivalent to minimizing the sum of squared residuals of the model subject to the constraint H_0 .The constrained least squares (CLS) estimator can be given by an explicit formula (Amemiya, 1985):
? ?^c=? ?-(X'X)^(-1) Q(Q'(X'X)^(-1) Q)^(-1) (Q^('? ? )-c)
This expression for the constrained estimator is valid as long as the matrix X'X is invertible. It was assumed from the beginning of this article that this matrix is of full rank, and it was noted that when the rank condition fails, ? will not be identifiable. However it may happen that adding the restriction H_0 makes ? identifiable, in which case one would like to find the formula for the estimator. The estimator is equal to (Amemiya, 1985)
? ?^c=R(R'X'XR)^(-1) R^' X^' y+(I_p-R(R^' X^' XR)^(-1) R'X'X)Q(Q'Q)^(-1) c,
where R is a p×(p-q) matrix such that the matrix [QR] is non-singular, and R'Q=0. Such a matrix can always be found, although generally it is not unique. The second formula coincides with the first in case when X'X is invertible (Amemiya, 1985).
Large sample properties
The least squares estimators are point estimates of the linear regression model parameters ?. However, generally we also want to know how close those estimates might be to the true values of parameters. In other words, we want to construct the interval estimates.
Since we haven’t made any assumption about the distribution of error term ?_i, it is impossible to infer the distribution of the estimators ? ? and ? ?^2. Nevertheless, we can apply the law of large numbers and central limit theorem to derive their asymptotic properties as sample size n goes to infinity. While the sample size is necessarily finite, it is customary to assume that n is “large enough” so that the true distribution of the OLS estimator is close to its asymptotic limit, and the former may be approximately replaced by the latter.
We can show that under the model assumptions, the least squares estimator for ? is consistent (that is ? ? converges in probability to ?) and asymptotically normal:
?n (? ?-?) ?(??d ) N(0,?^2 Q_xx^(-1) ),
where Q_xx=X'X.
Using this asymptotic distribution, approximate two-sided confidence intervals for the j-th component of the vector ? ? can be constructed as at the 1-? confidence level,
?_j?[? ?_j±q_((1-?)?2)^N(0,1)  ?(1/n ? ?^2 [Q_xx^(-1) ]_jj )]
where q denotes the quantile function of standard normal distribution, and ?[·]?_jj is the j-th diagonal element of a matrix.
Similarly, the least squares estimator for ?^2 is also consistent and asymptotically normal (provided that the fourth moment of ?_i exists) with limiting distribution
?n (? ?^2-?^2 ) ?(??d ) N(0,E[?_i^4 ]-?^4 ).
These asymptotic distributions can be used for prediction, testing hypotheses, constructing other estimators, etc. As an example consider the problem of prediction. Suppose x_0 is some point within the domain of distribution of the regressors, and one wants to know what the response variable would have been at that point. The mean response is the quantity y_0=x_0^' ?, whereas the predicted response is y ?_0=x_0^' ? ?. Clearly the predicted response is a random variable, its distribution can be derived from that of	:
?n (y ?_0-y_0 ) ?(??d ) N(0,?^2 x_0^' Q_xx^(-1) x_0 ),
which allows construct confidence intervals for mean response y_0 to be constructed:
y_0?[x_0^' ? ?±q_((1-?)?2)^N(0,1)  ?(1/n ? ?^2 x_0^' Q_xx^(-1) x_0 )]
 at the 1-? confidence level.
Example with real data
This example exhibits the common mistake of ignoring the condition of having zero error in the dependent variable.
The following data set gives average heights and weights for American women aged 30–39 (source: The World Almanac and Book of Facts, 1975). The data are plotted in Figure 15-4.
n	1	2	3	4	5	6	7	8
Height (m):	1.47	1.50	1.52	1.55	1.57	1.60	1.63	1.65
Weight (kg):	52.21	53.12	54.48	55.84	57.20	58.57	59.93	61.29

n	9	10	11	12	13	14	15
Height (m):	1.68	1.70	1.73	1.75	1.78	1.80	1.83
Weight (kg):	63.11	64.47	66.28	68.10	69.92	72.19	74.46

 
Figure 15-4. Plot of the height and weightdata
Scatterplot of the data; the relationship is slightly curved but close to linear.
When only one dependent variable is being modeled, a scatterplot will suggest the form and strength of the relationship between the dependent variable and regressors. It might also reveal outliers, heteroscedasticity, and other aspects of the data that may complicate the interpretation of a fitted regression model. The scatterplot suggests that the relationship is strong and can be approximated as a quadratic function. OLS can handle non-linear relationships by introducing the regressor ?HEIGHT?^2. The regression model then becomes a multiple linear model:
w_i=?_1+?_2 h_i+?_e h_i^2+?_i.
The output from most popular statistical packages will look similar to this, as wellas Figure 15-5:
Method: Least Squares Dependent variable: WEIGHT 
Included observations: 15

Variable	Coefficient		Std. Error	t-statistic	p-value

?	128.8128		16.3083	7.8986	0.0000
h
	–143.1620		19.8332	–7.2183	0.0000
h^2
	61.9603		6.0084	10.3122	0.0000

R^2	0.9989		S.E. of regression	0.2516
Adjusted R^2	0.9987		Model sum-of-sq	692.61
Log-likelihood
1.0890		Residual sum-of-sq	0.7595
Durbin–Watson stats.	2.1013		Total sum-of-sq	693.37
Akaike criterion	0.2548		F-statistic	5471.2
Schwarz criterion
0.3964		p-value (F-stat)	0.0000

In this table:
The Coefficient column gives the least squares estimates of parameters ?_j
The Std. errors column shows standard errors of each coefficient estimate: (1/n ? ?^2 [Q_xx^(-1) ]_jj )^(1?2) 
The t-statistic and p-value columns are testing whether any of the coefficients might be equal to zero. The t-statistic is calculated simply as t=? ?_j?? ?_j . If the errors ? follow a normal distribution, t follows a Student-t distribution. Under weaker conditions, t is asymptotically normal. Large values of t indicate that the null hypothesis can be rejected and that the corresponding coefficient is not zero. The second column, p-value, expresses the results of the hypothesis test as a significance level. Conventionally, p-values smaller than 0.05 are taken as evidence that the population coefficient is nonzero.
R-squaredR2 is the coefficient of determination indicating goodness-of-fit of the regression. This statistic will be equal to one if fit is perfect, and to zero when regressors X have no explanatory power whatsoever. This is a biased estimate of the population R-squared, and will never decrease if additional regressors are added, even if they are irrelevant.
Adjusted R-squaredR2 is a slightly modified version of R^2, designed to penalize for the excess number of regressors which do not add to the explanatory power of the regression. This statistic is always smaller than R2R2, can decrease as new regressors are added, and even be negative for poorly fitting models:
R ?^2=1-(n-1)/(n-p) (1-R^2 )
Log-likelihood is calculated under the assumption that errors follow normal distribution. Even though the assumption is not very reasonable, this statistic may still find its use in conducting LR tests.
Durbin–Watson statistic tests whether there is any evidence of serial correlation between the residuals. As a rule of thumb, the value smaller than 2 will be an evidence of positive correlation.
Akaike information criterion and Schwarz criterion are both used for model selection. Generally when comparing two alternative models, smaller values of one of these criteria will indicate a better model (Burnham & Anderson, 2002).
Standard error of regression is an estimate of ?, standard error of the error term.
Total sum of squares, model sum of squared, and residual sum of squares tell us how much of the initial variation in the sample were explained by the regression.
F-statistic tries to test the hypothesis that all coefficients (except the intercept) are equal to zero. This statistic has F(p–1,n–p) distribution under the null hypothesis and normality assumption, and its p-value indicates probability that the hypothesis is indeed true. Note that when errors are not normal this statistic becomes invalid, and other tests such as for example Wald test or LR test should be used.
 
Figure 15-5. Fitted Regression
Ordinary least squares analysis often includes the use of diagnostic plots designed to detect departures of the data from the assumed form of the model. These are some of the common diagnostic plots:
Residuals against the explanatory variables in the model. A non-linear relation between these variables suggests that the linearity of the conditional mean function may not hold. Different levels of variability in the residuals for different levels of the explanatory variables suggests possible heteroscedasticity (see Figure 15-6).
Residuals against explanatory variables not in the model. Any relation of the residuals to these variables would suggest considering these variables for inclusion in the model.
Residuals against the fitted values, y ?.
Residuals against the preceding residual. This plot may identify serial correlations in the residuals.
 
Figure15-6. Residuals plot

An important consideration when carrying out statistical inference using regression models is how the data were sampled. In this example, the data are averages rather than measurements on individual women. The fit of the model is very good, but this does not imply that the weight of an individual woman can be predicted with high accuracy based only on her height.
Sensitivity to rounding
This example also demonstrates that coefficients determined by these calculations are sensitive to how the data is prepared. The heights were originally given rounded to the nearest inch and have been converted and rounded to the nearest centimeter. Since the conversion factor is one inch to 2.54 cm this is not an exact conversion. The original inches can be recovered by Round(x/0.0254) and then re-converted to metric without rounding. If this is done the results become:
Constant	Height	?"Height" ?^2	
128.8128	-143.1620 	61.96033 	Converted to metric with rounding.
119.0205	-131.5076	58.50460 	Converted to metric without rounding.

Using either of these equations to predict the weight of a 5’ 6” (1.6764m) woman gives similar values: 62.94 kg with rounding vs. 62.98 kg without rounding. Thus a seemingly small variation in the data has a real effect on the coefficients but a small effect on the results of the equation.
While this may look innocuous in the middle of the data range it could become significant at the extremes or in the case where the fitted model is used to project outside the data range (extrapolation).
This highlights a common error: this example is an abuse of OLS which inherently requires that the errors in the dependent variable (in this case height) are zero or at least negligible. The initial rounding to nearest inch plus any actual measurement errors constitute a finite and non-negligible error. As a result the fitted parameters are not the best estimates they are presumed to be. Though not totally spurious the error in the estimation will depend upon relative size of the x and y errors.
Software
All major statistical software packages perform least squares regression analysis and inference. Simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators. While many statistical software packages can perform various types of nonparametric and robust regression, these methods are less standardized; different software packages implement different methods, and a method with a given name may be implemented differently in different packages. Specialized regression software has been developed for use in fields such as survey analysis and neuroimaging.
Open Source software that perform OLS include DAP, Octave, R, and Weke. Commercial software that perform OLS include Analytica, SPSS Modeler, MATLAB, SAS Enterprise and STATISTICA.
?
Example Using R
Now let’s look at an example concerning the number of species of tortoise on the various Galapagos Islands. There are 30 cases (Islands) and 7 variables in the dataset. We start by reading the data into R and examining it. Copy the following data and save it as a text file: gala.txt (tab delimited) in a directory, like “C/mydata/”. (download at: http://acropora.bio.mq.edu.au/wp-content/uploads/2013/02/gala.txt)
Island	Spec ies	Ende mics	Area	Eleva tion	Near est	Scruz	Adja cent
Baltra	58	23	25.09	346	0.6	0.6	1.84
Bartolome	31	21	1.24	109	0.6	26.3	572.33
Caldwell	3	3	0.21	114	2.8	58.7	0.78
Champion	25	9	0.1	46	1.9	47.4	0.18
Coamano	2	1	0.05	77	1.9	1.9	903.82
Daphne.Major	18	11	0.34	119	8	8	1.84
Daphne.Minor	24	0	0.08	93	6	12	0.34
Darwin	10	7	2.33	168	34.1	290.2	2.85
Eden	8	4	0.03	71	0.4	0.4	17.95
Enderby	2	2	0.18	112	2.6	50.2	0.1
Espanola	97	26	58.27	198	1.1	88.3	0.57
Fernandina	93	35	634.49	1494	4.3	95.3	4669.32
Gardner1	58	17	0.57	49	1.1	93.1	58.27
Gardner2	5	4	0.78	227	4.6	62.2	0.21
Genovesa	40	19	17.35	76	47.4	92.2	129.49
Isabela	347	89	4669.32	1707	0.7	28.1	634.49
Marchena	51	23	129.49	343	29.1	85.9	59.56
Onslow	2	2	0.01	25	3.3	45.9	0.1
Pinta	104	37	59.56	777	29.1	119.6	129.49
Pinzon	108	33	17.95	458	10.7	10.7	0.03
Las.Plazas	12	9	0.23	94	0.5	0.6	25.09
Rabida	70	30	4.89	367	4.4	24.4	572.33
SanCristobal	280	65	551.62	716	45.2	66.6	0.57
SanSalvador	237	81	572.33	906	0.2	19.8	4.89
SantaCruz	444	95	903.82	864	0.6	0	0.52
SantaFe	62	28	24.08	259	16.5	16.5	0.52
SantaMaria	285	73	170.92	640	2.6	49.2	0.1
Seymour	44	16	1.84	147	0.6	9.6	25.09
Tortuga	16	8	1.24	186	6.8	50.9	17.95
Wolf	21	12	2.85	253	34.1	254.7	2.33

Change the working directory to “c:/mydata” (do this by choosing “File”->“Change directory”, and then inputting the directory name). And now you can simply read the data into R by: 
> gala <- read.table(“gala.txt”) # read the data into R

Now we can take a look at the data.
> data(gala)
> gala
     Species Endemics Area Elevation Nearest Scruz Adjacent
   Baltra 58       23 25.09      346     0.6   0.6     1.84
Bartolome 31       21 1.24       109     0.6  26.3   572.33
--- cases deleted ---
 Tortuga  16        8 1.24       186     6.8  50.9    17.95
    Wolf  21       12 2.85       253    34.1 254.7     2.33

The variables are
Species 	The number of species of tortoise found on the island
Endemics 	The number of endemic species
Elevation 	The highest elevation of the island (m)
Nearest 	The distance from the nearest island (km)
Scruz 		The distance from Santa Cruz island (km)
Adjacent 	The area of the adjacent island (km2)

The data were presented by Johnson and Raven (Johnson & Raven, 1973) and also appear in Weisberg (Weisberg, 2005). We have filled in some missing values for simplicity. Fitting a linear model in R is done using the lm() command. Notice the syntax for specifying the predictors in the model. This is the so-called Wilkinson-Rogers notation. In this case, since all the variables are in the gala data frame, we must use the data= argument.

First, we generate a series of plots of various species-area relations. We consider three models: Linear, Gleason, and log-Arrhenius.
1. Linear model:  S~"Normal"(?,?^2) with identity link such that ?=?_0+?_1 A.
2. Gleason model:  S~"Normal"(?,?^2) with identity link such that ?=?_0+?_1 "log" A.
3. log-Arrhenius model:  S~"Normal"(?,?^2) with identity link such that ?=?_0 A^(?_1 ).
Model 1 is a Linear Model. We fit a linear model on the original scale, model 1 above and obtain the log-likelihood (with logLik) and the AIC (with AIC).
> model1<-lm(Species~Area,data=gala)
> logLik(model1)
‘log Lik.’ -177.0993 (df=3)
> AIC(model1)
[1] 360.1985

We superimpose the fitted model on a scatter plot of the data (see Figure 15-7).
> plot(gala$Area,gala$Species, xlab=‘Area’, ylab=‘Species’)
> abline(model1,col=2, lty=2)
> mtext(‘Model 1: linear model’, side=3, line=.5)

 
Figure 15-7. Fitted model and scatterplot of the data
Model 2 is a Gleason Model. The Gleason model requires a log-transformed predictor, but an untransformed response (see Figure 15-8).
> model2<-lm(Species~log(Area),data=gala)
> logLik(model2)
‘log Lik.’ -169.9574 (df=3)
> AIC(model2)
[1] 345.9147

> plot(log(gala$Area), gala$Species, xlab=‘log(Area)’, 
+	ylab=‘Species’)
> abline(model2,col=2,lty=2)
> mtext( ‘Model 2: Gleason model‘, side=3, line=.5)

 
Figure 15-8. Fitted Gleason model of the previous data
Model 3 is a log-Arrhenius. The log-Arrhenius model is just an ordinary regression model in which both the response and predictor are log-transformed.
> model3<-lm(log(Species)~log(Area), data=gala)
> logLik(model3)
‘log Lik.’ -34.23973 (df=3)
> AIC(model3)
[1] 74.47946

> plot(log(gala$Area), log(gala$Species), xlab=‘log(Area)’, 
+	ylab=‘log(Species)’)
> abline(model3,col=2,lty=2)
> mtext(‘Model 3: log-Arrhenius model‘, side=3, line=.5)
 
Figure 15-9. Fitted log-Arrhenius model of the previous data

Now we want to compare the models. We begin by collecting the names of our models.
> model.names<-c(‘Linear’, ‘Gleason’, ‘Log-Arrhenius’)

Next we concatenate all the calculated log-likelihoods together.
> loglike <-c(logLik(model1),logLik(model2),logLik(model3))

Next we concatenate a list of the number of parameters estimated for each model. Each model has three parameters—?_0, ?_1, and ?^2. 
> numparms<-c(3,3,3)

Finally, we create our function to carry out the calculations.
# LL is loglikelihood,
# K is number of estimated parameters
# n is the sample size 
> AIC.func<-function(LL,K,n,modelnames)
+ {
+ AIC<- -2*LL + 2*K
+  AICc<-AIC + 2*K*(K+1)/(n-K-1)
+  output<-cbind(LL,K,AIC,AICc)
+  colnames(output)<-c(‘LogL’,’K’,’AIC’,’AICc’)
+  minAICc<-min(output[,”AICc”])
+  deltai<-output[,”AICc”]-minAICc
+  rel.like<-exp(-deltai/2)
+  wi<-round(rel.like/sum(rel.like),3)
+  out<-data.frame(modelnames,output,deltai,wi)
+  out
+  }
> AIC.func(loglike,numparms,dim(gala)[1],model.names)

  modelnames      LogL    K       AIC      AICc   deltai wi
1 Linear       -177.09927 3 360.19853 361.12161 285.7191  0
2 Gleason      -169.95737 3 345.91473 346.83781 271.4353  0
3 Log-Arrhenius -34.23973 3  74.47946  75.40253   0.0000  1

Based on the output we see there is only one model that has any empirical support, the log-Arrhenius model. For reasons we will not explain here, although the log-Arrhenius model fits well, it is not guaranteed to be optimal. For the Galapagos data, a model that allows there to be heteroscedasticity on the scale of the original response is to be preferred.
The previous models only examine the Species-Area relationship. We now consider additional variables: Endemics, Elevation, Nearest, Scruz, and Adjacent. We will call our model gfit.
> gfit <- lm(Species~Area + Elevation + Nearest + Scruz + 
+	Adjacent, data=gala)
> summary(gfit)
Call:
> lm(formula = Species~Area + Elevation + Nearest + Scruz + 
+	Adjacent, data=gala)
> summary(gfit)

Call:
lm(formula = Species ~ Area + Elevation + Nearest + Scruz + Adjacent, 
    data = gala)

Residuals:
     Min       1Q   Median       3Q      Max 
-111.679  -34.898   -7.862   33.460  182.584 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.068221  19.154198   0.369 0.715351    
Area        -0.023938   0.022422  -1.068 0.296318    
Elevation    0.319465   0.053663   5.953 3.82e-06 ***
Nearest      0.009144   1.054136   0.009 0.993151    
Scruz       -0.240524   0.215402  -1.117 0.275208    
Adjacent    -0.074805   0.017700  -4.226 0.000297 ***
---
Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ‘ 1

Residual standard error: 60.98 on 24 degrees of freedom
Multiple R-squared:  0.7658,    Adjusted R-squared:  0.7171 
F-statistic:  15.7 on 5 and 24 DF,  p-value: 6.838e-07

We can identify several useful quantities in this output. Other statistical packages tend to produce output quite similar to this. One useful feature of R is that it is possible to directly calculate quantities of interest. Of course, it is not necessary here because the lm() function does the job but it is very useful when the statistic you want is not part of the pre-packaged functions.
First we make the X-matrix
> x <- cbind(1,gala[,-c(1,2)])

and here’s the response y:
> y <- gala$Species

Now let’s construct X’X: t() does transpose and %*% does matrix multiplication:
> t(x) %*% x
Error: %*% requires numeric matrix/vector arguments

Gives a somewhat cryptic error. The problem is that matrix arithmetic can only be done with numeric values but x here derives from the data frame type. Data frames are allowed to contain character variables, which would disallow matrix arithmetic. We need to force x into the matrix form:
> x <- as.matrix(x)
> t(x) %*% x
                 1        Area  Elevation   Nearest
1            30.00     7851.26    11041.0    301.80
Area       7851.26 23708665.46 10852798.5  39240.84
Elevation 11041.00 10852798.53  9218227.0 109139.20
Nearest     301.80    39240.84   109139.2   8945.30
Scruz      1709.30   275516.84   616237.8  34527.34
Adjacent   7832.95  5950313.65  8553187.9  37196.67
              Scruz    Adjacent
1           1709.30     7832.95
Area      275516.84  5950313.65
Elevation 616237.80  8553187.95
Nearest    34527.34    37196.67
Scruz     231613.77   534409.98
Adjacent  534409.98 23719568.46

Inverses can be taken using the solve() command:
> xtxi <- solve(t(x) %*% x)
> xtxi
                      1          Area     Elevation
1          9.867829e-02  3.778242e-05 -1.561976e-04
Area       3.778242e-05  1.352247e-07 -2.593617e-07
Elevation -1.561976e-04 -2.593617e-07  7.745339e-07
Nearest   -2.339027e-04  1.294003e-06 -3.549366e-06
Scruz     -3.760293e-04 -4.913149e-08  3.080831e-07
Adjacent   2.309832e-05  4.620303e-08 -1.640241e-07
                Nearest         Scruz      Adjacent
1         -2.339027e-04 -3.760293e-04  2.309832e-05
Area       1.294003e-06 -4.913149e-08  4.620303e-08
Elevation -3.549366e-06  3.080831e-07 -1.640241e-07
Nearest    2.988732e-04 -3.821077e-05  1.424729e-06
Scruz     -3.821077e-05  1.247941e-05 -1.958356e-07
Adjacent   1.424729e-06 -1.958356e-07  8.426543e-08

A somewhat more direct way to get _ (X’X)^(-1) is as follows:
> gfit <- lm(Species~Area+Elevation+Nearest+Scruz+Adjacent,
+	data=gala)
> gs <- summary(gfit)
> gs$cov.unscaled
              (Intercept)          Area     Elevation
(Intercept)  9.867829e-02  3.778242e-05 -1.561976e-04
Area         3.778242e-05  1.352247e-07 -2.593617e-07
Elevation   -1.561976e-04 -2.593617e-07  7.745339e-07
Nearest     -2.339027e-04  1.294003e-06 -3.549366e-06
Scruz       -3.760293e-04 -4.913149e-08  3.080831e-07
Adjacent     2.309832e-05  4.620303e-08 -1.640241e-07
                  Nearest         Scruz      Adjacent
(Intercept) -2.339027e-04 -3.760293e-04  2.309832e-05
Area         1.294003e-06 -4.913149e-08  4.620303e-08
Elevation   -3.549366e-06  3.080831e-07 -1.640241e-07
Nearest      2.988732e-04 -3.821077e-05  1.424729e-06
Scruz       -3.821077e-05  1.247941e-05 -1.958356e-07
Adjacent     1.424729e-06 -1.958356e-07  8.426543e-08

The names()command is the way to see the components of an Splus object - you can see that there are other useful quantities that are directly available:
> names(gs)
> names(gfit)

In particular, the fitted (or predicted) values and residuals are
> gfit$fit
      Baltra    Bartolome     Caldwell     Champion 
 116.7259460   -7.2731544   29.3306594   10.3642660 
     Coamano Daphne.Major Daphne.Minor       Darwin 
 -36.3839155   43.0877052   33.9196678   -9.0189919 
        Eden      Enderby     Espanola   Fernandina 
  28.3142017   30.7859425   47.6564865   96.9895982 
    Gardner1     Gardner2     Genovesa      Isabela 
  -4.0332759   64.6337956   -0.4971756  386.4035578 
    Marchena       Onslow        Pinta       Pinzon 
  88.6945404    4.0372328  215.6794862  150.4753750 
  Las.Plazas       Rabida SanCristobal  SanSalvador 
  35.0758066   75.5531221  206.9518779  277.6763183 
   SantaCruz      SantaFe   SantaMaria      Seymour 
 261.4164131   85.3764857  195.6166286   49.8050946 
     Tortuga         Wolf 
  52.9357316   26.7005735

> gfit$res
      Baltra    Bartolome     Caldwell     Champion 
  -58.725946    38.273154   -26.330659    14.635734 
     Coamano Daphne.Major Daphne.Minor       Darwin 
   38.383916   -25.087705    -9.919668    19.018992 
        Eden      Enderby     Espanola   Fernandina 
  -20.314202   -28.785943    49.343513    -3.989598 
    Gardner1     Gardner2     Genovesa      Isabela 
   62.033276   -59.633796    40.497176   -39.403558 
    Marchena       Onslow        Pinta       Pinzon 
  -37.694540    -2.037233  -111.679486   -42.475375 
  Las.Plazas       Rabida SanCristobal  SanSalvador 
  -23.075807    -5.553122    73.048122   -40.676318 
   SantaCruz      SantaFe   SantaMaria      Seymour 
  182.583587   -23.376486    89.383371    -5.805095 
     Tortuga         Wolf 
  -36.935732    -5.700573

We can get ? ??directly:
> xtxi %*% t(x) %*% y
                  [,1]
1          7.068220709
Area      -0.023938338
Elevation  0.319464761
Nearest    0.009143961
Scruz     -0.240524230
Adjacent  -0.074804832 

or in a computationally efficient and stable manner:
> solve(t(x) %*% x, t(x) %*% y)
             [,1]
[1,]  7.068220709
[2,] -0.023938338
[3,]  0.319464761
[4,]  0.009143961
[5,] -0.240524230
[6,] -0.074804832

We can estimate ??using the estimator in the text:
> root1<-sum((gfit$res)^2)
> sqrt(root1/(30-6))
[1] 60.97519

Compare this to the results above (Residual standard error).
We may also obtain the standard errors for the coefficients. Also diag() returns the diagonal of a matrix):
> sqrt(diag(xtxi))*60.97519
          1        Area   Elevation     Nearest 
19.15419834  0.02242235  0.05366281  1.05413598 
      Scruz    Adjacent 
 0.21540225  0.01770019

Finally we may compute R^2:
> 1-sum((gfit$res)ˆ2)/sum((y-mean(y))ˆ2)
[1] 0.7658469

?
Exercises
	For this exercise, you will use the crime dataset that appears in Statistical Methods for Social Sciences, Third Edition by Alan Agresti and Barbara Finlay (Prentice Hall, 1997). The variables are state id (sid), state name (state), violent crimes per 100,000 people (crime), murders per 1,000,000 (murder), the percent of the population living in metropolitan areas (pctmetro), the percent of the population that is white (pctwhite), percent of population with a high school education or above (pcths), percent of population living under poverty line (poverty), and percent of population that are single parents (single). It has 51 observations. You are going to use poverty and single to predict crime. The data may be downloaded at http://www.ats.ucla.edu/stat/data/crime.dta. 
	Use the data frame Gpa from the BSDA package to:
	Create a scatterplot of CollGPA versus HSGPA.
	Find the least squares estimates of ?0 and ?1 using the R
	function lm().
	Add the least squares line to the scatterplot created in 1 using the R function abline(). 
?
?
Generalized linear model

In statistics, the generalized linear model (GLM)—not  to be confused with general linear model or generalized least squares—is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.
Generalized linear models were formulated by John Nelder and Robert Wedderburn as a way of unifying various other statistical models, including linear regression, logistic regression and Poisson regression (Nelder & Wedderburn, 1972). They proposed an iteratively reweighted least squares method for maximum likelihood estimation of the model parameters. Maximum-likelihood estimation remains popular and is the default method on many statistical computing packages. Other approaches, including Bayesian approaches and least squares fits to variance stabilized responses, have been developed.
Intuition
Ordinary linear regression predicts the expected value of a given unknown quantity (the response variable, a random variable) as a linear combination of a set of observed values (predictors). This implies that a constant change in a predictor leads to a constant change in the response variable (i.e. a linear-response model). This is appropriate when the response variable has a normal distribution (intuitively, when a response variable can vary essentially indefinitely in either direction with no fixed “zero value”, or more generally for any quantity that only varies by a relatively small amount, e.g. human heights).
However, these assumptions are inappropriate for many types of response variables. For example, in many cases when the response variable must be positive and can vary over a wide scale, constant input changes lead to geometrically varying rather than constantly varying output changes. As an example, a model that predicts that each decrease in 10 degrees Fahrenheit leads to 1,000 fewer people going to a given beach is unlikely to generalize well over both small beaches (e.g. those where the expected attendance was 50 at the lower temperature) and large beaches (e.g. those where the expected attendance was 10,000 at the lower temperature). An even worse problem is that, since the model also implies that a drop in 10 degrees leads 1,000 fewer people going to a given beach, a beach whose expected attendance was 50 at the higher temperature would now be predicted to have the impossible attendance value of -950. Logically, a more realistic model would instead predict a constant rate of increased beach attendance (e.g. an increase in 10 degrees leads to a doubling in beach attendance, and a drop in 10 degrees leads to a halving in attendance). Such a model is termed an exponential-response model (or log-linear model, since the logarithm of the response is predicted to vary linearly).
Similarly, a model that predicts a probability of making a yes/no choice (a Bernoulli variable) is even less suitable as a linear-response model, since probabilities are bounded on both ends (they must be between 0 and 1). Imagine, for example, a model that predicts the likelihood of a given person going to the beach as a function of temperature. A reasonable model might predict, for example, that a change in 10 degrees makes a person two times more or less likely to go to the beach. But what does “twice as likely” mean in terms of a probability? It cannot literally mean to double the probability value (e.g. 50% becomes 100%, 75% becomes 150%, etc.). Rather, it is the odds that are doubling: from 2:1 odds, to 4:1 odds, to 8:1 odds, etc. Such a model is a log-odds model.
Generalized linear models cover all these situations by allowing for response variables that have arbitrary distributions (rather than simply normal distributions), and for an arbitrary function of the response variable (the link function) to vary linearly with the predicted values (rather than assuming that the response itself must vary linearly). For example, the case above of predicted number of beach attendees would typically be modeled with a Poisson distribution and a log link, while the case of predicted probability of beach attendance would typically be modeled with a Bernoulli distribution (or binomial distribution, depending on exactly how the problem is phrased) and a log-odds (or logit) link function.
Overview
In a generalized linear model (GLM), each outcome of the dependent variables, Y, is assumed to be generated from a particular distribution in the exponential family, a large range of probability distributions that includes the normal, binomial, Poisson and gamma distributions, among others. The mean, ?, of the distribution depends on the independent variables, X, through:
E(Y)=?=g^(-1) (X?),
where E(Y) is the expected value of Y; X? is the linear predictor, a linear combination of unknown parameters, ?; g is the link function.
In this framework, the variance is typically a function, V, of the mean:
"Var" (Y)=V(?)=V(g^(-1) (X?)).
It is convenient if V follows from the exponential family distribution, but it may simply be that the variance is a function of the predicted value.
The unknown parameters, ?, are typically estimated with maximum likelihood, maximum quasi-likelihood, or Bayesian techniques.
Model components
The GLM consists of three elements:
1. A probability distribution from the exponential family.
2. A linear predictor ?=X?.
3. A link function g such that E(Y)=?=g-1(?).
Probability distribution
The overdispersed exponential family of distributions is a generalization of the exponential family and exponential dispersion model of distributions and includes those probability distributions, parameterized by ? and	, whose density functions f (or probability mass function, for the case of a discrete distribution) can be expressed in the form
f_Y (y??,?)=h(y,?)"exp" ((b(?)T(y)-A(?))/d(?) ).
?, called the dispersion parameter, typically is known and is usually related to the variance of the distribution. The functions h(y,?), b(?), T(y), A(?), and d(?) are known. Many common distributions are in this family.
For scalar Y and ?, this reduces to
f_Y (y??,?)=h(y,?)"exp" ((b(?)T(y)-A(?))/d(?) ).
? is related to the mean of the distribution. If b(?) is the identity function, then the distribution is said to be in canonical form (or natural form). Note that any distribution can be converted to canonical form by rewriting ? as ?' and then applying the transformation ?=b(?'). It is always possible to convert A(?) in terms of the new parameterization, even if b(?')is not a one-to-one function; see comments in the page on the exponential family. If, in addition, T(y)is the identity and ? is known, then ? is called the canonical parameter (or natural parameter) and is related to the mean through
?=E(Y)=?A(?).
For scalar Y and ?, this reduces to
?=E(Y)=A^'(?) .
Under this scenario, the variance of the distribution can be shown to be (McCullagh & Nelder, 1989)
"Var" (Y)=??^T A(?)d(?).
For scalar Y and ?, this reduces to
"Var" (Y)=A''(?)d(?).
Linear predictor
The linear predictor is the quantity which incorporates the information about the independent variables into the model. The symbol ? (Greek “eta”) denotes a linear predictor. It is related to the expected value of the data (thus, “predictor”) through the link function.
? is expressed as linear combinations (thus, “linear”) of unknown parameters ?. The coefficients of the linear combination are represented as the matrix of independent variables X. ? can thus be expressed as
?=X?.
Link function
The link function provides the relationship between the linear predictor and the mean of the distribution function. There are many commonly used link functions, and their choice can be somewhat arbitrary. It makes sense to try to match the domain of the link function to the range of the distribution function’s mean.
When using a distribution function with a canonical parameter ?, the canonical link function is the function that expresses ? in terms of ?, i.e. ?=b(?). For the most common distributions, the mean ? is one of the parameters in the standard form of the distribution’s density function, and then b(?) is the function as defined above that maps the density function into its canonical form. When using the canonical link function, b(?)=?=X?, which allows X^TY  to be a sufficient statistic for ?.
Following is a table of several exponential-family distributions in common use and the data they are typically used for, along with the canonical link functions and their inverses (sometimes referred to as the mean function, as done here).
Common distributions with typical uses and canonical link functions
Distribution	Support of distribution	Typical uses	Link name	Link function	Mean function
Normal	real: (-?,?)	Linear-response data
Identity	X?=?	?=X?
Exponential	real: (0,?)	Exponential-response data, scale parameters	Inverse	X?=-?^(-1)

	?=(-X?)^(-1)


Gamma					
Inverse
Gaussian	real: (0,?)		Inverse squared	X?=-?^(-2)
	?=(-X?)^((-1)?2)
Poisson	integer: (0,?)	count of occurrences in fixed amount of time/space	Log	X?=ln?(?)

	?=e^X?


Bernoulli
integer: [0,1]	outcome of single yes/no occurrence	Logit	X?=ln?(?/(1-?))






	?=1/(1+e^X? )








Binomial	integer: [0,N]	count of # of “yes” occurrences out of N yes/no occurrences			
Categorical	integer: [0,K]	outcome of single K-way occurrence			
	K-vector of integer: [0,1], where exactly one element in the vector has the
value 1				
Multinomial	K-vector of [0,N]	count of occurrences of different types (1 .. K) out of N total K-way occurrences			

In the cases of the exponential and gamma distributions, the domain of the canonical link function is not the same as the permitted range of the mean. In particular, the linear predictor may be negative, which would give an impossible negative mean. When maximizing the likelihood, precautions must be taken to avoid this. An alternative is to use a noncanonical link function.
Note also that in the case of the Bernoulli, binomial, categorical and multinomial distributions, the support of the distributions is not the same type of data as the parameter being predicted. In all of these cases, the predicted parameter is one or more probabilities, i.e. real numbers in the range [0,1]. The resulting model is known as logistic regression (or multinomial logistic regression in the case that K-way rather than binary values are being predicted).
For the Bernoulli and binomial distributions, the parameter is a single probability, indicating the likelihood of occurrence of a single event. The Bernoulli still satisfies the basic condition of the generalized linear model in that, even though a single outcome will always be either 0 or 1, the expected value will nonetheless be a real-valued probability, i.e. the probability of occurrence of a “yes” (or 1) outcome. Similarly, in a binomial distribution, the expected value is Np, i.e. the expected proportion of “yes” outcomes will be the probability to be predicted.
For categorical and multinomial distributions, the parameter to be predicted is a K-vector of probabilities, with the further restriction that all probabilities must add up to 1. Each probability indicates the likelihood of occurrence of one of the K possible values. For the multinomial distribution, and for the vector form of the categorical distribution, the expected values of the elements of the vector can be related to the predicted probabilities similarly to the binomial and Bernoulli distributions.
Fitting
Maximum likelihood
The maximum likelihood estimates can be found using an iteratively reweighted least squares algorithm using either a Newton–Raphson method with updates of the form:
?^((t+1) )=?^((t) )+J^(-1) (?^((t) ) )u(?^((t) ) ),
where J(?^((t) ) )is the observed information matrix (the negative of the Hessian matrix) and u(?^((t) ) ) is the score function; or a Fisher’s scoring method:
?^((t+1) )=?^((t) )+I^(-1) (?^((t) ) )u(?^((t) ) ),
where I(?^((t) ) ) is the Fisher information matrix. Note that if the canonical link function is used, then they are the same.
Bayesian methods
In general, the posterior distribution cannot be found in closed form and so must be approximated, usually using Laplace approximations or some type of Markov chain Monte Carlo method such as Gibbs sampling.
Examples
General linear models
A possible point of confusion has to do with the distinction between generalized linear models and the general linear model, two broad statistical models. The general linear model may be viewed as a special case of the generalized linear model with identity link and responses normally distributed. As most exact results of interest are obtained only for the general linear model, the general linear model has undergone a somewhat longer historical development. Results for the generalized linear model with non-identity link are asymptotic (tending to work well with large samples).
Linear regression
A simple, very important example of a generalized linear model (also an example of a general linear model) is linear regression. In linear regression, the use of the least-squares estimator is justified by the Gauss-Markov theorem, which does not assume that the distribution is normal.
From the perspective of generalized linear models, however, it is useful to suppose that the distribution function is the normal distribution with constant variance and the link function is the identity, which is the canonical link if the variance is known.
For the normal distribution, the generalized linear model has a closed form expression for the maximum-likelihood estimates, which is convenient. Most other GLMs lack closed form estimates.
Binomial data
When the response data, Y, are binary (taking on only values 0 and 1), the distribution function is generally chosen to be the Bernoulli distribution and the interpretation of ?_i is then the probability, p, of Y_i taking on the value one.
There are several popular link functions for binomial functions; the most typical is the canonical logit link:
g(p)="ln" (p/(1-p)).
GLMs with this setup are logistic regression models (or logit models).
In addition, the inverse of any continuous cumulative distribution function (CDF) can be used for the link since the CDF‘s range is [0,1], the range of the binomial mean. The normal CDF ? is a popular choice and yields the probit model. Its link is
g(p)=?^(-1) (p).
The reason for the use of the probit model is that a constant scaling of the input variable to a normal CDF (which can be absorbed through equivalent scaling of all of the parameters) yields a function that is practically identical to the logit function, but probit models are more tractable in some situations than logit models. (In a Bayesian setting in which normally distributed prior distributions are placed on the parameters, the relationship between the normal priors and the normal CDF link function means that a probit model can be computed using Gibbs sampling, while a logit model generally cannot.)
The complementary log-log function "log"(-"log"(1-p)) may also be used. This link function is asymmetric and will often produce different results from the probit and logit link functions. 
The identity link is also sometimes used for binomial data to yield the linear probability model, but a drawback of this model is that the predicted probabilities can be greater than one or less than zero. In implementation it is possible to fix the nonsensical probabilities outside of [0,1], but interpreting the coefficients can be difficult. The model’s primary merit is that near p=0.5 it is approximately a linear transformation of  the probit and logit?econometricians sometimes call this the Harvard model.
The variance function for binomial data is given by:
"Var" (Y_i )=??_i (1-?_i ),
where the dispersion parameter ? is typically fixed at exactly one. When it is not, the resulting quasi-likelihood model often described as binomial with overdispersion or quasi-binomial.
Multinomial regression
The binomial case may be easily extended to allow for a multinomial distribution as the response (also, a Generalized Linear Model for counts, with a constrained total). There are two ways in which this is usually done:
Ordered response
If the response variable is an ordinal measurement, then one may fit a model function of the form:
g(?_m )=?_m=?_0+X_1 ?_1+?+X_p ?_p+?_2+?+?_m
=?_1+?_2+?+?_m,
where ?_m=P(Y?m), for m>2. Different links g lead to proportional odds models or ordered probit models.
Unordered response
If the response variable is a nominal measurement, or the data do not satisfy the assumptions of an ordered model, one may fit a model of the following form:
g(?_m )=?_m=?_(m,0)+X_1 ?_(m,1)+?+X_p ?_(m,p),
where ?_m=P(Y=m?|Y?{1,m}?), for m>2. Different links g lead to multinomial logit or multinomial probit models. These are more general than the ordered response models, and more parameters are estimated.
Count data
Another example of generalized linear models includes Poisson regression which models count data using the Poisson distribution. The link is typically the logarithm, the canonical link. The variance function is proportional to the mean
"var" (Y_i )=??_i,
where the dispersion parameter ? is typically fixed at exactly one. When it is not, the resulting quasi-likelihood model is often described as Poisson with overdispersion or quasi-Poisson.
Extensions
Correlated or clustered data
The standard GLM assumes that the observations are uncorrelated. Extensions have been developed to allow for correlation between observations, as occurs for example in longitudinal studies and clustered designs:
•  Generalized estimating equations (GEEs) allow for the correlation between observations without the use of an explicit probability model for the origin of the correlations, so there is no explicit likelihood. They are suitable when the random effects and their variances are not of inherent interest, as they allow for the correlation without explaining its origin. The focus is on estimating the average response over the population (“population-averaged” effects) rather than the regression parameters that would enable prediction of the effect of changing one or more components of X on a given individual. GEEs are usually used in conjunction with Huber-White standard errors (Zeger, Liang, & Albert, 1988) (Hardin & Hilbe, 2003).
•  Generalized linear mixed models (GLMMs) are an extension to GLMs that includes random effects in the linear predictor, giving an explicit probability model that explains the origin of the correlations. The resulting “subject-specific” parameter estimates are suitable when the focus is on estimating the effect of changing one or more components of X on a given individual. GLMMs are also referred to as multilevel models and as mixed model. In general, fitting GLMMs is more computationally complex and intensive than fitting GEEs.
Generalized additive models
Generalized additive models (GAMs) are another extension to GLMs in which the linear predictor ? is not restricted to be linear in the covariates X but is the sum of smoothing functions applied to the x_i’s:
?=?_0+f_1 (x_1 )+f_2 (x_2 )+?.
The smoothing functions f_i are estimated from the data. In general this requires a large number of data points and is computationally intensive (Hastie & Tibshirani, 1990) (Wood, 2006).
The model relates a univariate response variable, Y, to some predictor variables, x_i. An exponential family distribution is specified for Y (for example normal, binomial or Poisson distributions) along with a link function g (for example the identity or log functions) relating the expected value of Y to the predictor variables via a structure such as
g(E(Y))=?=?_0+f_1 (x_1 )+f_2 (x_2 )+?f_m (x_m ).
The functions f_i (x_i ) may be functions with a specified parametric form (for example a polynomial, or a coefficient depending on the levels of a factor variable) or maybe specified non-parametrically, or semi-parametrically, simply as ‘smooth functions’, to be estimated by non-parametric means. So a typical GAM might use a scatterplot smoothing function, such as a locally weighted mean, for f_1 (x_1 ), and then use a factor model for f_2 (x_2 ). This flexibility to allow non-parametric fits with relaxed assumptions on the actual relationship between response and predictor, provides the potential for better fits to data than purely parametric models, but arguably with some loss of interpretablity.
Generalized additive model for location, scale and shape
The generalized additive model location, scale and shape (GAMLSS) is a class of statistical model that provides extended capabilities compared to the simpler generalized linear models and generalized additive models. These simpler models allow the typical values of a quantity being modelled to be related to whatever explanatory variables are available. Here the “typical value” is more formally a location parameter, which only describes a limited aspect of the probability distribution of the dependent variable. The GAMLSS approach allows other parameters of the distribution to be related to the explanatory variables; where these other parameters might be interpreted as scale and shape parameters of the distribution, although the approach is not limited to such parameters.
In GAMLSS the exponential family distribution assumption for the response variable, (Y), (essential in GLMs and GAMs), is relaxed and replaced by a general distribution family, including highly skew and/or kurtotic continuous and discrete distributions.
The systematic part of the model is expanded to allow modeling not only of the mean (or location) but other parameters of the distribution of Y as linear and/or nonlinear, parametric and/or additive non-parametric functions of explanatory variables and/or random effects.
GAMLSS is especially suited for modeling leptokurtic or platykurtic and/or positive or negative skew response variable. For count type response variable data it deals with over-dispersion by using proper over-dispersed discrete distributions. Heterogeneity also is dealt with by modeling the scale or shape parameters using explanatory variables. There are several packages written in R related to GAMLSS models (Stasinopoulos & Rigby, 2007).
A GAMLSS model assumes independent observations y_i for i=1,…,n with probability (density) function f(y_i??_i,?_i,?_i,?_i ) conditional on (?_i,?_i,?_i,?_i ) a vector of four distribution parameters, each of which can be a function to the explanatory variables. The first two population distribution parameters ?_i and ?_i are usually characterized as location and scale parameters, while the remaining parameter(s), if any, are characterized as shape parameters, e.g. skewness and kurtosis parameters, although the model may be applied more generally to the parameters of any population distribution with up to four distribution parameters, and can be generalized to more than four distribution parameters (Stasinopoulos & Rigby, 2007).
g_1 (?)=?_1=X_1 ?_1+?_(j=1)^(J_1)??h_j1 (x_j1 ) ?
g_2 (?)=?_2=X_2 ?_2+?_(j=1)^(J_2)??h_j2 (x_j2 ) ?
g_3 (?)=?_23=X_3 ?_3+?_(j=1)^(J_3)??h_j3 (x_j3 ) ?
g_4 (?)=?_4=X_4 ?_4+?_(j=1)^(J_4)??h_j4 (x_j4 ) ?.
Where ?,?,?,? and n_k are vectors of length n, ?_k^T=(?_1k,?_2k,…,?_(J_k^' k) ) is a parameter vector of length J_k^', X_k is a fixed known design matrix of order n×J_k^' and h_jk is a smooth non-parametric function of explanatory variable x_jk, j=1,…,j_k and k=1,2,3,4 (Nelder & Wedderburn, 1972).
Confusion with general linear models
The term “generalized linear model“, and especially its abbreviation GLM, can be confused with general linear model. John Nelder has expressed regret about this in a conversation with Stephen Senn:
Senn: I must confess to having some confusion when I was a young statistician between general linear models and generalized linear models. Do you regret the terminology?
Nelder: I think probably I do. I suspect we should have found some more fancy name for it that would have stuck and not been confused with the general linear model, although general and generalized are not quite the same. I can see why it might have been better to have thought of something else (Senn, 2003).
Software
All the primary software packages discussed in Chapter 2 have this functionality.
Example 1 Using R
Viewing the data
Use data “rats” from “survival” package in R.
Data on 150 rats contain identifying “litter”, ““rx” (indicator of injection of drug after initial administration of carcinogen), time in days on study (ignored initially, and “status” which is indicator of tumor, our binary response variable.
library(survival)
summary(rats)

     litter           rx              time       
 Min.   : 1.0   Min.   :0.0000   Min.   : 34.00  
 1st Qu.:13.0   1st Qu.:0.0000   1st Qu.: 78.25  
 Median :25.5   Median :0.0000   Median : 94.50  
 Mean   :25.5   Mean   :0.3333   Mean   : 89.43  
 3rd Qu.:38.0   3rd Qu.:1.0000   3rd Qu.:104.00  
 Max.   :50.0   Max.   :1.0000   Max.   :104.00  
     status      
 Min.   :0.0000  
 1st Qu.:0.0000  
 Median :0.0000  
 Mean   :0.2667  
 3rd Qu.:1.0000  
 Max.   :1.0000
 
plot(rats) #see Figure 16-1
 
Figure 16-1. scatterplot matrix of the rats data
From the plots, it should be obvious that status and rx are binary variables, while time is a numeric (interval) variable.
Setting up the Model
We will build three models here, so we begin numbering them. This first model is a GLM with one dependent variable, rx. We also model the response variable as a binomial distributed random variable.
fitB1 = glm(cbind(status,1-status) ~ rx, family=binomial, data = rats)

NOTE you can use the column headers as variable names if you specify the data-frame using “data=“
fitB1

Call:  glm(formula = cbind(status, 1 - status) ~ rx, family = binomial, 
    data = rats)

Coefficients:
(Intercept)           rx  
     -1.450        1.127  

Degrees of Freedom: 149 Total (i.e. Null);  148 Residual
Null Deviance:      174 
Residual Deviance: 165.3        AIC: 169.3

summary(fitB1)$coef

             Estimate Std. Error   z value     Pr(>|z|)
(Intercept) -1.450010  0.2549063 -5.688404 1.282324e-08
rx           1.127237  0.3835089  2.939272 3.289845e-03

Coefficients fitted by MLE, link=“logit“ is default for binomial-family data: this is logistic regression.
The standard error is found as sqrt of diagonal in variance:
sqrt(diag(summary(fitB1)$cov.scaled))

(Intercept)          rx 
  0.2549063   0.3835089 

“Deviance” = “Residual Deviance” = -2*logLik

c(fitB1$deviance, -2*logLik(fitB1))

[1] 165.2738 165.2738

“Null.deviance” is -2 times the logLik for the same model with only a constant term 
c(fitB1$null.dev, -2*logLik(update(fitB1, formula = .~ 1)))

[1] 173.9746 173.9746

NOTE the use of the “update” function to refit a model of the same type as the one previously done: in the model formula, the left-hand side term is a “.” to indicate that it is the same as before. We have changed the right-hand side from rx (which automatically included intercept along with rx predictor) to one with “1” or intercept alone.
Next use “update” to change the fitB1 model not in its model formula but in its specified link within the binomial “family”.
fitB2 = update(fitB1, family=binomial(link=“probit”))
rbind(logit= fitB1$coef, probit= fitB2$coef, rescal.probit = fitB2$coef/0.5513)

              (Intercept)        rx
logit          -1.4500102 1.1272368
probit         -0.8778963 0.6760028
rescal.probit  -1.5924112 1.2261977


Recall that we use deviance and differences between them because Wilks’ Theorem says that 2 times the difference between logLik for a model with  p  extra parameter dimensions versus logLik for a base model is equal to approximately a chi-square (p, df) variate when the base model is the true one (see Chapter 10).
Model Quality
LET’s use this idea to examine the quality of the model with predictor log(time) in the model along with rx.
NOTE that we must put the expression log(time) within I( ) to have it evaluated and constructed as a new predictor within the glm fitting function.
fitB3 = update(fitB1, formula= . ~ . + I(log(time)))

The “data” is the same now as in fitB1, so “time” is the column in the data-frame, and a log(time) predictor is created under “glm”

It is reasonable to use “time” as a predictor, because the range of times is not too different for rats who generate tumors, so “time” is not really a response variable.
summary(rats$time[rats$status==1])

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  34.00   66.75   80.00   77.28   92.50  104.00 
summary(rats$time[rats$status==0])

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  45.00   83.50  104.00   93.85  104.00  104.00 

cbind(rats[1:10,], model.matrix(fitB3)[1:10,])

   litter rx time status (Intercept) rx I(log(time))
1       1  1  101      0           1  1     4.615121
2       1  0   49      1           1  0     3.891820
3       1  0  104      0           1  0     4.644391
4       2  1  104      0           1  1     4.644391
5       2  0  102      0           1  0     4.624973
6       2  0  104      0           1  0     4.644391
7       3  1  104      0           1  1     4.644391
8       3  0  104      0           1  0     4.644391
9       3  0  104      0           1  0     4.644391
10      4  1   77      0           1  1     4.343805

The first 4 columns are the original “rats” data. The last 3 are the design matrix columns created by glm. So we can look at the new model fit for significance of coefficients:
summary(fitB3)$coef

              Estimate Std. Error   z value     Pr(>|z|)
(Intercept)  17.868661  4.4924813  3.977459 6.965559e-05
rx            1.194019  0.4284626  2.786751 5.323935e-03
I(log(time)) -4.355407  1.0180848 -4.278040 1.885462e-05

Or alternatvely compare deviances or logLik’s with fitB1
c(2*(logLik(fitB3)-logLik(fitB1)), fitB1$dev-fitB3$dev)

[1] 24.37307 24.37307    

This is LRT stat to be compared with chisq 1 df.
1-pchisq(24.373,1)

[1] 7.937339e-07       

This is still highly significant but somewhat different p-value from the I(log(time)) coef probably because the model is still far from the right one.
We could try to enter additional terms like log(time)^2 or rx * log(time)
fitB4 = update(fitB3, .~. + I(rx*log(time)) + I(log(time)^2))
summary(fitB4)$coef

                    Estimate Std. Error    z value
(Intercept)        -9.784968  52.690740 -0.1857056
rx                -13.501206   8.732837 -1.5460274
I(log(time))       10.179730  24.364938  0.4178024
I(rx * log(time))   3.324780   1.973389  1.6848069
I(log(time)^2)     -1.871215   2.819823 -0.6635932
                    Pr(>|z|)
(Intercept)       0.85267560
rx                0.12209794
I(log(time))      0.67609159
I(rx * log(time)) 0.09202582
I(log(time)^2)    0.50695069

This time, the new variables do NOT look significant which we can check also through deviances:
fitB3$dev - fitB4$dev

[1] 3.901065      

This result is to be compared with chisq 2df, so it is not at all significant.
We can also do these deviance comparisons all at once by looking at an “analysis of deviance” table
anova(fitB4)

Analysis of Deviance Table
Model: binomial, link: logit

Response: cbind(status, 1 - status)
Terms added sequentially (first to last)

                  Df Deviance Resid. Df Resid. Dev
NULL                                149     173.97
rx                 1   8.7008       148     165.27
I(log(time))       1  24.3731       147     140.90
I(rx * log(time))  1   3.4784       146     137.42
I(log(time)^2)     1   0.4226       145     137.00

As in an ANOVA table, in which RSS replaced Deviance these “Deviance values” are the amounts by which the current model-fitting line decreases the deviance: (recall that fitB1 uses rx only, fitB3 augments by log(time), and fitB4 by  log(time) plus the 2 additional rx*log(tim) and log(time)^2  terms.

Devs = c(fitB1$null.dev, fitB1$dev, fitB3$dev, update(fitB3, .~.+I(rx*log(time)))$dev, fitB4$dev)
Devs

[1] 173.9746 165.2738 140.9007 137.4223 136.9997

round (-diff(Devs), 3 )### successive differences of llks

[1]  8.701 24.373  3.478  0.423      ### give Deviance col in “anova”

 
Figure 16-2. Residual plots for the three model 1
 
Figure 16-3. Residual plots for the three model 2
 
Figure 16-4. Residual plots for the three model 4
 
Figure 16-5. Residual plots for the three model 4
Example 2 Using R – Models Comparison
In Example 1, we looked at two basic type of GLMs, binomial (logit) and binomial (probit). Both of these were in the binomial distribution family according to the type of response variable. We now considered other types of GLMs and compare them accordingly. Using the Longley dataset, a macroeconomic data set which provides a well-known example for a highly collinear regression.
Viewing the Data
A data frame with 7 economic variables, observed yearly from 1947 to 1962 (n=16).
	GNP.deflator: GNP implicit price deflator (1954=100)
	GNP: Gross National Product
	Unemployed: number of unemployed
	Armed.Forces: number of people in the armed forces
	Population: ‘noninstitutionalized’ population ? 14 years of age
	Year: the year (time)
	Employed: number of people employed

 
Figure 16-6. scatterplot matrix of the data
Here we will see the ability of R to discriminate between most appropriate function forms of the following models. We should expect, given the highly collinear nature of the regression lm(Employed ~ .”) should be modeled with the Gaussian family
	binomial(link = "logit")
	binomial(link = "probit")
	gaussian(link = "identity")
	poisson(link = "log")
	quasi(link = "identity", variance = "constant")

require(stats); require(graphics)
longley.x <- data.matrix(longley[, 1:6])
summary(longley.x)

  GNP.deflator         GNP          Unemployed   
 Min.   : 83.00   Min.   :234.3   Min.   :187.0  
 1st Qu.: 94.53   1st Qu.:317.9   1st Qu.:234.8  
 Median :100.60   Median :381.4   Median :314.4  
 Mean   :101.68   Mean   :387.7   Mean   :319.3  
 3rd Qu.:111.25   3rd Qu.:454.1   3rd Qu.:384.2  
 Max.   :116.90   Max.   :554.9   Max.   :480.6  
  Armed.Forces     Population         Year     
 Min.   :145.6   Min.   :107.6   Min.   :1947  
 1st Qu.:229.8   1st Qu.:111.8   1st Qu.:1951  
 Median :271.8   Median :116.8   Median :1954  
 Mean   :260.7   Mean   :117.4   Mean   :1954  
 3rd Qu.:306.1   3rd Qu.:122.3   3rd Qu.:1958  
 Max.   :359.4   Max.   :130.1   Max.   :1962  

longley.y <- longley[, "Employed"]
pairs(longley, main = "longley data")
longley.mod1<-glm(Employed ~ .,data=longley,family=gaussian(identity))
summary(longley.mod1)

Call:
glm(formula = Employed ~ ., family = gaussian(identity), data = longley)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.41011  -0.15767  -0.02816   0.10155   0.45539  

Coefficients:
               Estimate Std. Error t value Pr(>|t|)
(Intercept)  -3.482e+03  8.904e+02  -3.911 0.003560
GNP.deflator  1.506e-02  8.492e-02   0.177 0.863141
GNP          -3.582e-02  3.349e-02  -1.070 0.312681
Unemployed   -2.020e-02  4.884e-03  -4.136 0.002535
Armed.Forces -1.033e-02  2.143e-03  -4.822 0.000944
Population   -5.110e-02  2.261e-01  -0.226 0.826212
Year          1.829e+00  4.555e-01   4.016 0.003037
                
(Intercept)  ** 
GNP.deflator    
GNP             
Unemployed   ** 
Armed.Forces ***
Population      
Year         ** 
---
Signif. codes:  
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 0.09293601)

    Null deviance: 185.00883  on 15  degrees of freedom
Residual deviance:   0.83642  on  9  degrees of freedom
AIC: 14.187

Number of Fisher Scoring iterations: 2

longley.mod1<-glm(formula = Employed ~ ., family = Gamma(inverse), data = longley)
longley.mod2<-glm(formula = Employed ~ ., family = gaussian(identity), data = longley)
longley.mod3<-glm(formula = Employed ~ ., family = poisson(log), data = longley)
longley.mod4<-glm(formula = Employed ~ ., family = inverse.gaussian, data = longley)
rbind(Gamma= longley.mod1$coef, gaussian= longley.mod2$coef, poisson=longley.mod3$coeff, inverse.gaussian=longley.mod4$coef)

                   (Intercept)  GNP.deflator
Gamma             9.216863e-01 -7.325451e-06
gaussian         -3.482259e+03  1.506187e-02
poisson          -5.262691e+01  3.570455e-04
inverse.gaussian  2.907252e-02 -2.786624e-07
                           GNP    Unemployed
Gamma             9.297934e-06  4.783348e-06
gaussian         -3.581918e-02 -2.020230e-02
poisson          -5.788587e-04 -3.111645e-04
inverse.gaussian  2.971755e-07  1.468006e-07
                  Armed.Forces    Population
Gamma             2.126791e-06  3.201065e-05
gaussian         -1.033227e-02 -5.110411e-02
poisson          -1.489846e-04 -1.444313e-03
inverse.gaussian  5.999068e-08  1.277048e-06
                          Year
Gamma            -4.681685e-04
gaussian          1.829151e+00
poisson           2.931733e-02
inverse.gaussian -1.490691e-05

rbind(Gamma= longley.mod1$aic, gaussian= longley.mod2$aic, poisson=longley.mod3$aic, inverse.gaussian=longley.mod4$aic)

                     [,1]
Gamma            15.42875
gaussian         14.18670
poisson               Inf
inverse.gaussian 16.66554

It should be readily apparent that the Gaussian family with the identity link function works best with this collinear regression, and it is not improved upon by other functional forms.
?
Example Using SAS Studio: Bank Marketing Campaign – Mixed Predictors
This data set was obtained from the UC Irvine Machine Learning Repository and contains information related to a direct marketing campaign of a Portuguese banking institution and its attempts to get its clients to subscribe for a term deposit.
Source
The path to this data set is https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip. This data set was obtained by downloading the zip bank.zip, which includes bank-full.csv and bank.csv. The table contains 41,188 rows and 21 columns.
Input Variables (see below: Attribute Information)
There are 20 columns in the table that provide information about each client, such as age, marital status, and education level. A subset of these are related to the last contact of the current campaign, such as the month and day of the week the last contact was made as well as the number of days since the client was last contacted in a previous campaign. There are 10 columns in the table that are categorical, meaning that they contain textual values that correspond to a particular category for a given variable.
Citation Request:
This dataset is public available for research. The details are described in (Moro, Laureano, & Cortez, 2011). Please include this citation if you plan to use this database:
Relevant Information:
The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed. 
Model Goal: The classification goal is to predict if the client will subscribe a term deposit (variable y=RESP).
Number of Instances: 45211 for bank-full.csv (4521 for bank.csv)
Number of Attributes: 16 + output attributes.
Modification:
I modified this data set by converting “Yes”/”No” variables to binary variables with 1 = yes and 0 = no. 
In addition to the response variable for low birth weight (RESP) there are eight variables in the dataset.
	AGE (numeric)
	JOB : type of job (categorical: "admin.", "unknown", "unemployed", "management", "housemaid", "entrepreneur", "student", "blue-collar", "self-employed", "retired", "technician", "services") 
	MARITAL : marital status (categorical: "married", "divorced", "single"; note: "divorced" means divorced or widowed)
	EDUCATION (categorical: "unknown", "secondary", "primary", "tertiary")
	DEFAULT: has credit in default? (binary: "yes", "no")
	BALANCE: average yearly balance, in euros (numeric) 
	HOMEOWNER: has housing loan? (binary: "yes", "no")
	LOANS: has personal loan? (binary: "yes", "no")
# related with the last contact of the current campaign:
	CONTACT: contact communication type (categorical: "unknown", "telephone", "cellular") 
	LENGTH: length of most recent membership (numeric)
	CAMPAIGN: number of contacts performed during this campaign and for this client (numeric, includes last contact)
	PDAYS: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)
	PREVIOUS: number of contacts performed before this campaign and for this client (numeric)
	POUTCOME: outcome of the previous marketing campaign (categorical: "unknown", "other", "failure", "success")
	Output variable (desired target):
RESP - has the client subscribed a term deposit? (binary: 1="yes", 0="no")
Model Code
proc genmod data=WORK.BANKING descending plots = (all) plots=(predicted resraw(index) 
		stdreschi(index) );
	class job marital education / param=glm;
	model RESP=age default balance homeowner loans contact length /
dist=binomial link=logit;
	score out=bank_scores; code;
run;

The “Model Information” tables in Figures 16-7 through 16-10 summarize information about the model we fit, including the model functional form, variable information, and response profile. Figure 16-11 shows the model fit statistics.

Model Information
Data Set	WORK.BANKING
Distribution	Binomial
Link Function	Logit
Dependent Variable	RESP
Figure 16-7. Model Information
?
Number of Observations Read	45212
Number of Observations Used	45211
Number of Events	5289
Number of Trials	45211
Missing Values	1
Figure 16-8. Model Observation Information

Class Level Information
Class	Levels	Values
job	12	admin. blue-collar entrepreneur housemaid management retired self-employe services student technician unemployed unknown
marital	3	divorced married single
education	4	primary secondary tertiary unknown
Figure 16-9. Model Class Level Information

Response Profile
Ordered Value	RESP	Total Frequency
1	1	5289
2	0	39922
Figure 16-10. Model Response Profile
PROC GENMOD is modeling the probability that RESP='1'.

Criteria For Assessing Goodness Of Fit
Criterion	DF	Value	Value/DF
Log Likelihood		-12645.0715	
Full Log Likelihood		-12645.0715	
AIC (smaller is better)		25306.1430	
AICC (smaller is better)		25306.1462	
BIC (smaller is better)		25375.8958	
Figure 16-11. Model Goodness of Fit Information
Algorithm converged
?
The “Analysis of Maximum Likelihood Parameter Estimates” table in Figure 16-12 summarizes maximum likelihood estimates of the model parameters. Figure 16-13 shows the diagnostic plots for the response variable.
Analysis Of Maximum Likelihood Parameter Estimates
Parameter	DF	Estimate	Standard
Error	Wald 95% Confidence Limits	Wald Chi-Square	Pr > ChiSq
Intercept	1	-3.6376	0.0803	-3.7950	-3.4802	2051.59	<.0001
age	1	-0.0047	0.0015	-0.0076	-0.0019	10.48	0.0012
default	1	-0.4109	0.1577	-0.7201	-0.1018	6.79	0.0092
balance	1	0.0000	0.0000	0.0000	0.0000	27.16	<.0001
homeowner	1	-0.8934	0.0356	-0.9632	-0.8237	630.28	<.0001
loans	1	-0.7626	0.0564	-0.8731	-0.6522	183.11	<.0001
contact	1	0.5942	0.0223	0.5505	0.6379	709.62	<.0001
length	1	0.0039	0.0001	0.0037	0.0040	4249.18	<.0001
Scale	0	1.0000	0.0000	1.0000	1.0000		
Figure 16-12. Maximum Likelihood Parameter Estimates
Note:	The scale parameter was held fixed.
 
Figure 16-13. Response diagnostic plots

Model Scoring Code
The code is comprised of two macros. The first contains the score code from the PROC GLM log file and designates the input file BANK_TEST and the output file BANK_SCORE_FILE. The second macro rank the scores from the first macro and places them into twenty groups, named pentiles. The grouped pentiles are then put into a table with PROC TABULATE and the score distribution is plotted using PROC UNIVARIATE.
FILENAME REFFILE "/home/jeff47/sasuser.v94/BANK_TEST.csv" TERMSTR=CR;

PROC IMPORT DATAFILE=REFFILE
	DBMS=CSV
	OUT=WORK.BANK_TEST;
	GETNAMES=YES;
RUN;

PROC CONTENTS DATA=WORK.BANK_TEST; RUN;
%MACRO SCOREPROG(INDATA,OUTDATA);
DATA &OUTDATA;
SET &INDATA;
 *****************************************;
 ** SAS Scoring Code for PROC Genmod;
 *****************************************;
 
 length I_RESP $ 12;
 label I_RESP = 'Into: RESP' ;
 label U_RESP = 'Unnormalized Into: RESP' ;
 format U_RESP BEST12.0;
 
 label P_RESP1 = 'Predicted: RESP=1' ;
 label P_RESP0 = 'Predicted: RESP=0' ;
 
 drop _LMR_BAD;
 _LMR_BAD=0;
 
 *** Check interval variables for missing values;
 if nmiss(age,default,balance,homeowner,loans,contact,length) then do;
    _LMR_BAD=1;
    goto _SKIP_000;
 end;
 
 *** Compute Linear Predictors;
 drop _LP0;
 _LP0 = 0;
 
 *** Effect: age;
 _LP0 = _LP0 + (-0.00473540767201) * age;
 *** Effect: default;
 _LP0 = _LP0 + (-0.41092854687724) * default;
 *** Effect: balance;
 _LP0 = _LP0 + (0.00002289722652) * balance;
 *** Effect: homeowner;
 _LP0 = _LP0 + (-0.89341627896875) * homeowner;
 *** Effect: loans;
 _LP0 = _LP0 + (-0.76261627728163) * loans;
 *** Effect: contact;
 _LP0 = _LP0 + (0.59421068534497) * contact;
 *** Effect: length;
 _LP0 = _LP0 + (0.00385440048199) * length;
 
 *** Predicted values;
 drop _MAXP _IY _P0 _P1;
 _TEMP = -3.63759181008361  + _LP0;
 if (_TEMP < 0) then do;
    _TEMP = exp(_TEMP);
    _P0 = _TEMP / (1 + _TEMP);
 end;
 else _P0 = 1 / (1 + exp(-_TEMP));
 _P1 = 1.0 - _P0;
 P_RESP1 = _P0;
 _MAXP = _P0;
 _IY = 1;
 P_RESP0 = _P1;
 if (_P1 >  _MAXP + 1E-8) then do;
    _MAXP = _P1;
    _IY = 2;
 end;
 select( _IY );
    when (1) do;
       I_RESP = '1' ;
       U_RESP = 1;
    end;
    when (2) do;
       I_RESP = '0' ;
       U_RESP = 0;
    end;
    otherwise do;
       I_RESP = '';
       U_RESP = .;
    end;
 end;
 _SKIP_000:
 if _LMR_BAD = 1 then do;
 I_RESP = '';
 U_RESP = .;
 P_RESP1 = .;
 P_RESP0 = .;
 end;
 drop _TEMP;
 ********************************;
 ***   END SAS SCORING CODE   ***;
 ********************************;
 %MEND SCOREPROG;
 %SCOREPROG(WORK.BANK_TEST,WORK.BANK_SCORE);

 %MACRO PUTPENTS(INDATA,SCORENAME,OUTDATA);
 DATA GETPENT;
 SET &INDATA(KEEP = RESP &SCORENAME);
 RUN;
 PROC SORT DATA=GETPENT; BY P_RESP1; RUN;
 PROC RANK DATA=GETPENT GROUPS=20 OUT=HIGH3;
 	VAR &SCORENAME;
 	RANKS PENTILE;
 RUN;
 DATA &OUTDATA;
 SET HIGH3;
 	RENAME &SCORENAME = SCORE;
 	PENTILE = 20 - PENTILE;
 RUN;
 PROC FREQ DATA=&OUTDATA;
 	TITLE 'FREQ - PENTILES FOR RESP BIRTH WEIGHT MODEL';
 	TABLES PENTILE;
 RUN;
 %MEND;
 %PUTPENTS(WORK.BANK_SCORE,P_RESP1,WORK.BANK_SCORE_OUT);
 
 DATA PER_FILE;
 SET WORK.BANK_SCORE_OUT;
 WHERE SCORE NE .;
 RUN;

 PROC TABULATE DATA=PER_FILE;
 VAR SCORE RESP;
 CLASS PENTILE/ ORDER=UNFORMATTED MISSING;
 TABLE PENTILE, N SCORE*SUM RESP*SUM;
 RUN;

 PROC UNIVARIATE DATA=PER_FILE;
 VAR SCORE;
 HISTOGRAM;
 TITLE "SCORE DISTRIBUTION BANK CAMPAIGN";
 RUN;
Model Score Output
The output is comprised of several tables. I have only included the most important ones for discussion, plus the plot of score distributions.
Data Set Name	WORK.BANK_TEST	Observations	22674
Member Type	DATA	Variables	15
Engine	V9	Indexes	0
Created	04/23/2016 18:42:58	Observation Length	128
Last Modified	04/23/2016 18:42:58	Deleted Observations	0
Protection		Compressed	NO
Data Set Type		Sorted	NO
Label			
Data Representation	SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64		
Encoding	utf-8  Unicode (UTF-8)		

Alphabetic List of Variables and Attributes
#	Variable	Type	Len	Format	Informat
15	RESP	Num	8	BEST12.	BEST32.
4	age	Num	8	BEST12.	BEST32.
5	balance	Num	8	BEST12.	BEST32.
11	campaign	Num	8	BEST12.	BEST32.
9	contact	Num	8	BEST12.	BEST32.
8	default	Num	8	BEST12.	BEST32.
3	education	Char	9	$9.	$9.
6	homeowner	Num	8	BEST12.	BEST32.
1	job	Char	13	$13.	$13.
10	length	Num	8	BEST12.	BEST32.
7	loans	Num	8	BEST12.	BEST32.
2	marital	Char	8	$8.	$8.
12	pdays	Num	8	BEST12.	BEST32.
14	poutcome	Num	8	BEST12.	BEST32.
13	previous	Num	8	BEST12.	BEST32.

?
FREQ - PENTILES FOR RESP BIRTH WEIGHT MODEL

	N	Predicted: RESP=1	RESP
		Sum	Sum
Rank for Variable P_RESP1	1133	754.32	643.00
1			
2	1134	370.70	429.00
3	1134	253.51	361.00
4	1133	198.31	278.00
5	1134	163.99	253.00
6	1134	140.76	176.00
7	1133	123.04	110.00
8	1134	109.45	78.00
9	1134	97.17	59.00
10	1134	84.31	70.00
11	1133	70.64	60.00
12	1134	60.52	54.00
13	1134	52.43	26.00
14	1133	46.01	24.00
15	1134	40.19	9.00
16	1134	33.37	12.00
17	1133	25.99	9.00
18	1134	20.34	5.00
19	1134	15.57	9.00
20	1133	10.21	3.00

SCORE DISTRIBUTION BANK CAMPAIGN
Variable:  SCORE  (Predicted: RESP=1)

Moments
N	22673	Sum Weights	22673
Mean	0.1177982	Sum Observations	2670.83865
Std Deviation	0.15336972	Variance	0.02352227
Skewness	3.07485983	Kurtosis	10.9879094
Uncorrected SS	847.91693	Corrected SS	533.296936
Coeff Variation	130.196996	Std Error Mean	0.00101856


Basic Statistical Measures
Location	Variability
Mean	0.117798	Std Deviation	0.15337
Median	0.068191	Variance	0.02352
Mode	0.016458	Range	0.99702
		Interquartile Range	0.10035

Note: The mode displayed is the smallest of 25 modes with a count of 2.

Tests for Location: Mu0=0
Test	Statistic	p Value
Student's t	t	115.6521	Pr > |t|	<.0001
Sign	M	11336.5	Pr >= |M|	<.0001
Signed Rank	S	1.2852E8	Pr >= |S|	<.0001

Quantiles (Definition 5)
Level	Quantile
100% Max	0.99999990
99%	0.85274041
95%	0.43325182
90%	0.25807880
75% Q3	0.13295820
50% Median	0.06819134
25% Q1	0.03261007
10%	0.01569351
5%	0.01177164
1%	0.00670658
0% Min	0.00298479

Extreme Observations
Lowest	Highest
Value	Obs	Value	Obs
0.00298479	1	0.999961	22669
0.00299457	2	0.999977	22670
0.00324199	3	0.999979	22671
0.00330294	4	0.999984	22672
0.00344419	5	1.000000	22673

 
Figure 16-14. ScoreDistribution


?
Exercises
	Fit a GLM model containing predictors age, blood.pressure, sex and cholesterol, with age fitted with a smooth 5-knot restricted cubic spline function and a different shape of the age relationship for males and females. As an intermediate step, predict mean cholesterol from age using a proportional odds ordinal logistic model. Data is defined by:
	n <- 1000 # define sample size
	set.seed(17) # so can reproduce the results
	age <- rnorm(n, 50, 10)
	blood.pressure <- rnorm(n, 120, 15)
	cholesterol <- rnorm(n, 200, 25)
	sex <- factor(sample(c('female','male'), n,TRUE))
	label(age) <- 'Age' # label is in Hmisc
	label(cholesterol) <- 'Total Cholesterol'
	label(blood.pressure) <- 'Systolic Blood Pressure'
	label(sex) <- 'Sex'
	units(cholesterol) <- 'mg/dl' # uses units.default in Hmisc
	units(blood.pressure) <- 'mmHg'
	To use proper odds model, avoid using a huge number of intercepts by grouping cholesterol into 40-tiles
	These are the salary data used in Weisberg (1985), consisting of observations on six variables for 52 tenure-track professors in a small college. The data can be downloaded at http://data.princeton.edu/wws509/datasets/salary.dat. The variables are:
	sx = Sex, coded 1 for female and 0 for male
	rk = Rank, coded
	1 for assistant professor,
	2 for associate professor, and
	3 for full professor
	yr = Number of years in current rank
	dg = Highest degree, coded 1 if doctorate, 0 if masters
	yd = Number of years since highest degree was earned
	sl = Academic year salary, in dollars.
	Reference: S. Weisberg (1985). Applied Linear Regression, Second Edition. New York: John Wiley and Sons. Page 194.
	Fit a GLM to predict salary
	Fit a GLM to rank
	For both models, does sex strongly influence salary and rank?

	This dataset has information on lung cancer deaths by age and smoking status. The data can be downloaded at http://data.princeton.edu/wws509/datasets/smoking.dat. The variables are:
	age: in five-year age groups coded 1 to 9 for 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-79, 80+.
	smoking status: coded 1 = doesn't smoke, 2 = smokes cigars or pipe only, 3 = smokes cigarrettes and cigar or pipe, and 4 = smokes cigarrettes only,
	population: in hundreds of thousands, and
	deaths: number of lung cancer deaths in a year.
	Fit a GLM to predict lung cancer deaths.
	Determine if smoking results in higher number of deaths.
	Is age a stronger predictor than smoking??
Logistic regression

In statistics, logistic regression, or logit regression, is a type of probabilistic statistical classification model (Bishop, 2006). It is also used to predict a binary response from a binary predictor, used for predicting the outcome of a categorical dependent variable (i.e., a class label) based on one or more predictor variables (features). That is, it is used in estimating the parameters of a qualitative response model. The probabilities describing the possible outcomes of a single trial are modeled, as a function of the explanatory (predictor) variables, using a logistic function. Frequently (and subsequently in this article) “logistic regression” is used to refer specifically to the problem in which the dependent variable is binary—that is, the number of available categories is two—while problems with more than two categories are referred to as multinomial logistic regression or, if the multiple categories are ordered, as ordered logistic regression.
Logistic regression measures the relationship between a categorical dependent variable and one or more independent variables, which are usually (but not necessarily) continuous, by using probability scores as the predicted values of the dependent variable (Bhandari & Joensson, 2008). As such it treats the same set of problems as does probit regression using similar techniques.
Fields and examples of applications
Logistic regression was put forth in the 1940s as an alternative to Fisher’s 1936 classification method, linear discriminant analysis (James, Witten, Hastie, & Tibshirani, 2013). It is used extensively in numerous disciplines, including the medical and social science fields. For example, the Trauma and Injury Severity Score (TRISS), which is widely used to predict mortality in injured patients, was originally developed by Boyd et al. using logistic regression (Boyd, Tolson, & Copes, 1987). Logistic regression might be used to predict whether a patient has a given disease (e.g. diabetes), based on observed characteristics of the patient (age, gender, body mass index, results of various blood tests, etc.). Another example, a propensity model, might be to predict whether an American voter will vote Democratic or Republican, based on age, income, gender, race, state of residence, votes in previous elections, etc. (Harrell, 2010). The technique can also be used in engineering, especially for predicting the probability of failure of a given process, system or product (Strano & Colosimo, 2006) (Palei & Das, 2009). It is also used in marketing applications such as prediction of a customer’s propensity to purchase a product or cease a subscription, etc. In economics it can be used to predict the likelihood of a person’s choosing to be in the labor force, and a business application would be to predict the likelihood of a homeowner defaulting on a mortgage. Conditional random fields, an extension of logistic regression to sequential data, are used in natural language processing.
Basics
Logistic regression can be binomial or multinomial. Binomial or binary logistic regression deals with situations in which the observed outcome for a dependent variable can have only two possible types (for example, “dead” vs. “alive”). Multinomial logistic regression deals with situations where the outcome can have three or more possible types (e.g., “disease A” vs. “disease B” vs. “disease C”). In binary logistic regression, the outcome is usually coded as “0”or “1”, as this leads to the most straightforward interpretation (Hosmer & Lemeshow, 2000). If a particular observed outcome for the dependent variable is the noteworthy possible outcome (referred to as a “success” or a “case”) it is usually coded as “1” and the contrary outcome (referred to as a “failure” or a “noncase”) as “0”. Logistic regression is used to predict the odds of being a case based on the values of the independent variables (predictors). The odds are defined as the probability that a particular outcome is a case divided by the probability that it is a noncase.
Like other forms of regression analysis, logistic regression makes use of one or more predictor variables that may be either continuous or categorical data. Unlike ordinary linear regression, however, logistic regression is used for predicting binary outcomes of the dependent variable (treating the dependent variable as the outcome of a Bernoulli trial) rather than continuous outcomes. Given this difference, it is necessary that logistic regression take the natural logarithm of the odds of the dependent variable being a case (referred  to as the logit or log-odds) to create a continuous criterion as a transformed version of the dependent variable. Thus the logit transformation is referred to as the link function in logistic regression—although the dependent variable in logistic regression is binomial, the logit is the continuous criterion upon which linear regression is conducted (Hosmer & Lemeshow, 2000).
The logit of success is then fit to the predictors using linear regression analysis. The predicted value of the logit is converted back into predicted odds via the inverse of the natural logarithm, namely the exponential function. Therefore, although the observed dependent variable in logistic regression is a zero-or-one variable, the logistic regression estimates the odds, as a continuous variable, that the dependent variable is a success (a case). In some applications the odds are all that is needed. In others, a specific yes-or-no prediction is needed for whether the dependent variable is or is not a case; this categorical prediction can be based on the computed odds of a success, with predicted odds above some chosen cut-off value being translated into a prediction of a success.
Logistic function, odds ratio, and logit
An explanation of logistic regression begins with an explanation of the logistic function, which always takes on values between zero and one (Hosmer & Lemeshow, 2000):
F(t)=e^t/(e^t+1)=1/(1+e^t ),
and viewing t as a linear function of an explanatory variable x (or of a linear combination of explanatory variables), the logistic function can be written as:
F(x)=1/(1+e^(-(?_0+?_1 x) ) ).
This will be interpreted as the probability of the dependent variable equaling a “success” or “case” rather than a failure or non-case. We also define the inverse of the logistic function, the logit:
g(x)=ln??F(x)/(1-F(x) )=?_0+?_1 x?,
and equivalently:
F(x)/(1-F(x) )=e^((?_0+?_1 x) ).
A graph of the logistic function is shown in Figure 1. The input is the value of and the output is F(x). The logistic function is useful because it can take an input with any value from negative infinity to positive infinity, whereas the output is confined to values between 0 and 1 and hence is interpretable as a probability. In the above equations, refers to the logit function of some given linear combination of the predictors, denotes the natural logarithm, is the probability that the dependent variable equals a case, is the intercept from the linear regression equation (the value of the criterion when the predictor is equal to zero), is the regression coefficient multiplied by some value of the predictor, and base e denotes the exponential function.
 
Figure 1. The logistic function, with ?_0+?_1 on the horizontal axis and F(x) on the vertical axis
The formula for F(x) illustrates that the probability of the dependent variable equaling a case is equal to the value of the logistic function of the linear regression expression. This is important in that it shows that the value of the linear regression expression can vary from negative to positive infinity and yet, after transformation, the resulting expression for the probability F(x) ranges between 0 and 1. The equation for g(x) illustrates that the logit (i.e., log-odds or natural logarithm of the odds) is equivalent to the linear regression expression. Likewise, the next equation illustrates that the odds of the dependent variable equaling a case is equivalent to the exponential function of the linear regression expression. This illustrates how the logit serves as a link function between the probability and the linear regression expression. Given that the logit ranges between negative infinity and positive infinity, it provides an adequate criterion upon which to conduct linear regression and the logit is easily converted back into the odds (Hosmer & Lemeshow, 2000).
Multiple explanatory variables
If there are multiple explanatory variables, then the above expression ?_0+?_1 x can be revised to ?_0+?_1 x_1+?_2 x_1+?+?_m x_m. Then when this is used in the equation relating the logged odds of a success to the values of the predictors, the linear regression will be a multiple regression with m explanators; the parameters ?_j for all j=0,1,2,...,m are all estimated.
Model fitting
Estimation
Maximum likelihood estimation
The regression coefficients are usually estimated using maximum likelihood estimation (Menard, 2002). Unlike linear regression with normally distributed residuals, it is not possible to find a closed-form expression for the coefficient values that maximizes the likelihood function, so an iterative process must be used instead, for example Newton’s method. This process begins with a tentative solution, revises it slightly to see if it can be improved, and repeats this revision until improvement is minute, at which point the process is said to have converged (Menard, 2002).
In some instances the model may not reach convergence. When a model does not converge this indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions. A failure to converge may occur for a number of reasons: having a large proportion of predictors to cases, multicollinearity, sparseness, or complete separation.
Having a large proportion of variables to cases results in an overly conservative Wald statistic (discussed below) and can lead to nonconvergence.
Multicollinearity refers to unacceptably high correlations between predictors. As multicollinearity increases, coefficients remain unbiased but standard errors increase and the likelihood of model convergence decreases. To detect multicollinearity amongst the predictors, one can conduct a linear regression analysis with the predictors of interest for the sole purpose of examining the tolerance statistic used to assess whether multicollinearity is unacceptably high (Menard, 2002).
Sparseness in the data refers to having a large proportion of empty cells (cells with zero counts). Zero cell counts are particularly problematic with categorical predictors. With continuous predictors, the model can infer values for the zero cell counts, but this is not the case with categorical predictors. The reason the model will not converge with zero cell counts for categorical predictors is because the natural logarithm of zero is an undefined value, so final solutions to the model cannot be reached. To remedy this problem, researchers may collapse categories in a theoretically meaningful way or may consider adding a constant to all cells (Menard, 2002).
Another numerical problem that may lead to a lack of convergence is complete separation, which refers to the instance in which the predictors perfectly predict the criterion – all cases are accurately classified. In such instances, one should reexamine the data, as there is likely some kind of error (Hosmer & Lemeshow, 2000).
Although not a precise number, as a general rule of thumb, logistic regression models require a minimum of 10 events per explaining variable (where event denotes the cases belonging to the less frequent category in the dependent variable) (Peduzzi, Concato, Kemper, Holford, & Feinstein, 1996).
Minimum chi-squared estimator for grouped data
While individual data will have a dependent variable with a value of zero or one for every observation, with grouped data one observation is on a group of people who all share the same characteristics (e.g., demographic characteristics); in this case the researcher observes the proportion of people in the group for whom the response variable falls into one category or the other. If this proportion is neither zero nor one for any group, the minimum chi-squared estimator involves using weighted least squares to estimate a linear model in which the dependent variable is the logit of the proportion: that is, the log of the ratio of the fraction in one group to the fraction in the other group (Greene, 2011).
Evaluating goodness of fit
Goodness of fit in linear regression models is generally measured using the R^2. Since this has no direct analog in logistic regression, various methods (Greene, 2011) including the following can be used instead.
Deviance and likelihood ratio tests
In linear regression analysis, one is concerned with partitioning variance via the sum of squares calculations – variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance. In logistic regression analysis, deviance is used in lieu of sum of squares calculations (Cohen, Cohen, West, & Aiken, 2002). Deviance is analogous to the sum of squares calculations in linear regression (Hosmer & Lemeshow, 2000) and is a measure of the lack of fit to the data in a logistic regression model (Cohen, Cohen, West, & Aiken, 2002). Deviance is calculated by comparing a given model with the saturated model—a model with a theoretically perfect fit. This computation is called the likelihood-ratio test (Hosmer & Lemeshow, 2000):
D=-2"ln" ?(y_i )=-2 ln??"likelihood of the fitted model" /"likelihood of the saturated model" ?
In the above equation D represents the deviance and "ln"  represents the natural logarithm, and ?=?(y_i ). The log of the likelihood ratio (the ratio of the fitted model to the saturated model) will produce a negative value, so the product is multiplied by negative two times its natural logarithm to produce a value with an approximate ?^2-squared distribution (Hosmer & Lemeshow, 2000). Smaller values indicate better fit as the fitted model deviates less from the saturated model. When assessed upon a chi-square distribution, nonsignificant chi-square values indicate very little unexplained variance and thus, good model fit. Conversely, a significant chi-square value indicates that a significant amount of the variance is unexplained.
Two measures of deviance are particularly important in logistic regression: null deviance and model deviance. The null deviance represents the difference between a model with only the intercept (which means “no predictors“) and the saturated model (Cohen, Cohen, West, & Aiken, 2002). And, the model deviance represents the difference between a model with at least one predictor and the saturated model. In this respect, the null model provides a baseline upon which to compare predictor models. Given that deviance is a measure of the difference between a given model and the saturated model, smaller values indicate better fit. Therefore, to assess the contribution of a predictor or set of predictors, one can subtract the model deviance from the null deviance and assess the difference on a ?_(s-p)^2chi-square distribution with degree of freedom (Hosmer & Lemeshow, 2000) equal to the difference in the number of parameters estimated.
Let
D_null=-2"ln"  "likelihood of the null model" /"likelihood of the saturated model" 
D_fitted=-2 ln??"likelihood of the fitted model" /"likelihood of the saturated model" ?
Then
D_fitted ?-D?_null
=(-2 ln??"likelihood of the fitted model" /"likelihood of the saturated model" ? )-(2"ln"  "likelihood of the null model" /"likelihood of the saturated model" )
=-2(ln??"likelihood of the fitted model" /"likelihood of the saturated model" ?-ln??"likelihood of the null model" /"likelihood of the saturated model" ? )
=-2 ln??("likelihood of the fitted model" /"likelihood of the saturated model" )/("likelihood of the null model" /"likelihood of the saturated model" )?
=-2 ln??"likelihood of the fitted model" /"likelihood of the null model" ?.
If the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improved model fit. This is analogous to the F-test used in linear regression analysis to assess the significance of prediction (Cohen, Cohen, West, & Aiken, 2002). A convenient result, attributed to Samuel S. Wilks, says that as the sample size n approaches?, the test statistic -2"ln"(?) for a nested model will be asymptotically ?^2-distributed with degrees of freedom equal to the difference in dimensionality of the saturated model and null model (Wilks, 1938). This means that for a great variety of hypotheses, a practitioner can compute the likelihood ratio ? for the data and compare -2"ln"(?) to the ?^2 value corresponding to a desired statistical significance as an approximate statistical test. This is often referred to a Wilks’ Theorem.
Pseudo-R^2s
In linear regression the squared multiple correlation, R2 is used to assess goodness-of-fit as it represents the proportion of variance in the criterion that is explained by the predictors (Cohen, Cohen, West, & Aiken, 2002). In logistic regression analysis, there is no agreed upon analogous measure, but there are several competing measures each with limitations. Three of the most commonly used indices are examined on this section beginning with the likelihood ratio R^2, R_L^2 (Cohen, Cohen, West, & Aiken, 2002):
R_L^2  (D_null-D_model)/D_null .
This is the most analogous index to the squared multiple correlation in linear regression (Menard, 2002). It represents the proportional reduction in the deviance, wherein the deviance is treated as a measure of variation analogous but not identical to the variance in linear regression analysis (Menard, 2002). One limitation of the likelihood ratio R2 is that it is not monotonically related to the odds ratio (Cohen, Cohen, West, & Aiken, 2002), meaning that it does not necessarily increase as the odds ratio increases, and does not necessarily decrease as the odds ratio decreases.
The Cox and Snell R^2 is an alternative index of goodness-of-fit related to the R^2 value from linear regression. The Cox and Snell index is problematic as its maximum value is 0.75, when the variance is at its maximum (0.25). The Nagelkerke R^2 provides a correction to the Cox and Snell R2 so that the maximum value is equal to one. Nevertheless, the Cox and Snell and likelihood ratio R2s show greater agreement with each other than either does with the Nagelkerke R2 (Cohen, Cohen, West, & Aiken, 2002). Of course, this might not be the case for values exceeding 0.75 as the Cox and Snell index is capped at this value. The likelihood ratio R2 is often preferred to the alternatives as it is most analogous to R2 in linear regression, is independent of the base rate (both Cox and Snell and Nagelkerke R^2s increase as the proportion of cases increase from 0 to 0.5) and varies between 0 and 1.
A word of caution is in order when interpreting pseudo-R2 statistics. The reason these indices of fit are referred to as pseudo R^2 is because they do not represent the proportionate reduction in error as the R^2 in linear regression does (Cohen, Cohen, West, & Aiken, 2002). Linear regression assumes homoscedasticity, that the error variance is the same for all values of the criterion. Logistic regression will always be heteroscedastic – the error variances differ for each value of the predicted score. For each value of the predicted score there would be a different value of the proportionate reduction in error. Therefore, it is inappropriate to think of R^2 as a proportionate reduction in error in a universal sense in logistic regression (Cohen, Cohen, West, & Aiken, 2002).
Hosmer–Lemeshow test
The Hosmer–Lemeshow test uses a test statistic that asymptotically follows a ?^2 distribution to assess whether or not the observed event rates match expected event rates in subgroups of the model population (Hosmer & Lemeshow, 2000).
Evaluating binary classification performance
If the estimated probabilities are to be used to classify each observation of independent variable values as predicting the category that the dependent variable is found in, the various methods below for judging the model’s suitability in out-of-sample forecasting can also be used on the data that were used for estimation—accuracy, precision (also called positive predictive value), recall (also called sensitivity), specificity and negative predictive value. In each of these evaluative methods, an aspect of the model’s effectiveness in assigning instances to the correct categories is measured.
Coefficients
After fitting the model, it is likely that researchers will want to examine the contribution of individual predictors. To do so, they will want to examine the regression coefficients. In linear regression, the regression coefficients represent the change in the criterion for each unit change in the predictor (Cohen, Cohen, West, & Aiken, 2002). In logistic regression, however, the regression coefficients represent the change in the logit for each unit change in the predictor. Given that the logit is not intuitive, researchers are likely to focus on a predictor’s effect on the exponential function of the regression coefficient – the odds ratio (see definition). In linear regression, the significance of a regression coefficient is assessed by computing a t-test. In logistic regression, there are several different tests designed to assess the significance of an individual predictor, most notably the likelihood ratio test and the Wald statistic.
Likelihood ratio test
The likelihood-ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual “predictors“ to a given model (Cohen, Cohen, West, & Aiken, 2002) (Hosmer & Lemeshow, 2000) (Menard, 2002). In the case of a single predictor model, one simply compares the deviance of the predictor model with that of the null model on a chi-square distribution with a single degree of freedom. If the predictor model has a significantly smaller deviance (c.f chi-square using the difference in degrees of freedom of the two models), then one can conclude that there is a significant association between the “predictor” and the outcome. Although some common statistical packages (e.g. SPSS) do provide likelihood ratio test statistics, without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case. To assess the contribution of individual predictors one can enter the predictors hierarchically, comparing each new model with the previous to determine the contribution of each predictor (Cohen, Cohen, West, & Aiken, 2002). (There is considerable debate among statisticians regarding the appropriateness of so-called “stepwise“ procedures. They do not preserve the nominal statistical properties and can be very misleading (Harrell, 2010).
Wald statistic
Alternatively, when assessing the contribution of individual predictors in a given model, one may examine the significance of the Wald statistic. The Wald statistic, analogous to the t-test in linear regression, is used to assess the significance of coefficients. The Wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as a chi-square distribution (Menard, 2002).
W_j=(B_j^2)/(SE_(B_j)^2 ).
Although several statistical packages (e.g., SPSS, SAS) report the Wald statistic to assess the contribution of individual predictors, the Wald statistic has limitations. When the regression coefficient is large, the standard error of the regression coefficient also tends to be large increasing the probability of Type-II error. The Wald statistic also tends to be biased when data are sparse (Cohen, Cohen, West, & Aiken, 2002).
Case-control sampling
Suppose cases are rare. Then we might wish to sample them more frequently than their prevalence in the population. For example, suppose there is a disease that affects 1 person in 10,000 and to collect our data we need to do a complete physical. It may be too expensive to do thousands of physicals of healthy people in order to get data on only a few diseased individuals. Thus, we may evaluate more diseased individuals. This is also called unbalanced data. As a rule of thumb, sampling controls at a rate of five times the number of cases is sufficient to get enough control data (Prentice & Pyke, 1979).
If we form a logistic model from such data, if the model is correct, the ?_j parameters are all correct except for ?_0. We can correct ?_0 if we know the true prevalence as follows (Prentice & Pyke, 1979):
? ?_0^*=? ?_0+log???/(1-?)?  log??? ?/(1-? ? )?,
where ? is the true prevalence and ? ? is the prevalence in the sample.
Formal mathematical specification
There are various equivalent specifications of logistic regression, which fit into different types of more general models. These different specifications allow for different sorts of useful generalizations.
Setup
The basic setup of logistic regression is the same as for standard linear regression.
It is assumed that we have a series of N observed data points. Each data point i consists of a set of m explanatory variables x_(1,i),…,x_(m,i) (also called independent variables, predictor variables, input variables, features, or attributes), and an associated binary-valued outcome variable Y_i (also known as a dependent variable, response variable, output variable, outcome variable or class variable), i.e. it can assume only the two possible values 0 (often meaning “no” or “failure”) or 1 (often meaning “yes” or “success”). The goal of logistic regression is to explain the relationship between the explanatory variables and the outcome, so that an outcome can be predicted for a new set of explanatory variables.
Some examples:
The observed outcomes are the presence or absence of a given disease (e.g. diabetes) in a set of patients, and the explanatory variables might be characteristics of the patients thought to be pertinent (sex, race, age, blood pressure, body-mass index, etc.).
The observed outcomes are the votes (e.g. Democratic or Republican) of a set of people in an election, and the explanatory variables are the demographic characteristics of each person (e.g. sex, race, age, income, etc.). In such a case, one of the two outcomes is arbitrarily coded as 1, and the other as 0.
As in linear regression, the outcome variables Y are assumed to depend on the explanatory variables x_(1,i),…,x_(m,i).
Explanatory variables
As shown above in the above examples, the explanatory variables may be of any type: real-valued, binary, categorical, etc. The main distinction is between continuous variables (such as income, age and blood pressure) and discrete variables (such as sex or race). Discrete variables referring to more than two possible choices are typically coded using dummy variables (or indicator variables), that is, separate explanatory variables taking the value 0 or 1 are created for each possible value of the discrete variable, with a 1 meaning “variable does have the given value” and a 0 meaning “variable does not have that value”. For example, a four-way discrete variable of blood type with the possible values “A, B, AB, O” can be converted to four separate two-way dummy variables, “is-A, is-B, is-AB, is-O”, where only one of them has the value 1 and all the rest have the value 0. This allows for separate regression coefficients to be matched for each possible value of the discrete variable. (In a case like this, only three of the four dummy variables are independent of each other, in the sense that once the values of three of the variables are known, the fourth is automatically determined. Thus, it is only necessary to encode three of the four possibilities as dummy variables. This also means that when all four possibilities are encoded, the overall model is not identifiable in the absence of additional constraints such as a regularization constraint. Theoretically, this could cause problems, but in reality almost all logistic regression models are fit with regularization constraints.)
Outcome variables
Formally, the outcomes Y_i are described as being Bernoulli-distributed data, where each outcome is determined by an unobserved probability p_i that is specific to the outcome at hand, but related to the explanatory variables. This can be expressed in any of the following equivalent forms:
Yix1,i,…,xm,i~Bernoullipi
E[Y_i ?|x_(1,i),…,x_(m,i) ?]=p_i
"Pr" (Y_i=y_i ?|x_(1,i),…,x_(m,i) ?)={?(p_i&"if"  y_i=1@1-p_i&"if"  y_i=0)?
"Pr" (Y_i=y_i ?|x_(1,i),…,x_(m,i) ?)=p_i^(y_i ) (1-p_i )^((1-y_i ) )
The meanings of these four lines are:
1. The first line expresses the probability distribution of each Y_i: Conditioned on the explanatory variables, it follows a Bernoulli distribution with parameters p_i, the probability of the outcome of 1 for trial i. As noted above, each separate trial has its own probability of success, just as each trial has its own explanatory variables. The probability of success p_i is not observed, only the outcome of an individual Bernoulli trial using that probability.
2. The second line expresses the fact that the expected value of each Y_i is equal to the probability of success p_i, which is a general property of the Bernoulli distribution. In other words, if we run a large number of Bernoulli trials using the same probability of success p_i, then take the average of all the 1 and 0 outcomes, then the result would be close to p_i. This is because doing an average this way simply computes the proportion of successes seen, which we expect to converge to the underlying probability of success.
3. The third line writes out the probability mass function of the Bernoulli distribution, specifying the probability of seeing each of the two possible outcomes.
4. The fourth line is another way of writing the probability mass function, which avoids having to write separate cases and is more convenient for certain types of calculations. This relies on the fact that Y_i can take only the value 0 or 1. In each case, one of the exponents will be 1, “choosing” the value under it, while the other is 0, “canceling out” the value under it. Hence, the outcome is either p_i or 1-p_i , as in the previous line.
Linear predictor function
The basic idea of logistic regression is to use the mechanism already developed for linear regression by modeling the probability p_i using a linear predictor function, i.e. a linear combination of the explanatory variables and a set of regression coefficients that are specific to the model at hand but the same for all trials. The linear predictor function f(i) for a particular data point i is written as:
f(i)=?_0+?_1 x_(1,i)+?_2 x_(1,i)+?+?_m x_(m,i),
where ?_0,…,?_mare regression coefficients indicating the relative effect of a particular explanatory variable on the outcome.
The model is usually put into a more compact form as follows:
The regression coefficients?_0,?_1,…,?_m ? , ? , ..., ? are grouped into a single vector ? of size m+1.
For each data point i, an additional explanatory pseudo-variable x_(0,i) is added, with a fixed value of 1, corresponding to the intercept coefficient ?_0 .
The resulting explanatory variables x_(0,i),x_(1,i),…,x_(m,i) are then grouped into a single vector X_i of size m+1.
This makes it possible to write the linear predictor function as follows:
f(i)=??X_i,
using the notation for a dot product between two vectors.
As a generalized linear model
The particular model used by logistic regression, which distinguishes it from standard linear regression and from other types of regression analysis used for binary-valued outcomes, is the way the probability of a particular outcome is linked to the linear predictor function:
"logit" (E[Y_i ?|x_(1,i),…,x_(m,i) ?])="logit" (p_i )=ln??(p_i/(1-p_i ))=?_1 x_(1,i)+?_2 x_(1,i)+?+?_m x_(m,i) ?
Written using the more compact notation described above, this is:
"logit" (E[Y_i ?|X_i ?])="logit" (p_i )=ln??(p_i/(1-p_i ))=??X_i ?.
This formulation expresses logistic regression as a type of generalized linear model, which predicts variables with various types of probability distributions by fitting a linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable.
The intuition for transforming using the logit function (the natural log of the odds) was explained above. It also has the practical effect of converting the probability (which is bounded to be between 0 and 1) to a variable that ranges over (-?,+?) — thereby matching the potential range of the linear prediction function on the right side of the equation.
Note that both the probabilities p_i and the regression coefficients are unobserved, and the means of determining them is not part of the model itself. They are typically determined by some sort of optimization procedure, e.g. maximum likelihood estimation, which finds values that best fit the observed data (i.e. that give the most accurate predictions for the data already observed), usually subject to regularization conditions that seek to exclude unlikely values, e.g. extremely large values for any of the regression coefficients. The use of a regularization condition is equivalent to doing maximum a posteriori (MAP) estimation, an extension of maximum likelihood. (Regularization is most commonly done using a squared regularizing function, which is equivalent to placing a zero-mean Gaussian prior distribution on the coefficients, but other regularizers are also possible.) Whether or not regularization is used, it is usually not possible to find a closed-form solution; instead, an iterative numerical method must be used, such as iteratively reweighted least squares (IRLS) or, more commonly these days, a quasi-Newton method such as the L-BFGS method.
The interpretation of the ?_j parameter estimates is as the additive effect on the log of the odds for a unit change in the jth explanatory variable. In the case of a dichotomous explanatory variable, for instance gender, e^? is the estimate of the odds of having the outcome for, say, males compared with females.
An equivalent formula uses the inverse of the logit function, which is the logistic function, i.e.:
E[Y_i ?|X_i ?]=p_i=(??X_i )=1/(1+e^(-??X_i ) ).
The formula can also be written (somewhat awkwardly) as a probability distribution (specifically, using a probability mass function):
"Pr" (Y_i=y_i ?|X_i ?)=p_i^(y_i ) (1-p_i )^((1-y_i ) )
=(1/(1+e^(-??X_i ) ))^(y_i ) (1-1/(1+e^(-??X_i ) ))^(?1-y?_i ).
As a latent-variable model
The above model has an equivalent formulation as a latent-variable model. This formulation is common in the theory of discrete choice models, and makes it easier to extend to certain more complicated models with multiple, correlated choices, as well as to compare logistic regression to the closely related probit model.
Imagine that, for each trial i, there is a continuous latent variable Y_i^* (i.e. an unobserved random variable) that is distributed as follows:
Y_i^*= ??X_i+?,

Where
?~"Logistic" (0,1),
i.e., the latent variable can be written directly in terms of the linear predictor function and an additive random error variable that is distributed according to a standard logistic distribution.
Then Y_i can be viewed as an indicator for whether this latent variable is positive:
Y_i={?(1&"if"  Y_i^*>0,i.e.-?<??X_i@0&"otherwise" )?.
The choice of modeling the error variable specifically with a standard logistic distribution, rather than a general logistic distribution with the location and scale set to arbitrary values, seems restrictive, but in fact it is not. It must be kept in mind that we can choose the regression coefficients ourselves, and very often can use them to offset changes in the parameters of the error variable’s distribution. For example, a logistic error-variable distribution with a non-zero location parameter ? (which sets the mean) is equivalent to a distribution with a zero location parameter, where ? has been added to the intercept coefficient. Both situations produce the same value for  Y_i^* regardless of settings of explanatory variables. Similarly, an arbitrary scale parameter s is equivalent to setting the scale parameter to 1 and then dividing all regression coefficients by s. In the latter case, the resulting value of  Y_i^* will be smaller by a factor of s than in the former case, for all sets of explanatory variables — but critically, it will always remain on the same side of 0, and hence lead to the same Y_i choice.
 (Note that this predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available.)
It turns out that this formulation is exactly equivalent to the preceding one, phrased in terms of the generalized linear model and without any latent variables. This can be shown as follows, using the fact that the cumulative distribution function (CDF) of the standard logistic distribution is the logistic function, which is the inverse of the logit function, i.e.
Pr(?<x)=(x).
Then:
"Pr" (Y_i=1?|X_i ?)="Pr" ((Y_i^*>0)?X_i )
="Pr"(??X_i+?>0)
="Pr"(?>-??X_i)
="Pr"(?<??X_i)
=(??X_i )
=p_i.
This formulation — which is standard in discrete choice models — makes clear the relationship between logistic regression (the “logit model“) and the probit model, which uses an error variable distributed according to a standard normal distribution instead of a standard logistic distribution. Both the logistic and normal distributions are symmetric with a basic unimodal, “bell curve” shape. The only difference is that the logistic distribution has somewhat heavier tails, which means that it is less sensitive to outlying data (and hence somewhat more robust to model misspecifications or erroneous data).
As a two-way latent-variable model
Yet another formulation uses two separate latent variables:
Y_i^(0*)= ?_0?X_i+?_0,
Y_i^(1*)= ?_01?X_i+?_1,
where
?_0~EV_1 (0,1),
?_1~EV_1 (0,1),
where EV_1 (0,1) is a standard type-1 extreme value distribution: i.e.
Pr(?_0=x)=Pr(?_1=x)=e^(-x) e^(?-e?^(-x) ),
Then
Y_i={?(1&"if"  Y_i^(1*)>Y_i^(0*)@0&"otherwise" )?.
This model has a separate latent variable and a separate set of regression coefficients for each possible outcome of the dependent variable. The reason for this separation is that it makes it easy to extend logistic regression to multi-outcome categorical variables, as in the multinomial logit model. In such a model, it is natural to model each possible outcome using a different set of regression coefficients. It is also possible to motivate each of the separate latent variables as the theoretical utility associated with making the associated choice, and thus motivate logistic regression in terms of utility theory. (In terms of utility theory, a rational actor always chooses the choice with the greatest associated utility.) This is the approach taken by economists when formulating discrete choice models, because it both provides a theoretically strong foundation and facilitates intuitions about the model, which in turn makes it easy to consider various sorts of extensions. (See the example below.)
The choice of the type-1 extreme value distribution seems fairly arbitrary, but it makes the mathematics work out, and it may be possible to justify its use through rational choice theory.
It turns out that this model is equivalent to the previous model, although this seems non-obvious, since there are now two sets of regression coefficients and error variables, and the error variables have a different distribution. In fact, this model reduces directly to the previous one with the following substitutions:
?=?_1-?_0,
?=?_1-?_0.
An intuition for this comes from the fact that, since we choose based on the maximum of two values, only their difference matters, not the exact values — and this effectively removes one degree of freedom. Another critical fact is that the difference of two type-1 extreme-value-distributed variables is a logistic distribution, i.e. if
?=?_1-?_0~"Logistic" (0,1).
We can demonstrate the equivalent as follows:
"Pr" (Y_i=1?|X_i ?)="Pr" (Y_i^(1*)>Y_i^(0*)?X_i )
"= Pr" (Y_i^(1*)-Y_i^(0*)>0?X_i )
="Pr"(?_1?X_i+?_1-(?_0?X_i+?_0 )>0)
="Pr"((?_1?X_i-?_0?X_i )+(?_1-?_0 )>0)
="Pr"((?_1-?_0 )?X_i+(?_1-?_0 )>0)
="Pr"((?_1-?_0 )?X_i+?>0)
="Pr"(??X_i+?>0)
="Pr"(?>-??X_i)
="Pr"(?<??X_i)
=(??X_i )
=p_i.
Example
As an example, consider a province-level election where the choice is between a right-of-center party, a left-of-center party, and a secessionist party (e.g. the Parti Québécois, which wants Quebec to secede from Canada (Hale & Hale, 2006)). We would then use three latent variables, one for each choice. Then, in accordance with utility theory, we can then interpret the latent variables as expressing the utility that results from making each of the choices. We can also interpret the regression coefficients as indicating the strength that the associated factor (i.e. explanatory variable) has in contributing to the utility — or more correctly, the amount by which a unit change in an explanatory variable changes the utility of a given choice. A voter might expect that the right-of-center party would lower taxes, especially on rich people. This would give low-income people no benefit, i.e. no change in utility (since they usually don’t pay taxes); would cause moderate benefit (i.e. somewhat more money, or moderate utility increase) for middle-incoming people; and would cause significant benefits for high-income people. On the other hand, the left-of-center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes. This would cause significant positive benefit to low-income people, perhaps weak benefit to middle-income people, and significant negative benefit to high-income people. Finally, the secessionist party would take no direct actions on the economy, but simply secede. A low-income or middle-income voter might expect basically no clear utility gain or loss from this, but a high-income voter might expect negative utility, since he/she is likely to own companies, which will have a harder time doing business in such an environment and probably lose money.
These intuitions can be expressed as follows:
Estimated strength of regression coefficient for different outcomes (party choices) and different values of explanatory variables
	Center-right	Center-left	Secessionist
High-income	strong +	strong ?	strong ?
Middle-income	moderate +	weak +	none
Low-income	none	strong +	none




This clearly shows that
1. Separate sets of regression coefficients need to exist for each choice. When phrased in terms of utility, this can be seen very easily. Different choices have different effects on net utility; furthermore, the effects vary in complex ways that depend on the characteristics of each individual, so there need to be separate sets of coefficients for each characteristic, not simply a single extra per-choice characteristic.
2. Even though income is a continuous variable, its effect on utility is too complex for it to be treated as a single variable. Either it needs to be directly split up into ranges, or higher powers of income need to be added so that polynomial regression on income is effectively done.
?
As a “log-linear” model
Yet another formulation combines the two-way latent variable formulation above with the original formulation higher up without latent variables, and in the process provides a link to one of the standard formulations of the multinomial logit (Greene, 2011).
Here, instead of writing the logit of the probabilities p_i as a linear predictor, we separate the linear predictor into two, one for each of the two outcomes:
"ln Pr" (Y_i=0)= ?_0?X_i-"ln" Z
"ln Pr" (Y_i=0)= ?_1?X_i-"ln" Z.
Note that two separate sets of regression coefficients have been introduced, just as in the two-way latent variable model, and the two equations appear a form that writes the logarithm of the associated probability as a linear predictor, with an extra term –"ln" Z at the end. This term, as it turns out, serves as the normalizing factor ensuring that the result is a distribution. This can be seen by exponentiating both sides:
"Pr" (Y_i=0)=1/Z e^(?_0?X_i )
"Pr" (Y_i=1)=1/Z e^(?_1?X_i ).
In this form it is clear that the purpose of Z is to ensure that the resulting distribution over Y_i is in fact a probability distribution, i.e. it sums to 1. This means that Z is simply the sum of all un-normalized probabilities, and by dividing each probability by Z, the probabilities become “normalized”. That is:
Z=e^(?_0?X_i )+e^(?_1?X_i ),
and the resulting equations are
"Pr" (Y_i=0)=e^(?_0?X_i )/(e^(?_0?X_i )+e^(?_1?X_i ) )
"Pr" (Y_i=1)=e^(?_1?X_i )/(e^(?_0?X_i )+e^(?_1?X_i ) ).
Or generally:
"Pr" (Y_i=c)=e^(?_c?X_i )/(?_h?e^(?_h?X_i ) ).
This shows clearly how to generalize this formulation to more than two outcomes, as in multinomial logit (Greene, 2011).
In order to prove that this is equivalent to the previous model, note that the above model is over specified, in that "Pr" (Y_i=0) and "Pr" (Y_i=1)  cannot be independently specified: "Pr" (Y_i=0)+"Pr" (Y_i=1)=1 rather so knowing one automatically determines the other. As a result, the model is nonidentifiable, in that multiple combinations of ?_0 and ?_1 will produce the same probabilities for all possible explanatory variables. In fact, it can be seen that adding any constant vector to both of them will produce the same probabilities:
"Pr" (Y_i=1)=e^((?_1+C)?X_i )/(e^((?_0+C)?X_i )+e^((?_1+C)?X_i ) )
=(e^(?_1?X_i ) e^(C?X_i ))/(e^(?_0?X_i ) e^(C?X_i )+e^(?_1?X_i ) e^(C?X_i ) )
=(e^(?_1?X_i ) e^(C?X_i ))/(e^(C?X_i ) (e^(?_0?X_i )+e^(?_1?X_i ) ) )
=e^(?_1?X_i )/((e^(?_0?X_i )+e^(?_1?X_i ) ) ).
As a result, we can simplify matters, and restore identifiability, by picking an arbitrary value for one of the two vectors. We choose to set ?_0=0. Then,
e^(?_0?X_i )=e^(0?X_i )=1,
and so
"Pr" (Y_i=1)=e^(?_1?X_i )/(1+e^(?_1?X_i ) )=1/(1+e^(-?_1?X_i ) )=p_i,
which shows that this formulation is indeed equivalent to the previous formulation. (As in the two-way latent variable formulation, any settings where ?=?_1-?_0 will produce equivalent results.)
Note that most treatments of the multinomial logit model start out either by extending the “log-linear” formulation presented here or the two-way latent variable formulation presented above, since both clearly show the way that the model could be extended to multi-way outcomes (Greene, 2011). In general, the presentation with latent variables is more common in econometrics and political science, where discrete choice models and utility theory reign, while the “log-linear” formulation here is more common in computer science, e.g. machine learning and natural language processing.
As a single-layer perceptron
The model has an equivalent formulation
p_i=1/(1+e^(-(?_10+?_1?X_(1,i)+?+k?X_(k,i) ) ) ).
This functional form is commonly called a single-layer perceptron or single-layer artificial neural network (Da & Xiurun, 2005). A single-layer neural network computes a continuous output instead of a step function. The derivative of p_i with respect to X=(x_1,...,x_k) is computed from the general form:
y=1/(1+e^(-f(x) ) ),
where f(X) is an analytic function in X. With this choice, the single-layer neural network is identical to the logistic regression model. This function has a continuous derivative, which allows it to be used in backpropagation. This function is also preferred because its derivative is easily calculated:
dy/dX=y(1-4)  df/dX.
In terms of binomial data
A closely related model assumes that each i is associated not with a single Bernoulli trial but with n_i independent identically distributed trials, where the observation Y_i is the number of successes observed (the sum of the individual Bernoulli-distributed random variables), and hence follows a binomial distribution:
Y_i~Bin(n_i,p_p )," for" i=1,…,n.
An example of this distribution is the fraction of seeds (p_i) that germinate after n_i are planted. In terms of expected values, this model is expressed as follows:
p_i=E[? Y_i/n_i ?| X_i ],
so that
"logit" (E[? Y_i/n_i ?| X_i ])="logit" (p_i )=ln?(p_i/(1-p_i ))=??X_i,
Or equivalently:
"Pr" (Y_i=y_i?X_i )=(n_i¦k_i ) p_i^(y_i ) (1-p_i )^(n_i-y_i )=(n_i¦k_i ) (1/(1+e^(-??X_i ) ))^(y_i ) (1-1/(1+e^(-??X_i ) ))^(?1-y?_i ).
This model can be fit using the same sorts of methods as the above more basic model.
Bayesian logistic regression
In a Bayesian statistics context, prior distributions are normally placed on the regression coefficients, usually in the form of Gaussian distributions. Unfortunately, the Gaussian distribution is not the conjugate prior of the likelihood function in logistic regression; in fact, the likelihood function is not an exponential family and thus does not have a conjugate prior at all. As a result, the posterior distribution is difficult to calculate, even using standard simulation algorithms (e.g. Gibbs sampling).
There are various possibilities:
Don’t do a proper Bayesian analysis, but simply compute a maximum a posteriori point estimate of the parameters. This is common, for example, in “maximum entropy” classifiers in machine learning.
Use a more general approximation method such as the Metropolis–Hastings algorithm.
Draw a Markov chain Monte Carlo sample from the exact posterior by using the Independent Metropolis–Hastings algorithm with heavy-tailed multivariate candidate distribution found by matching the mode and curvature at the mode of the normal approximation to the posterior and then using the Student’s t shape with low degrees of freedom. This is shown to have excellent convergence properties.
Use a latent variable model and approximate the logistic distribution using a more tractable distribution, e.g. a Student’s t-distribution or a mixture of normal distributions.
Do probit regression instead of logistic regression. This is actually a special case of the previous situation, using a normal distribution in place of a Student’s t, mixture of normals, etc. This will be less accurate but has the advantage that probit regression is extremely common, and a ready-made Bayesian implementation may already be available (see Figure 17-2).
Use the Laplace approximation of the posterior distribution. This approximates the posterior with a Gaussian distribution. This is not a terribly good approximation, but it suffices if all that is desired is an estimate of the posterior mean and variance. In such a case, an approximation scheme such as variational Bayes can be used (Bishop, 2006).
 
Figure 17-2. Comparison of logistic function with a scaled inverse probit function (i.e. the CDF of the normal distribution), comparing ?(x) vs. ?=(?(?/8) x), which makes the slopes the same at the origin. This shows the heavier tails of the logistic distribution.
Gibbs sampling with an approximating distribution
As shown above, logistic regression is equivalent to a latent variable model with an error variable distributed according to a standard logistic distribution. The overall distribution of the latent variable Y_i^* is also a logistic distribution, with the mean equal to ??X_i (i.e. the fixed quantity added to the error variable). This model considerably simplifies the application of techniques such as Gibbs sampling (George & McCullochb, 1993). However, sampling the regression coefficients is still difficult, because of the lack of conjugacy between the normal and logistic distributions. Changing the prior distribution over the regression coefficients is of no help, because the logistic distribution is not in the exponential family and thus has no conjugate prior.
One possibility is to use a more general Markov chain Monte Carlo technique (Walsh, 2004), such as the Metropolis–Hastings algorithm (Chib & Greenberg, 1995), which can sample arbitrary distributions. Another possibility, however, is to replace the logistic distribution with a similar-shaped distribution that is easier to work with using Gibbs sampling. In fact, the logistic and normal distributions have a similar shape, and thus one possibility is simply to have normally distributed errors. Because the normal distribution is conjugate to itself, sampling the regression coefficients becomes easy. In fact, this model is exactly the model used in probit regression.
However, the normal and logistic distributions differ in that the logistic has heavier tails. As a result, it is more robust to inaccuracies in the underlying model (which are inevitable, in that the model is essentially always an approximation) or to errors in the data. Probit regression loses some of this robustness.
Another alternative is to use errors distributed as a Student’s t-distribution. The Student’s t-distribution has heavy tails, and is easy to sample from because it is the compound distribution of a normal distribution with variance distributed as an inverse gamma distribution. In other words, if a normal distribution is used for the error variable, and another latent variable, following an inverse gamma distribution, is added corresponding to the variance of this error variable, the marginal distribution of the error variable will follow a Student’s t-distribution. Because of the various conjugacy relationships, all variables in this model are easy to sample from.
The Student’s t-distribution that best approximates a standard logistic distribution can be determined by matching the moments of the two distributions. The Student’s t-distribution has three parameters, and since the skewness of both distributions is always 0, the first four moments can all be matched, using the following equations:
?=0
?/(?-2) s^2=?^2/3
6/(?-4)=6/5.
This yields the following values:
?=0
s=?(7/9  ?^2/3)
?=9.
The following graphs compare the standard logistic distribution with the Student’s t-distribution that matches the first four moments using the above-determined values, as well as the normal distribution that matches the first two moments. Note how much closer the Student’s t-distribution agrees, especially in the tails. Beyond about two standard deviations from the mean, the logistic and normal distributions diverge rapidly, but the logistic and Student’s t-distributions don’t start diverging significantly until more than 5 standard deviations away (see Figure 17-3 through Fgure 17-6).
(Another possibility, also amenable to Gibbs sampling, is to approximate the logistic distribution using a mixture density of normal distributions (Chen, Zhu, Wang, Zheng, & Zhang, 2013).)
 
Figure 17-3. Comparison of logistic and approximating distributions (t, normal).
 
Figure 17-4. Tails of distributions.
 
Figure 17-5. Further tails of distributions.
 
Figure 17-6. Extreme tails of distributions.
Extensions
There are large numbers of extensions:
Multinomial logistic regression (or multinomial logit) handles the case of a multi-way categorical dependent variable (with unordered values, also called “classification”). Note that the general case of having dependent variables with more than two values is termed polytomous regression.
Ordered logistic regression (or ordered logit) handles ordinal dependent variables (ordered values).
Mixed logit is an extension of multinomial logit that allows for correlations among the choices of the dependent variable.
An extension of the logistic model to sets of interdependent variables is the conditional random field.
Model suitability
A way to measure a model’s suitability is to assess the model against a set of data that was not used to create the model (Mark & Goldberg, 2001). The class of techniques is called cross-validation. This holdout model assessment method is particularly valuable when data are collected in different settings (e.g., at different times or places) or when models are assumed to be generalizable.
To measure the suitability of a binary regression model, one can classify both the actual value and the predicted value of each observation as either 0 or 1 (Myers & Forgy, 1963). The predicted value of an observation can be set equal to 1 if the estimated probability that the observation equals 1 is above 1/2, and set equal to 0 if the estimated probability is below 1/2. Here logistic regression is being used as a binary classification model. There are four possible combined classifications:
1. prediction of 0 when the holdout sample has a 0 (True Negatives, the number of which is TN)
2. prediction of 0 when the holdout sample has a 1 (False Negatives, the number of which is FN)
3. prediction of 1 when the holdout sample has a 0 (False Positives, the number of which is FP)
4. prediction of 1 when the holdout sample has a 1 (True Positives, the number of which is TP)
These classifications are used to calculate accuracy, precision (also called positive predictive value), recall (also called sensitivity), specificity and negative predictive value:
"Accuracy"=(TP+TN)/(TP+FP+FN+TN)= "fraction of observations with correct predicted classification" 
Precision =PositivePredictiveValue=TP/(TP+FP)= "Fraction of predicted positives that are correct" 
NegativePredictiveVlaue=TN/(TN+FN)= "fraction of predicted negatives that are correct" 
Recall=Sensitivity=TP/(TP+FN)= "fraction of observations that are actually 1 with "
"a correct predicted classification" 
Specificity=TN/(TN+FP)= "fraction of observations that are actually 0 with a correct predicted" 
Software
All the primary software packages discussed in Chapter 2 have this functionality.
Logistic Regression in SAS
This example illustrates the following:
	The PROC LOGISTIC procedure in SAS
	Generating and using scoring model scoring code
	Performing detailed diagnostics
The dataset I chose for this example in Longitudinal Low Birth Weight Study (CLSLOWBWT.DAT). Hosmer and Lemeshow (2000) Applied Logistic Regression: Second Edition. These data are copyrighted by John Wiley & Sons Inc. and must be acknowledged and used accordingly.
Variable Description Codes/Values Name
	Identification Code ID Number ID
	Birth Number 1-4 BIRTH
	Smoking Status 0 = No, 1 = Yes SMOKE During Pregnancy
	Race 1 = White, 2 = Black RACE 3 = Other
	Age of Mother Years AGE
	Weight of Mother at Pounds LWT Last Menstrual Period
	Birth Weight Grams BWT
	Low Birth Weight 1 = BWT <=2500g, LOW 0 = BWT >2500g

PROBLEM STATEMENT: In this example, we want to predict Low Birth Weight using the remaining dataset variables. Low Birth Weight, the dependent variable, 1 = BWT <=2500g and 0 = BWT >2500g.
SAS File Import and Logistic Regression Code
ODS GRAPHICS ON
/*Import dataset CLSLOWBWT.CSV*/
%web_drop_table(WORK.IMPORT);

/*Here we name the file we will import by its url*/ 
FILENAME REFFILE "/home/jeff47/sasuser.v94/clslowbwt.csv" TERMSTR=CR;

PROC IMPORT DATAFILE=REFFILE
	DBMS=CSV
	OUT=WORK.LOW_BIRTH_WEIGHT;
	GETNAMES=YES;
RUN;

/*View dataset CLSLOWBWT*/
PROC CONTENTS DATA=WORK. LOW_BIRTH_WEIGHT; RUN;

%web_open_table(WORK. LOW_BIRTH_WEIGH);

PROC MEANS DATA = WORK. LOW_BIRTH_WEIGHT;
  VAR BIRTH  SMOKE  RACE  AGE  LWT;
RUN;
PROC FREQ data = WORK. LOW_BIRTH_WEIGHT;
  TABLES BIRTH LWT LWT*BIRTH;
RUN;


/*Perform Logistic Regression for target variable LOW*/
/*There ae 15 statement for PROC LOGISTIC but we only use three here: PLOTS, */
/*MODEL and OUTPUT*/
PROC LOGISTIC DATA=WORK. LOW_BIRTH_WEIGHT
PLOTS(ONLY)=ALL
;
MODEL RESP(EVENT='1')= BIRTH SMOKE RACE AGE LWT /
SELECTION = STEPWISE
SLE=0.05 /* specifies significance level for entering effects*/
SLS=0.05 /*specifies significance level for removing effects*/
INCLUDE=0 /*specifies number of effects included in every model*/
CORRB	 /*Correlation matrix*/
COVB	 /*Covariance matrix*/
LACKFIT /*specifies method to correct overdispersion*/
RSQUARE /*displays generalized R-squared*/
CTABLE	/*displays classification table*/
PPROB=(0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0) 
/*specifies probability cutpoints for classification*/
LINK=LOGIT /*specifies link function*/
CLPARM=WALD /*computes confidence intervals for parameters*/
CLODDS=WALD /*computes confidence intervals for odds ratios*/
ALPHA=0.05 /*specifies alpha for confidence intervals*/
	;
OUTPUT out=WORK.Logistic_stats /*Names the output data set*/
xbeta=xbeta_  /* Names the linear predictor*/
predicted=pred_ /*Names the predicted probabilities*/
lower=lcl_ /*Names the lower confidence limit*/
upper=ucl_ /*Names the upper confidence limit*/
reslik=reslik_ /*Names the likelihood residual*/
h=leverage_ /*Names the leverage*/
dfbetas=stdDfbeta_ /*Names the standardized deletion parameter differences*/
/ alpha=0.05;/* Specifies ? for the 100(1-?)% confidence intervals
SCORE out=WORK.Logistic_scores; /*Names the SAS data set containing predictions*/
code; /*soring code is recorded at end of log file*/

RUN;

ODS GRAPHICS OFF

For more information on PROC LOGISTIC see reference [2].

SAS Logistic Regression Scoring Code
%MACRO SCOREPROG(INDATA,OUTDATA);
DATA &OUTDATA;
SET &INDATA;
 *****************************************;
 ** SAS Scoring Code for PROC Logistic;
 *****************************************;
 
 length I_RESP $ 12;
 label I_RESP = 'Into: RESP' ;
 label U_RESP = 'Unnormalized Into: RESP' ;
 format U_RESP BEST12.0;
 
 label P_RESP1 = 'Predicted: RESP=1' ;
 label P_RESP0 = 'Predicted: RESP=0' ;
 
 drop _LMR_BAD;
 _LMR_BAD=0;
 
 *** Check interval variables for missing values;
 if nmiss(SMOKE,RACE,AGE) then do;
    _LMR_BAD=1;
    goto _SKIP_000;
 end;
 
 *** Compute Linear Predictors;
 drop _LP0;
 _LP0 = 0;
 
 *** Effect: SMOKE;
 _LP0 = _LP0 + (0.55511560456703) * SMOKE;
 *** Effect: RACE;
 _LP0 = _LP0 + (-0.29892228555342) * RACE;
 *** Effect: AGE;
 _LP0 = _LP0 + (0.03273258128386) * AGE;
 
 *** Predicted values;
 drop _MAXP _IY _P0 _P1;
 _TEMP = -1.42125183052254  + _LP0;
 if (_TEMP < 0) then do;
    _TEMP = exp(_TEMP);
    _P0 = _TEMP / (1 + _TEMP);
 end;
 else _P0 = 1 / (1 + exp(-_TEMP));
 _P1 = 1.0 - _P0;
 P_RESP1 = _P0;
 _MAXP = _P0;
 _IY = 1;
 P_RESP0 = _P1;
 if (_P1 >  _MAXP + 1E-8) then do;
    _MAXP = _P1;
    _IY = 2;
 end;
 select( _IY );
    when (1) do;
       I_RESP = '1' ;
       U_RESP = 1;
    end;
    when (2) do;
       I_RESP = '0' ;
       U_RESP = 0;
    end;
    otherwise do;
       I_RESP = '';
       U_RESP = .;
    end;
 end;
 _SKIP_000:
 if _LMR_BAD = 1 then do;
 I_RESP = '';
 U_RESP = .;
 P_RESP1 = .; /*score variable for positive response*/
 P_RESP0 = .; /*score variable for no response*/
 end;
 drop _TEMP;
********************************;
***   END SAS SCORING CODE   ***;
********************************;
%MEND SCOREPROG; /*End macro*/
%SCOREPROG(WORK.LOW_BIRTH_WEIGHT,WORK.LBW_SCORE); /*Execute macro*/

/*Macro to put variable into pentiles*/
%MACRO PUTPENTS(INDATA,SCORENAME,OUTDATA);
DATA GETPENT;
SET &INDATA(KEEP = ID RESP &SCORENAME);
RUN;

PROC SORT DATA=GETPENT NODUPKEY; BY ID; RUN;/*Sort score data*/
PROC RANK DATA=GETPENT GROUPS=20 OUT=HIGH3; /*Rank data by pentile & score*/
 	VAR &SCORENAME;
 	RANKS PENTILE;
RUN;

 DATA &OUTDATA;
 SET HIGH3;
 	RENAME &SCORENAME = SCORE; /*Change PRESP_1 to SCORE*/
 	PENTILE = 20 - PENTILE;
 RUN;

 PROC FREQ DATA=&OUTDATA;
 	TITLE 'FREQ - PENTILES FOR RESP BIRTH WEIGHT MODEL';
 	TABLES PENTILE;
 RUN;
 %MEND;/*End macro*/
 %PUTPENTS(WORK.LBW_SCORE,P_RESP1,WORK.LBW_SCORE_OUT); /*Execute macro*/

 DATA PER_FILE; /*build file for scores by pentile*/
 SET WORK.LBW_SCORE_OUT;
 WHERE SCORE NE .; /*exclude blank entries*/
 RUN;

 PROC TABULATE DATA=PER_FILE; /*generate score table by pentile for performance*/
 VAR SCORE RESP;
 CLASS PENTILE/ ORDER=UNFORMATTED MISSING;
 TABLE PENTILE, N SCORE*SUM RESP*SUM; /*Calculate modeled and actual responses*/
 RUN;

 PROC UNIVARIATE DATA=PER_FILE;/*generate histogram for scores*/
 VAR SCORE;
 HISTOGRAM;
 TITLE "SCORE DISTRIBUTION FORLOW BIRTH WEIGHT";
 RUN;?
SAS LOGISTIC REGRESSION OUTPUT
The following tables are output from running PROC CONTENTS.
Data Set Name	WORK.LOW_BIRTH_WEIGHT	Observations	489
Member Type	DATA	Variables	8
Engine	V9	Indexes	0
Created	03/09/2016 23:05:42	Observation Length	64
Last Modified	03/09/2016 23:05:42	Deleted Observations	0
Protection		Compressed	NO
Data Set Type		Sorted	NO
Label			
Data Representation	SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64		
Encoding	utf-8  Unicode (UTF-8)		

Engine/Host Dependent Information
Data Set Page Size	131072
Number of Data Set Pages	1
First Data Page	1
Max Obs per Page	2043
Obs in First Data Page	489
Number of Data Set Repairs	0
Filename	/saswork/SAS_work209D00008688_odaws02-prod-us/SAS_work641B00008688_odaws02-prod-us/low_birth_weight.sas7bdat
Release Created	9.0401M3
Host Created	Linux
Inode Number	28442641
Access Permission	rw-r--r--
Owner Name	jeff47
File Size	256KB
File Size (bytes)	262144


Alphabetic List of Variables and Attributes
#	Variable	Type	Len	Format	Informat
5	AGE	Num	8	BEST12.	BEST32.
2	BIRTH	Num	8	BEST12.	BEST32.
7	BWT	Num	8	BEST12.	BEST32.
1	ID	Char	2	$2.	$2.
8	LOW	Num	8	BEST12.	BEST32.
6	LWT	Num	8	BEST12.	BEST32.
4	RACE	Num	8	BEST12.	BEST32.
3	SMOKE	Num	8	BEST12.	BEST32.

The next set of tables are produced by running PROC MEANS.
Variable	N	Mean	Std Dev	Minimum	Maximum
BIRTH
SMOKE
RACE
AGE
LWT	488
488
488
488
488	1.8729508
0.3995902
1.8524590
26.4405738
142.7500000	0.8283019
0.4903167
0.9123576
5.8253635
32.4372558	1.0000000
0
1.0000000
14.0000000
80.0000000	4.0000000
1.0000000
3.0000000
48.0000000
272.0000000

The next set of tables are output resulting from running PROC FREQ.
BIRTH	Frequency	Percent	Cumulative
Frequency	Cumulative
Percent
1	188	38.52	188	38.52
2	188	38.52	376	77.05
3	98	20.08	474	97.13
4	14	2.87	488	100.00
Frequency Missing = 1

LWT	Frequency	Percent	Cumulative
Frequency	Cumulative
Percent
80	1	0.20	1	0.20
85	2	0.41	3	0.61
89	1	0.20	4	0.82
90	3	0.61	7	1.43
91	1	0.20	8	1.64
.
.
.				
92	1	0.20	9	1.84
250	1	0.20	484	99.18
254	1	0.20	485	99.39
262	1	0.20	486	99.59
267	1	0.20	487	99.80
272	1	0.20	488	100.00
Frequency Missing = 1


Table of LWT by BIRTH
LWT	BIRTH
Frequency 
Percent
Row Pct
Col Pct	1	2	3	4	Total
80	1
0.20
100.00
0.53	0
0.00
0.00
0.00	0
0.00
0.00
0.00	0
0.00
0.00
0.00	1
0.20


85	2
0.41
100.00
1.06	0
0.00
0.00
0.00	0
0.00
0.00
0.00	0
0.00
0.00
0.00	2
0.41


89	1
0.20
100.00
0.53	0
0.00
0.00
0.00	0
0.00
0.00
0.00	0
0.00
0.00
0.00	1
0.20


90	3
0.61
100.00
1.60	0
0.00
0.00
0.00	0
0.00
0.00
0.00	0
0.00
0.00
0.00	3
0.61


.
.
.					
267	0
0.00
0.00
0.00	1
0.20
100.00
0.53	0
0.00
0.00
0.00	0
0.00
0.00
0.00	1
0.20


272	0
0.00
0.00
0.00	0
0.00
0.00
0.00	1
0.20
100.00
1.02	0
0.00
0.00
0.00	1
0.20


Total	188
38.52	188
38.52	98
20.08	14
2.87	488
100.00
Frequency Missing = 1

The rest of the output is a result of running PROC LOGISTIC.

Model Information
Data Set	WORK.IMPORT
Response Variable	LOW
Number of Response Levels	2
Model	binary logit
Optimization Technique	Fisher's scoring

Number of Observations Read	489
Number of Observations Used	488

Response Profile
Ordered
Value	LOW	Total
Frequency
1	0	337
2	1	151

Probability modeled is LOW='1'.

The first part of the above output tells us the file being analyzed (WORK.LOW_BIRTH_WEIGHT) and the number of observations used. We see that all 488 observations in our data set were used in the analysis. Note: one observation was deleted due to missing values for the response or explanatory variables. The output also shows the target variable and number of levels (two since it is binary).
Stepwise Selection Procedure
Step  0. Intercept entered:
Model Convergence Status
Convergence criterion (GCONV=1E-8) satisfied.

-2 Log L	=	603.793


Residual Chi-Square Test
Chi-Square	DF	Pr > ChiSq
38.5095	5	<.0001

Step  1. Effect RACE entered:
Model Convergence Status
Convergence criterion (GCONV=1E-8) satisfied.

Model Fit Statistics
Criterion	Intercept Only	Intercept and Covariates
AIC	605.793	587.065
SC	609.984	595.445
-2 Log L	603.793	583.065

R-Square	0.0416	Max-rescaled R-Square	0.0586

Testing Global Null Hypothesis: BETA=0
Test	Chi-Square	DF	Pr > ChiSq
Likelihood Ratio	20.7286	1	<.0001
Score	20.0951	1	<.0001
Wald	19.4757	1	<.0001

Residual Chi-Square Test
Chi-Square	DF	Pr > ChiSq
18.0354	4	0.0012
Note:	No effects for the model in Step 1 are removed.

Step  2. Effect LWT entered:
Model Convergence Status
Convergence criterion (GCONV=1E-8) satisfied.

Model Fit Statistics
Criterion	Intercept Only	Intercept and Covariates
AIC	605.793	581.536
SC	609.984	594.107
-2 Log L	603.793	575.536

R-Square	0.0563	Max-rescaled R-Square	0.0793

Testing Global Null Hypothesis: BETA=0
Test	Chi-Square	DF	Pr > ChiSq
Likelihood Ratio	28.2575	2	<.0001
Score	27.3781	2	<.0001
Wald	26.2370	2	<.0001

Residual Chi-Square Test
Chi-Square	DF	Pr > ChiSq
11.1477	3	0.0110
Note:	No effects for the model in Step 2 are removed.

Step  3. Effect BIRTH entered:
Model Convergence Status
Convergence criterion (GCONV=1E-8) satisfied.

Model Fit Statistics
Criterion	Intercept Only	Intercept and Covariates
AIC	605.793	577.926
SC	609.984	594.687
-2 Log L	603.793	569.926

R-Square	0.0670	Max-rescaled R-Square	0.0945

Testing Global Null Hypothesis: BETA=0
Test	Chi-Square	DF	Pr > ChiSq
Likelihood Ratio	33.8673	3	<.0001
Score	32.5767	3	<.0001
Wald	30.8361	3	<.0001

Residual Chi-Square Test
Chi-Square	DF	Pr > ChiSq
5.5932	2	0.0610

Note:	No effects for the model in Step 3 are removed.

Step  4. Effect SMOKE entered:
Model Convergence Status
Convergence criterion (GCONV=1E-8) satisfied.

Model Fit Statistics 
The portion of the output labeled Model Fit Statistics describes and tests the overall fit of the model. The -2 Log L (565.509) can be used in comparisons of nested models, but we will not show an example of that here.
Model Fit Statistics
Criterion	Intercept Only	Intercept and Covariates
AIC	605.793	575.509
SC	609.984	596.461
-2 Log L	603.793	565.509

R-Square	0.0755	Max-rescaled R-Square	0.1063

Testing Global Null Hypothesis: BETA=0
Test	Chi-Square	DF	Pr > ChiSq
Likelihood Ratio	38.2842	4	<.0001
Score	37.4511	4	<.0001
Wald	35.1100	4	<.0001

Residual Chi-Square Test
Chi-Square	DF	Pr > ChiSq
1.1254	1	0.2888

Note:	No effects for the model in Step 4 are removed.
Note:	No (additional) effects met the 0.05 significance level for entry into the model.

In the next section of output, the likelihood ratio chi-square of 38.2842 with a p-value of < 0.0001 tells us that our model as a whole fits significantly better than an empty model. The Score and Wald tests are asymptotically equivalent tests of the same hypothesis tested by the likelihood ratio test, not surprisingly, these tests also indicate that the model is statistically significant.
Summary of Stepwise Selection
Step	Effect	DF	Number
In	Score
Chi-Square	Wald
Chi-Square	Pr > ChiSq
	Entered	Removed					
1	RACE		1	1	20.0951		<.0001
2	LWT		1	2	7.1831		0.0074
3	BIRTH		1	3	5.6510		0.0174
4	SMOKE		1	4	4.4718		0.0345

Analysis of Maximum Likelihood Estimates 
The section shows the hypothesis tests for each of the variables in the model individually. The chi-square test statistics and associated p-values shown in the table indicate that each of the four variables in the model significantly improve the model fit. 
Analysis of Maximum Likelihood Estimates
Parameter	DF	Estimate	Standard
Error	Wald
Chi-Square	Pr > ChiSq
Intercept	1	0.7346	0.6085	1.4573	0.2274
BIRTH	1	0.2788	0.1279	4.7543	0.0292
SMOKE	1	0.4641	0.2202	4.4441	0.0350
RACE	1	-0.4695	0.1262	13.8324	0.0002
LWT	1	-0.0101	0.00362	7.8257	0.0052

The above table shows the coefficients (labeled Estimate), their standard errors (error), the Wald Chi-Square statistic, and associated p-values. The coefficients for BIRTH, SMOKE, RACE, and LWT are statistically significant.  The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.
For every one unit change in BIRTH, the log odds of low birth weight (versus high birth weight) increases by 0.2788.
For a one unit increase in LWT, the log odds of having a low birth weight decreases by 0.0101.
Parameter Estimates and Wald Confidence Intervals
Parameter	Estimate	95% Confidence Limits
Intercept	0.7346	-0.4581	1.9273
BIRTH	0.2788	0.0282	0.5294
SMOKE	0.4641	0.0326	0.8957
RACE	-0.4695	-0.7169	-0.2221
LWT	-0.0101	-0.0172	-0.00303

Association of Predicted Probabilities and Observed Responses
Percent Concordant refers to a pair of observations with different observed responses is said to be concordant if the observation with the lower ordered response value (LOW = 0) has a lower predicted mean score than the observation with the higher ordered response value (LOW = 1). Percent Discordant refers to the observation with the lower ordered response value has a higher predicted mean score than the observation with the higher ordered response value, then the pair is discordant.
Association of Predicted Probabilities and Observed Responses
Percent Concordant	67.1	Somers' D	0.343
Percent Discordant	32.8	Gamma	0.343
Percent Tied	0.1	Tau-a	0.147
Pairs	50887	c	0.671

Somers' delta (or Somers' D, for short), is a nonparametric measure of the strength and direction of association that exists between an ordinal dependent variable and an ordinal independent variable. It is defined as 
(n_c-n_d)/t
where n_c is the number of pairs that are concordant, n_d the number of pairs that are discordant, and t is the number of total number of pairs with different responses While it is possible to analyze the association between two ordinal variables using Goodman and Kruskal's Gamma, Somers' D is appropriate when you want to distinguish between a dependent and independent variable. Since this model does not have ordinal variables, the Somers’ D assumptions are violated. 
Kendall's Tau-a is a modification of Somers’ D that takes into the account the difference between the number of possible paired observations and the number of paired observations with a different response. It is defined to be the ratio of the difference between the number of concordant pairs and the number of discordant pairs to the number of possible pairs
(2(n_c-n_d )/N(N-1) ).
 Usually Tau-a is much smaller than Somers’ D since there would be many paired observations with the same response.
The discriminative-ability of a logistic regression model is frequently assessed using the concordance (or c) statistic, a unitless index denoting the probability that a randomly selected subject who experienced the outcome will have a higher predicted probability of having the outcome occur compared to a randomly selected subject who did not experience the event. One can calculate the c-statistic by taking all possible pairs of subjects consisting of one subject who experienced the event of interest and one subject who did not experience the event of interest. The c-statistic is the proportion of such pairs in which the subject who experienced the event had a higher predicted probability of experiencing the event than the subject who did not experience the event (Harrell F. E., 2001). For binary logistic regression, the c-statistic (equivalent to the area under the Receiver Operating Characteristic curve) is a standard measure of the predictive accuracy of a model (see Figures 17-8 and 17-9).
Odds Ratio Estimates and Wald Confidence Intervals
The last table above gives the coefficients as odds ratios. An odds ratio is the exponentiated coefficient, and can be interpreted as the multiplicative change in the odds for a one unit change in the predictor variable. For example, for a one unit increase in BIRTH, the odds of having a low birth weight (versus a high birth weight) increase by a factor of 1.322 (se Figure 17-7).
Odds Ratio Estimates and Wald Confidence Intervals
Effect	Unit	Estimate	95% Confidence Limits
BIRTH	1.0000	1.322	1.029	1.698
SMOKE	1.0000	1.591	1.033	2.449
RACE	1.0000	0.625	0.488	0.801
LWT	1.0000	0.990	0.983	0.997

 
Figure 17-7. Wald Confidence Intervals of the Odds Ratios
 
Figure 17-8. Delected Model ROC Curve
 
Figure 17-9.ROC Curves for all model building steps
A Receiver Operating Characteristic Curve (ROC) is a standard technique for summarizing classifier performance over a range of trade-offs between true positive (TP) and false positive (FP) error rates. It is plotting the values of the cumulative percent of the captured responses.

Estimated Covariance Matrix
Parameter	Intercept	BIRTH	SMOKE	RACE	LWT
Intercept	0.370317	-0.00738	-0.05322	-0.04181	-0.00178
BIRTH	-0.00738	0.016348	-0.00186	-0.00154	-0.00014
SMOKE	-0.05322	-0.00186	0.048474	0.010067	0.000122
RACE	-0.04181	-0.00154	0.010067	0.015934	0.000093
LWT	-0.00178	-0.00014	0.000122	0.000093	0.000013

Estimated Correlation Matrix
Parameter	Intercept	BIRTH	SMOKE	RACE	LWT
Intercept	1.0000	-0.0948	-0.3972	-0.5443	-0.8079
BIRTH	-0.0948	1.0000	-0.0662	-0.0955	-0.3121
SMOKE	-0.3972	-0.0662	1.0000	0.3622	0.1528
RACE	-0.5443	-0.0955	0.3622	1.0000	0.2026
LWT	-0.8079	-0.3121	0.1528	0.2026	1.0000

Partition for the Hosmer and Lemeshow Test
Group	Total	LOW = 1	LOW = 0
		Observed	Expected	Observed	Expected
1	49	8	6.68	41	42.32
2	51	10	8.99	41	42.01
3	49	12	9.88	37	39.12
4	49	6	11.29	43	37.71
5	49	10	13.07	39	35.93
6	50	16	15.65	34	34.35
7	49	21	17.41	28	31.59
8	48	21	19.94	27	28.06
9	49	20	23.27	29	25.73
10	45	27	24.82	18	20.18

Hosmer and Lemeshow Goodness-of-Fit Test
Chi-Square	DF	Pr > ChiSq
7.7688	8	0.4564

Classification Table
Prob
Level	Correct	Incorrect	Percentages
	Event	Non-
Event	Event	Non-
Event	Correct	Sensi-
tivity	Speci-
ficity	False
POS	False
NEG
0.100	150	4	333	1	31.6	99.3	1.2	68.9	20.0
0.200	126	95	242	25	45.3	83.4	28.2	65.8	20.8
0.300	100	202	135	51	61.9	66.2	59.9	57.4	20.2
0.400	61	269	68	90	67.6	40.4	79.8	52.7	25.1
0.500	27	316	21	124	70.3	17.9	93.8	43.8	28.2
0.600	3	334	3	148	69.1	2.0	99.1	50.0	30.7
0.700	0	337	0	151	69.1	0.0	100.0	.	30.9
0.800	0	337	0	151	69.1	0.0	100.0	.	30.9
0.900	0	337	0	151	69.1	0.0	100.0	.	30.9
1.000	0	337	0	151	69.1	0.0	100.0	.	30.9

The following diagnostic detect potential observations that have a significant impact on the model. There are several reasons that we need to detect influential observations. 
	We might have data entry errors
	We might have influential observations of interest by themselves for us to study
	We might have influential data points that badly skew the regression estimation
How large does each one have to be, to be considered influential? First of all, we always have to make our judgment based on our theory and our analysis. Secondly, there are some rule-of-thumb cutoffs when the sample size is large. These are shown below. When the sample size is large, the asymptotic distribution of some of the measures would follow some standard distribution. That is why we have these cutoff values, and why they only apply when the sample size is large enough. Usually, we would look at the relative magnitude of a statistic an observation has compared to others. That is, we look for data points that are farther away from most of the data points.
Measure	Value
leverage (hat value)	>2 or 3 times of the average of leverage
abs(Pearson Residuals)	> 2
abs(Deviance Residuals)	> 2

Pearson residuals and its standardized version is one type of residual. Pearson residuals are defined to be the standardized difference between the observed frequency and the predicted frequency. They measure the relative deviations between the observed and fitted values. Deviance residual is another type of residual. It measures the disagreement between the maxima of the observed and the fitted log likelihood functions. Since logistic regression uses the maximal likelihood principle, the goal in logistic regression is to minimize the sum of the deviance residuals. Therefore, this residual is parallel to the raw residual in OLS regression, where the goal is to minimize the sum of squared residuals. Another statistic, sometimes called the hat diagonal since technically it is the diagonal of the hat matrix, measures the leverage of an observation. It is also sometimes called the Pregibon leverage. These three statistics, Pearson residual, deviance residual and Pregibon leverage are considered to be the three basic building blocks for logistic regression diagnostics. We always want to inspect these first.
 
Figure 17-10. Influence Diagnostic plots
 
Figure 17-11. Influence Diagnostic plots
 
Figure 17-12. Influence Diagnostic plots
The last type of diagnostic statistics is related to coefficient sensitivity and is plotted for each predictor. Similar to OLS regression, logistic regression also has dfbeta's (see figures 17-13 and 17-14). They measure how much impact each observation has on each parameter estimate. 
 
Figure 17-13. Influence Diagnostic plots
 
Figure 17-14. Influence Diagnostic plots

Figure 17-15 is generated from the PHAT option (I just chose to print all diagnostic plots with PLOTS(ONLY) = ALL. 
The shapes of the plots are similar and show quadratic like curves. Points falling in the top left or top right corners of the plots are poorly fit. Assessment of this distance is partly based on numerical value and partly based on visual impression. Under n-asymptotic the value of upper ninety-fifth percentile of chi-square distribution with 1 degree of freedom is 3.84 and may provide some guidance as to whether an observation is an outlier or influential point. Thus the cases having numerical values larger than this cut-off point, which is based on ?2 difference or deviance difference, can be considered as outlying observations.
 
Figure 17-15. Predicted probability diagnostic plots
High-leverage points (see Figure 17-16) are those observations, if any, made at extreme or outlying values of the independent variables such that the lack of neighboring observations means that the fitted regression model will pass close to that particular observation (Everitt, Cambridge Dictionary of Statistics, 2002).

 
Figure 17-16. Leverage Diagnostic plots
Figure 17-17 is generated from the DPC option (or PLOTS(ONLY) = ALL). If we have observations in the bottom “cup" in red then we need to scrutinize them more closely. These observations influence the parameter estimates to a relatively large extent but are not poorly fitted.

 
Figure 17.17. Influence on the Model Fir and Parameter Estimates
 
Figure 17-18. Predicted Probabilities for LOW = 1 with 95% confidence limits
?
SAS Score Output
FREQ - PENTILES FOR RESP BIRTH WEIGHT MODEL
The FREQ Procedure

Rank for Variable P_RESP1*
PENTILE	Frequency	Percent	Cumulative
Frequency	Cumulative
Percent
1	34	4.93	34	4.93
2	34	4.93	68	9.87
3	31	4.50	99	14.37
4	41	5.95	140	20.32
5	32	4.64	172	24.96
6	36	5.22	208	30.19
7	28	4.06	236	34.25
8	40	5.81	276	40.06
9	31	4.50	307	44.56
10	38	5.52	345	50.07
11	31	4.50	376	54.57
12	36	5.22	412	59.80
13	37	5.37	449	65.17
14	33	4.79	482	69.96
15	35	5.08	517	75.04
16	38	5.52	555	80.55
17	31	4.50	586	85.05
18	33	4.79	619	89.84
19	38	5.52	657	95.36
20	32	4.64	689	100.00
Frequency Missing = 1

*This table is used for performance 
	N	Predicted: RESP=1	RESP
		Sum	Sum
Rank for Variable P_RESP1	34	16.84	20.00
1			
2	34	15.55	15.00
3	31	13.43	10.00
4	41	17.00	16.00
5	32	12.58	18.00
6	36	13.50	10.00
7	28	10.01	7.00
8	40	13.55	16.00
9	31	9.87	10.00
10	38	11.58	12.00
11	31	8.92	11.00
12	36	9.68	12.00
13	37	9.36	7.00
14	33	7.78	4.00
15	35	7.69	8.00
16	38	7.75	5.00
17	31	5.89	5.00
18	33	5.96	11.00
19	38	6.41	6.00
20	32	4.85	6.00

Moments
N	689	Sum Weights	689
Mean	0.30220458	Sum Observations	208.218959
Std Deviation	0.10039431	Variance	0.01007902
Skewness	0.2557412	Kurtosis	-1.0445616
Uncorrected SS	69.8590883	Corrected SS	6.93436435
Coeff Variation	33.220645	Std Error Mean	0.00382472

Basic Statistical Measures
Location	Variability
Mean	0.302205	Std Deviation	0.10039
Median	0.300191	Variance	0.01008
Mode	0.172906	Range	0.40127
		Interquartile Range	0.16919
Note: The mode displayed is the smallest of 2 modes with a count of 18.
Tests for Location: Mu0=0
Test	Statistic	p Value
Student's t	t	79.01355	Pr > |t|	<.0001
Sign	M	344.5	Pr >= |M|	<.0001
Signed Rank	S	118852.5	Pr >= |S|	<.0001

Quantiles (Definition 5)
Level	Quantile
100% Max	0.535999
99%	0.511510
95%	0.470630
90%	0.438181
75% Q3	0.382801
50% Median	0.300191
25% Q1	0.213609
10%	0.172906
5%	0.163743
1%	0.146594
0% Min	0.134731

Extreme Observations
Lowest	Highest
Value	Obs	Value	Obs
0.134731	563	0.511510	468
0.134731	452	0.511510	600
0.138592	526	0.527849	689
0.142547	222	0.535999	96
0.142547	125	0.535999	269

 
Figure 17-19. Score Distribution
Logistic Regression using Python
This is an example of performing logistic regression in Python with the scikit-learn module. In this example, we perform many useful python functions beyond what we need for a simple model. This is done partially to explore some more advanced modeling, array manipulation, evaluation, and so on. We will also use several external modules importing data, formatting data, manipulating data, modeling and graphical exploration.
To perform this task, download and install Anaconda (64 bit). The download will include everything you need. When you start Anaconda
Dataset
The dataset I chose for this example is the same Longitudinal Low Birth Weight Study (CLSLOWBWT.DAT) from Hosmer and Lemeshow (2000) Applied Logistic Regression: Second Edition. These data are copyrighted by John Wiley & Sons Inc. and must be acknowledged and used accordingly.
List of Variables:
Variable Description Codes/Values Name
	Identification Code ID Number ID
	Birth Number 1-4 BIRTH
	Smoking Status 0 = No, 1 = Yes SMOKE During Pregnancy
	Race 1 = White, 2 = Black RACE 3 = Other
	Age of Mother Years AGE
	Weight of Mother at Pounds LWT Last Menstrual Period
	Birth Weight Grams BWT
	Low Birth Weight 1 = BWT <=2500g, LOW 0 = BWT >2500g
Problem Statement
In this example, we want to predict Low Birth Weight using the remaining dataset variables. Low Birth Weight, the dependent variable, 1 = BWT <=2500g and 0 = BWT >2500g.
Import Modules
In [1]:
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from patsy import dmatrices
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import train_test_split
from sklearn import metrics
from sklearn.cross_validation import cross_val_score
Data Pre-Processing
In [120]:
# load dataset
dta = pd.read_csv("C:\Users\Strickland\Documents\Python Scripts\CLSLOWBWT.csv")
Data Exploration
In [121]:
dta.groupby('LOW').mean()
Out[121]:
	ID	BIRTH	SMOKE	RACE	AGE	LWT	BWT
LOW							
0	92.8753	1.8367	0.3382	1.9762	26.1632	144.7418	3204.059
1	95.0927	1.9536	0.5364	1.5761	27.0596	138.3046	2033.867

We can see that on average, women who have children with a low birth weight are more likely to be smokers than nonsmokers. This makes sense based on scientific studies. Let's take another look at the Birth Number variable.
In [8]:
dta.groupby('BIRTH').mean()
Out[8]:
	ID	SMOKE	RACE	AGE		LWT	BWT	LOW
BIRTH								
1	94.5000	0.3936	1.8510	22.8191		129.8510	2848.1595	0.2819
2	94.5000	0.3936	1.8510	26.9308		150.6382	2857.3670	0.3031
3	89.0918	0.4081	1.8571	31.1530		150.5204	2838.1530	0.3673
4	99.6428	0.5000	1.8571	35.5000		155.6428	2578.8571	0.3571

Low Birth Weight trends upward with more births.
Data Visualization
In [9]:
# show plots in the notebook
%matplotlib inline

In [14]:
# histogram of birth number (see Figure 17-20)
dta.BIRTH.hist()
plt.title('Histogram of Low Birth Weight')
plt.xlabel('Birth Number')
plt.ylabel('Frequency')
Out[14]:

 
Figure 17-20.Histogram of Low Birth Weight
In [15]:
# histogram of age of mother (see Figure 17-21)
dta.AGE.hist()
plt.title('Histogram of Age of Mother')
plt.xlabel('Age')
plt.ylabel('Frequency')
Out[15]:
 
Figure 17-21.Histogram of Age of Mother
Let's take a look at the distribution of smokers for those having children with low birth weights versus those who do not (see Figure 17-22).
In [16]:
# barplot of low birth weights grouped by smoker status (True or False
pd.crosstab(dta.SMOKE, dta.LOW.astype(bool)).plot(kind='bar')
plt.title('Somker Distribution by Low Birth Weight')
plt.xlabel('Smoker')
plt.ylabel('Frequency')
Out[16]:
 
Figure 17-22. Bar Chart of Smoker Distribution by Low Birth Weight
Now let's use a stacked barplot to look at the percentage of women having children with low birth weights by age (see Figure 17-23).
In [17]:
low_age = pd.crosstab(dta.AGE, dta.LOW.astype(bool))
low_age.div(low_age.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Low Birth Weight Percentage by Age of Mother')
plt.xlabel('Age of Mother')
plt.ylabel('Percentage')
Out[17]:
 
Figure 17-23. Bar Chart of Low Birth Weight by Age of Mother

Prepare Data for Logistic Regression¶
To prepare the data, we want to add an intercept column as well as dummy variables for Age of Mother and Weight of Mother at Last Menstrual Period, since we are treating them as categorical variables. The dmatrices function from the patsy module can do that using formula language.
The column names for the dummy variables are messy, so let's rename those.
In [193]:
# create dataframes with an intercept column and dummy variables for
y, X = dmatrices('LOW ~ ID + BIRTH + SMOKE + RACE + AGE + LWT',
                  dta, return_type="dataframe")
print X.columns
Index([u'Intercept', u'ID', u'BIRTH', u'SMOKE', u'RACE', u'AGE', u'LWT'], dtype='object')

We now want to convert the numeric (interval) variable AGE to a categorical variable with 4 classes.
We also need to flatten y into a 1-D array, so that scikit-learn will properly understand it as the response variable.
In [194]:
# flatten y into a 1-D array
y = np.ravel(y)
First Logistic Regression¶
Let's go ahead and run logistic regression on the entire data set, and see how accurate it is.
In [195]:
# instantiate a logistic regression model, and fit with X and y
model = LogisticRegression()
model = model.fit(X, y)

# check the accuracy on the training set
model.score(X, y)
Out[195]:
0.70696721311475408

70.7% accuracy seems good, but what's the null error rate?
In [149]:
# what percentage had low birth weights?
y.mean()
Out[149]:
0.3094262295081967

Only 31% of the women had low birth rate children, which means that you could obtain 69% accuracy by always predicting "no". So we're doing better than the null error rate, but not by much. Let's examine the coefficients to see what we learn.
In [150]:
# examine the coefficients
pd.DataFrame(zip(X.columns, np.transpose(model.coef_)))
Out[150]:
0	1	
0	Intercept	[0.0336484036311]
1	ID	[0.00231314008486]
2	BIRTH	[0.185995251393]
3	SMOKE	[0.546655639398]
4	RACE	[-0.413795537922]
5	AGE	[0.0257483072665]
6	LWT	[-0.0115073134929]

Converting Variables & Merging Dataframes
Now, we want to take the variable AGE and convert it to a categorical variable, to see if we can improve the model. We will do this by creating two data frames from our original data. We will then merge the two data frames, so both need to contain the ID variable. One data from will have the converted categorical variable age_group, and the other will have the dependent variable and the other independent variables.
In [205]:
df1 = pd.DataFrame(dta, columns=['ID','AGE'])
df2 = pd.DataFrame(dta, columns=['ID', 'BIRTH', 'SMOKE', 'RACE', 'LWT', 'LOW'])
bins = [15, 25, 35, 45, 55]
group_names = ['15-24', '25-34', '35-44', '45-55']
age_groups = pd.cut(df1['AGE'], bins, labels=group_names)
df1['age_groups'] = pd.cut(df1['AGE'], bins, labels=group_names)
categories
df1.head(5)
Out[205]:
	ID	AGE	age_groups
0	1	28	25-34
1	1	33	25-34
2	2	29	25-34
3	2	34	25-34
4	2	37	35-44

We now merge the two data frames.
In [215]:
left = df2
right = df1
result = pd.merge(left, right, on='ID')
result.head(5)
Out[215]:
	ID	BIRTH	SMOKE	RACE	LWT	LOW	AGE	age_groups
0	1	1	1	3	120	0	28	25-34
1	1	1	1	3	120	0	33	25-34
2	1	2	1	3	141	0	28	25-34
3	1	2	1	3	141	0	33	25-34
4	2	1	0	1	130	0	29	25-34

Second Logistic Regression
We are now ready to build and evaluate our second logistic regression model, using the merged data frames.
In [208]:
y, Z = dmatrices('LOW ~ BIRTH + SMOKE + RACE + age_groups + LWT', result, return_type="dataframe")
print Z.columns
Index([u'Intercept', u'age_groups[T.25-34]', u'age_groups[T.35-44]',
       u'age_groups[T.45-55]', u'BIRTH', u'SMOKE', u'RACE', u'LWT'],
      dtype='object')
Since we change the size of y when we converted AGE and merged, we also need to flatten y again into a 1-D array, so that scikit-learn will properly understand it as the response variable
In [209]:
# flatten y into a 1-D array
y = np.ravel(y)
Before we perform the logistic regression, we want to check the matrix we just formed to ensure it is consistent with our intent.
In [210]:
Z.head(5)
Out[210]:
	Intercept	age_groups [T.25-34]	age_groups [T.35-44]	age_groups [T.45-55]	BIRTH	SMOKE	RACE	LWT
0	1	1	0	0	1	1	3	120
1	1	1	0	0	1	1	3	120
2	1	1	0	0	2	1	3	141
3	1	1	0	0	2	1	3	141
4	1	1	0	0	1	0	1	130

Finally, we are ready to execute the logistic regression model and see how accurate it is.
In [211]:
# instantiate a logistic regression model, and fit with X and y
model1 = LogisticRegression()
model1 = model1.fit(Z, y)

# check the accuracy on the training set
model1.score(Z, y)

Out[211]:
0.71320754716981127

71.3% accuracy seems good, but what's the null error rate? We also want to recheck the percentage of low birth weights.
In [212]:
# what percentage had low birth weights?
y.mean()

Out[212]:
0.30867924528301888

Still, only 31% of the women had low birth rate children, which means that you could obtain 69% accuracy by always predicting "no". So we're doing better than the null error rate, but not by much. Let's examine the coefficients to see what we learn.
In [213]:
# examine the coefficients
pd.DataFrame(zip(Z.columns, np.transpose(model.coef_)))
Out[213]:
	0	1
0	Intercept	[0.0336484036311]
1	age_groups[T.25-34]	[0.00231314008486]
2	age_groups[T.35-44]	[0.185995251393]
3	age_groups[T.45-55]	[0.546655639398]
4	BIRTH	[-0.413795537922]
5	SMOKE	[0.0257483072665]
6	RACE	[-0.0115073134929]

Increases in Birth Number and RACE correspond to a decrease in the likelihood of having a Low Birth Weight child. A decrease in Smoking Status corresponds to a decrease in the likelihood of having an Low Birth Weight child. For Age Group, the lowest likelihood of having Low Birth Weight child corresponds to the baseline age group (15-24), since all of the dummy coefficients are positive.
Model Evaluation Using a Validation Set¶
So far, we have trained and tested on the same set. Let's instead split the data into a training set and a testing set.
In [216]:
# evaluate the model by splitting into train and test sets
Z_train, Z_test, y_train, y_test = train_test_split(Z, y, test_size=0.3, random_state=0)
model2 = LogisticRegression()
model2.fit(Z_train, y_train)
Out[216]:
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr',
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0)

We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look.
In [222]:
# predict class labels for the test set
predicted = model2.predict(Z_test)
print predicted
[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0.
 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.
 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0.]
In [223]:
# generate class probabilities
probs = model2.predict_proba(Z_test)
print probs
[[ 0.70610757  0.29389243]
 [ 0.82942743  0.17057257]
 [ 0.78996271  0.21003729]
 [ 0.71714883  0.28285117]
 [ 0.82860543  0.17139457]
 [ 0.66919892  0.33080108]
 [ 0.80411336  0.19588664]
 [ 0.82942743  0.17057257]
 [ 0.80489925  0.19510075]
 .
 .
 .
 [ 0.70983649  0.29016351]
 [ 0.53676859  0.46323141]
 [ 0.87417101  0.12582899]
 [ 0.59655861  0.40344139]
 [ 0.78276183  0.21723817]
 [ 0.71000202  0.28999798]]

As you can see, the classifier is predicting a 1 (having a Low Birth Weight child) any time the probability in the second column is greater than 0.5. Now let's generate some evaluation metrics.
In [224]:
# generate evaluation metrics
print metrics.accuracy_score(y_test, predicted)
print metrics.roc_auc_score(y_test, probs[:, 1])
0.678391959799
0.678443267259

The accuracy is 67.8%, which is the close to what we experienced when training and predicting on the same data. We can also see the confusion matrix and a classification report with other metrics.
In [225]:
print metrics.confusion_matrix(y_test, predicted)
print metrics.classification_report(y_test, predicted)
[[239  27]
 [101  31]]
             precision    recall  f1-score   support

        0.0       0.70      0.90      0.79       266
        1.0       0.53      0.23      0.33       132

avg / total       0.65      0.68      0.64       398

Model Evaluation Using Cross-Validation¶
Now let's try 10-fold cross-validation, to see if the accuracy holds up more rigorously.
In [226]:
# evaluate the model using 10-fold cross-validation
scores = cross_val_score(LogisticRegression(), Z, y, scoring='accuracy', cv=10)
print scores
print scores.mean()
[ 0.53383459  0.7518797   0.69924812  0.68421053  0.64661654  0.80451128
  0.73484848  0.66666667  0.61363636  0.71755725]
0.685300951894

Looks good. It's still performing at 69% accuracy.
In [ ]:
 
Predicting the Probability of Low Birth Weight Child¶
Just for fun, let's predict the probability of a low birth weight child for a random woman not present in the dataset. She's a 35-year-old Other race, has had 2 births,(has 2 children), is a smoker, and her weight is 132.
In [228]:
model.predict_proba(np.array([0, 0, 1, 1, 3, 2, 1]))
Out[228]:
array([[ 0.60709037,  0.39290963]])

The predicted probability of low birth weight child is 39.3%
Next Steps¶
There are many different steps that could be tried in order to improve the model:
including interaction terms
removing features
regularization techniques
using a non-linear model

Examples Using R
Logistic Regression: Bank Marketing Campaign – Mixed Predictors
This data set was obtained from the UC Irvine Machine Learning Repository and contains information related to a direct marketing campaign of a Portuguese banking institution and its attempts to get its clients to subscribe for a term deposit.
Source
The path to this data set is https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip. This data set was obtained by downloading the zip bank.zip, which includes bank-full.csv and bank.csv. The table contains 41,188 rows and 21 columns.
Input Variables (see below: Attribute Information)
There are 20 columns in the table that provide information about each client, such as age, marital status, and education level. A subset of these are related to the last contact of the current campaign, such as the month and day of the week the last contact was made as well as the number of days since the client was last contacted in a previous campaign. There are 10 columns in the table that are categorical, meaning that they contain textual values that correspond to a particular category for a given variable.
Citation Request:
This dataset is public available for research. The details are described in [Moro et al., 2011]. Please include this citation if you plan to use this database:
Relevant Information:
The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed. 
Model Goal: The classification goal is to predict if the client will subscribe a term deposit (variable y=RESP).
Number of Instances: 45211 for bank-full.csv (4521 for bank.csv)
Number of Attributes: 16 + output attributes.
Modification:
I modified this data set by converting “Yes”/”No” variables to binary variables with 1 = yes and 0 = no. 
In addition to the response variable for low birth weight (RESP) there are eight variables in the dataset.
	AGE (numeric)
	JOB : type of job (categorical: "admin.", "unknown", "unemployed", "management", "housemaid", "entrepreneur", "student", "blue-collar", "self-employed", "retired", "technician", "services") 
	MARITAL : marital status (categorical: "married", "divorced", "single"; note: "divorced" means divorced or widowed)
	EDUCATION (categorical: "unknown", "secondary", "primary", "tertiary")
	DEFAULT: has credit in default? (binary: "yes", "no")
	BALANCE: average yearly balance, in euros (numeric) 
	HOMEOWNER: has housing loan? (binary: "yes", "no")
	LOANS: has personal loan? (binary: "yes", "no")
# related with the last contact of the current campaign:
	CONTACT: contact communication type (categorical: "unknown", "telephone", "cellular") 
	LENGTH: length of most recent membership (numeric)
	CAMPAIGN: number of contacts performed during this campaign and for this client (numeric, includes last contact)
	PDAYS: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)
	PREVIOUS: number of contacts performed before this campaign and for this client (numeric)
	POUTCOME: outcome of the previous marketing campaign (categorical: "unknown", "other", "failure", "success")
	Output variable (desired target):
RESP - has the client subscribed a term deposit? (binary: 1="yes", 0="no")
file = "C:/’your directory path’/Banking.csv"
Next, we read the .csv file into an R dataset called “lbw.”
read.csv(file) -> bank
We can explore the data using the summary command.
summary(bank)

          job           marital          education    
 blue-collar:9732   divorced: 5207   primary  : 6851  
 management :9458   married :27214   secondary:23202  
 technician :7597   single  :12790   tertiary :13301  
 admin.     :5171                    unknown  : 1857  
 services   :4154                                     
 retired    :2264                                     
 (Other)    :6835                                     
      age           balance         homeowner     
 Min.   :18.00   Min.   : -8019   Min.   :0.0000  
 1st Qu.:33.00   1st Qu.:    72   1st Qu.:0.0000  
 Median :39.00   Median :   448   Median :1.0000  
 Mean   :40.94   Mean   :  1362   Mean   :0.5558  
 3rd Qu.:48.00   3rd Qu.:  1428   3rd Qu.:1.0000  
 Max.   :95.00   Max.   :102127   Max.   :1.0000  
                                                  
     loans           default           contact     
 Min.   :0.0000   Min.   :0.00000   Min.   :0.000  
 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.000  
 Median :0.0000   Median :0.00000   Median :2.000  
 Mean   :0.1602   Mean   :0.01803   Mean   :1.488  
 3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:2.000  
 Max.   :1.0000   Max.   :1.00000   Max.   :3.000  
                                                   
     length          campaign          pdays      
 Min.   :   0.0   Min.   : 1.000   Min.   : -1.0  
 1st Qu.: 103.0   1st Qu.: 1.000   1st Qu.: -1.0  
 Median : 180.0   Median : 2.000   Median : -1.0  
 Mean   : 258.2   Mean   : 2.764   Mean   : 40.2  
 3rd Qu.: 319.0   3rd Qu.: 3.000   3rd Qu.: -1.0  
 Max.   :4918.0   Max.   :63.000   Max.   :871.0  
                                                  
    previous           poutcome         RESP      
 Min.   :  0.0000   Min.   :0.00   Min.   :0.000  
 1st Qu.:  0.0000   1st Qu.:2.00   1st Qu.:0.000  
 Median :  0.0000   Median :2.00   Median :0.000  
 Mean   :  0.5803   Mean   :1.75   Mean   :0.117  
 3rd Qu.:  0.0000   3rd Qu.:2.00   3rd Qu.:0.000  
 Max.   :275.0000   Max.   :2.00   Max.   :1.000  

Now we divide the data into a training set and test set of equal length. We will use the training set to build a new logistic regression model and the test set to make predictions based on the model.
bank_train<-bank[2:22606,]
bank_test<-bank[22607:45211,]

We should form these as random samples from the base dataset, but I wanted to show you how to index arrays. In bank_train[1:22606,] we are calling records (rows) 1 to 22606 and by leaving a blank after the comma, we are calling all variables (columns). So we can think of the command as bank[rows,columns]. We could have also specified the columns as bank[1:22606, 1:6].
We can also plot the data. Here I will just plot variables 3 through 6. From the plot matrix it should be clear that EDUCATON is a categorical variable, while AGE and BALANCE are numeric variables. HOMEOWNER is a binary variable, for instance, it has two categories, Yes or No, represented by 1 and 0 respectively.
plot(bank_train[,3:6])
 
Figure 17-24. Scatterplot matrix of Low Birth Weight training set
Now that we have the data in R, we can also print a correlation matrix and examine the results to see if there are any highly correlated variables. We can also plot the individual variables, for instance, AGE:
plot(bank_train[,"age"])
 
Figure 17-25. Scatterplot of Age from the training set
plot(bank_train[,"education"])
 
Figure 17-26. Plot of Education from the training set
cor(bank_train[,4:11])

                    age      balance    homeowner
age        1.0000000000  0.080224168 -0.214549425
balance    0.0802241683  1.000000000 -0.053650626
homeowner -0.2145494247 -0.053650626  1.000000000
loans     -0.0199884191 -0.085608194 -0.039299216
default   -0.0308172862 -0.079035629 -0.019621671
contact    0.0702207722 -0.016283046 -0.335629034
length    -0.0488567712  0.016637554  0.019384029
campaign  -0.0004657117  0.005620917 -0.004349685
                  loans      default      contact
age       -0.0199884191 -0.030817286  0.070220772
balance   -0.0856081939 -0.079035629 -0.016283046
homeowner -0.0392992163 -0.019621671 -0.335629034
loans      1.0000000000  0.083548500  0.101075921
default    0.0835484997  1.000000000  0.010899902
contact    0.1010759214  0.010899902  1.000000000
length    -0.0009396837 -0.006051515  0.001365633
campaign   0.0046629738  0.004675987  0.115804459
                 length      campaign
age       -0.0488567712 -0.0004657117
balance    0.0166375539  0.0056209170
homeowner  0.0193840290 -0.0043496853
loans     -0.0009396837  0.0046629738
default   -0.0060515155  0.0046759871
contact    0.0013656332  0.1158044593
length     1.0000000000 -0.0714542068
campaign  -0.0714542068  1.0000000000

It appears that of all our variables, AGE and BIRTH are highly correlated. We can include both for the initial modeling, but we have to ensure the less significant variable is removed afterward if one or poth prove to be significant. 
Now, we fit a logistic regression model using a general linear model in R (glm is part of base R). RESP is the dependent variable and we use JOB, MARITAL, EDUCATION, AGE, BALANCE, HOMEOWNER, LOANS, DEFAULT, CONTACT, LENGTH, CAMPAIGN, PDAYS, PREVIOUS, and POUTCOME as possible predictors. Recall that our response variable, RESP, is binomial. Since we are performing a logistic regression, we use a logit link function.
bank.model <- glm( RESP ~ job+marital+education+age+balance+homeowner+loans+default+contact+length+campaign+pdays+previous+poutcome , data = bank_train, family=binomial(logit))
Print a summary of the fitted model
summary(bank.model)

Call:
glm(formula = RESP ~ job + marital + education + age + balance + 
    homeowner + loans + default + contact + length + campaign + 
    pdays + previous + poutcome, family = binomial(logit), data = bank_train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-5.1945  -0.1982  -0.1414  -0.1066   3.2080  

Coefficients: (3 not defined because of singularities)
                     Estimate Std. Error z value
(Intercept)        -4.682e+00  3.210e-01 -14.585
jobblue-collar     -2.164e-02  1.508e-01  -0.143
jobentrepreneur     1.198e-01  2.407e-01   0.498
jobhousemaid       -9.775e-01  3.005e-01  -3.253
jobmanagement      -2.320e-01  1.758e-01  -1.319
jobretired         -1.325e-01  2.490e-01  -0.532
jobself-employed   -1.611e-01  2.541e-01  -0.634
jobservices        -2.266e-01  1.796e-01  -1.262
jobstudent          2.950e-01  4.694e-01   0.628
jobtechnician      -6.867e-02  1.529e-01  -0.449
jobunemployed       1.587e-01  2.648e-01   0.599
jobunknown         -9.036e-01  7.610e-01  -1.187
maritalmarried     -6.651e-01  1.142e-01  -5.824
maritalsingle      -3.288e-01  1.327e-01  -2.477
educationsecondary  2.445e-02  1.285e-01   0.190
educationtertiary   8.051e-03  1.635e-01   0.049
educationunknown   -3.372e-01  2.455e-01  -1.374
age                -8.115e-03  5.018e-03  -1.617
balance            -1.054e-05  1.593e-05  -0.662
homeowner          -2.429e-01  8.783e-02  -2.766
loans              -2.800e-01  1.065e-01  -2.630
default             4.963e-01  2.233e-01   2.223
contact             3.147e-01  4.150e-02   7.583
length              5.496e-03  1.179e-04  46.623
campaign           -1.220e-03  1.287e-02  -0.095
pdays                      NA         NA      NA
previous                   NA         NA      NA
poutcome                   NA         NA      NA
                   Pr(>|z|)    
(Intercept)         < 2e-16 ***
jobblue-collar      0.88594    
jobentrepreneur     0.61870    
jobhousemaid        0.00114 ** 
jobmanagement       0.18710    
jobretired          0.59451    
jobself-employed    0.52613    
jobservices         0.20695    
jobstudent          0.52973    
jobtechnician       0.65345    
jobunemployed       0.54889    
jobunknown          0.23505    
maritalmarried     5.74e-09 ***
maritalsingle       0.01324 *  
educationsecondary  0.84905    
educationtertiary   0.96073    
educationunknown    0.16946    
age                 0.10584    
balance             0.50804    
homeowner           0.00567 ** 
loans               0.00855 ** 
default             0.02622 *  
contact            3.38e-14 ***
length              < 2e-16 ***
campaign            0.92446    
pdays                    NA    
previous                 NA    
poutcome                 NA    
---
Signif. codes:  
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8724.4  on 22604  degrees of freedom
Residual deviance: 5046.9  on 22580  degrees of freedom
AIC: 5096.9

Number of Fisher Scoring iterations: 7
Jobhousemaid, maritalmarried, maritalsingle, homeowner, loans, default, contact, length
The conclusion we make here is Response to a Marketing Campaign is predictable using JOB, MARITAL, HOMEOWNER, LOANS, DEFAULT, CONTACT and LENGTH. 
Previously, we divided the data into a training set and test set of equal length. We will use the training set to build a new logistic regression model and the test set to make predictions based on the model.
The next step is to fit a logistic regression model to the training set.
bank.model2 <- glm( RESP ~ job+marital+homeowner+loans+default+contact+length , data = bank_train, family=binomial(logit))

Now perform an analysis of variance (ANOVA)
anova(bank.model2, test="Chisq")

Analysis of Deviance Table

Model: binomial, link: logit

Response: RESP

Terms added sequentially (first to last)


          Df Deviance Resid. Df Resid. Dev  Pr(>Chi)
NULL                      22604     8724.4          
job       11     14.7     22593     8709.6 0.1947876
marital    2     53.8     22591     8655.8 2.055e-12
homeowner  1     13.3     22590     8642.6 0.0002695
loans      1      1.6     22589     8640.9 0.2031058
default    1      0.5     22588     8640.5 0.4898255
contact    1     62.4     22587     8578.0 2.743e-15
length     1   3524.5     22586     5053.5 < 2.2e-16
             
NULL         
job          
marital   ***
homeowner ***
loans        
default      
contact   ***
length    ***
---
Signif. codes:  
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

The ANOVA gives us what amounts to regression coefficients with standard errors and a z-test. Three of the coefficients are significantly different from zero, SMOKE, RACE, and LWT, which supports our model analysis above. The deviance was reduced by 3670.9 points (8724.4 – 5053.5) on 18 degrees of freedom, for a p-value of:
pval<-1 - pchisq(5053.5, df=18)
pval

[1] 0

Now, we construct a frequency distribution to use for constructing a plot of the fitted data.
with(bank_train(RESP))

RESP
  0      1 
21517 1088 

This tells us that the percent of low birth weights is about 1088/21517 = 5%.  
Finally, we plot the fitted data
plot(bank.model$fitted)
abline(v=11303,col="red",lwd=2)
abline(h=.25,col="green",lwd=2)
abline(h=.5,col="green",lwd=2)
abline(h=.75,col="green",lwd=2)
text(15,.6,"RESP = 0")
text(400,.1,"RESP = 1")
 
Figure 17-27. Plot of the Bank model with fitted values
For our next task, we need to load the library “gains”. Here we will partition our data and build a new logistic regression using a training set and then use the fitted model to make a prediction using a test set. After we have the predicted values, we will use gains to construct a gains table and plot.gains to plot several outcomes of the gains function.
require(gains)

Loading required package: gains
Warning message:
package ‘gains’ was built under R version 3.1.3 

Now we predict values using the fitted model. 
bank.pred<-predict(bank.model2, newdata = bank_test, type="response")
Note the command, predict(fitted model, test set, type of prediction), fits the test dated using the logistic regression model that we constructed from the training set.
Now we use our fitted values from the logistic regression and the values we just predicted to calculate the gains table.
bank.gains<-gains(actual=bank.model$fitted.values,predicted=bank.pred,optimal=TRUE)
bank.gains

 
Depth of File	N	Cume N	Mean Resp	Cume Mean Resp	Cume Pct of Total Resp
10	2260	2260	0.04	0.04	9.00%
20	2261	4521	0.05	0.05	19.60%
30	2260	6781	0.04	0.05	29.00%
40	2261	9042	0.05	0.05	39.60%
50	2260	11302	0.05	0.05	49.50%
60	2262	13564	0.05	0.05	60.10%
70	2259	15823	0.04	0.05	69.30%
80	2261	18084	0.05	0.05	79.60%
90	2266	20350	0.05	0.05	89.90%
100	2255	22605	0.05	0.05	100.00%
					
Depth of File	Lift Index	Cume Lift	Optimal Lift Index	Optimal Cume Lift	Mean Model Score
10	90	90	720	720	0.38
20	106	98	102	411	0.07
30	93	97	50	291	0.04
40	107	99	34	226	0.03
50	99	99	26	186	0.02
60	106	100	21	159	0.02
70	92	99	17	138	0.01
80	103	99	14	123	0.01
90	103	100	10	110	0.01
100	101	100	7	100	0.01

 
Now we plot the "Mean Response", "Cumulative Mean Response", and "Mean Predicted Response" using the gains output.
plot(with(subset(bank,bank_train==0), bank.gains), main="Bank Gains Table Plot")
 
Figure 17-28. Gain plot for the Bank model
Create the following individual plots: Cumulative Percent of Total, Cumulative Mean Response, cumulative Lift, Optimal Cumulative Lift, and Mean Prediction.
plot(bank.gains$cume.mean.resp,type="l",main="Mean Response",,lwd=2)
 
Figure 17-29. Plot of the Mean Response from the Gains Table
plot(bank.gains$opt.cume.lift,type="l",main="Optimal Cummulative Lift",,lwd=2)
 
Figure 17-30. Plot of the Optimal Cumulative Lift from the Gains Table
plot(bank.gains$mean.prediction,type="l", lwd=2, main="Mean Predicted")
 
Figure 17-31. Plot of the Mean Predicted Response from the Gains Table

Logistic Regression: Multiple Numerical Predictors
Inattentional Blindness (IB) refers to situations in which a person fails to see an obvious stimulus right in front of his eyes. It is hypothesized that IB could be predicted from performance on the Stroop Color Word test. This test produces three scores: “W” (word alone, i.e., a score derived from reading a list of color words such as red, green, black), “C” (color alone, in which a score is derived from naming the color in which a series of Xs are printed), and “CW” (the Stroop task, in which a score is derived from the subject’s attempt to name the color in which a color word is printed when the word and the color do not agree). The data are in the following table, in which the response, “seen”, is coded as 0=no and 1=yes... 
   seen   W   C CW
1     0 126  86 64
2     0 118  76 54
3     0  61  66 44
4     0  69  48 32
5     0  57  59 42
6     0  78  64 53
7     0 114  61 41
8     0  81  85 47
9     0  73  57 33
10    0  93  50 45
11    0 116  92 49
12    0 156  70 45
13    0  90  66 48
14    0 120  73 49
15    0  99  68 44
16    0 113 110 47
17    0 103  78 52
18    0 123  61 28
19    0  86  65 42
20    0  99  77 51
21    0 102  77 54
22    0 120  74 53
23    0 128 100 56
24    0 100  89 56
25    0  95  61 37
26    0  80  55 36
27    0  98  92 51
28    0 111  90 52
29    0 101  85 45
30    0 102  78 51
31    1 100  66 48
32    1 112  78 55
33    1  82  84 37
34    1  72  63 46
35    1  72  65 47
36    1  89  71 49
37    1 108  46 29
38    1  88  70 49
39    1 116  83 67
40    1 100  69 39
41    1  99  70 43
42    1  93  63 36
43    1 100  93 62
44    1 110  76 56
45    1 100  83 36
46    1 106  71 49
47    1 115 112 66
48    1 120  87 54
49    1  97  82 41

To get them into R, try this first... 
> file = “http://ww2.coastal.edu/kingw/statistics/R-
+ tutorials/text/gorilla.csv”
> read.csv(file) -> gorilla
> str(gorilla)
‘data.frame’:   49 obs. of  4 variables:
 $ seen: int  0 0 0 0 0 0 0 0 0 0 ...
 $ W   : int  126 118 61 69 57 78 114 81 73 93 ...
 $ C   : int  86 76 66 48 59 64 61 85 57 50 ...
 $ CW  : int  64 54 44 32 42 53 41 47 33 45 ...

If that doesn’t work (and it should), try copying and pasting this script into R at the command prompt... 
### Begin copying here.
gorilla = data.frame(rep(c(0,1),c(30,19)),
            c(126,118,61,69,57,78,114,81,73,93,116,156,90,120,99,113,103,123,
              86,99,102,120,128,100,95,80,98,111,101,102,100,112,82,72,72,
              89,108,88,116,100,99,93,100,110,100,106,115,120,97),
            c(86,76,66,48,59,64,61,85,57,50,92,70,66,73,68,110,78,61,65,
              77,77,74,100,89,61,55,92,90,85,78,66,78,84,63,65,71,46,70,
              83,69,70,63,93,76,83,71,112,87,82),
            c(64,54,44,32,42,53,41,47,33,45,49,45,48,49,44,47,52,28,42,51,54,
              53,56,56,37,36,51,52,45,51,48,55,37,46,47,49,29,49,67,39,43,36,
              62,56,36,49,66,54,41))
colnames(gorilla) = c(“seen”,”W”,”C”,”CW”)
str(gorilla)
### End copying here.

And if that does not work, well, you know what you have to do! We might begin like this... 
> cor(gorilla)            ### a correlation matrix
            seen           W          C         CW
seen  1.00000000 -0.03922667 0.05437115 0.06300865
W    -0.03922667  1.00000000 0.43044418 0.35943580
C     0.05437115  0.43044418 1.00000000 0.64463361
CW    0.06300865  0.35943580 0.64463361 1.00000000

...or like this... 
> with(gorilla, tapply(W, seen, mean))
        0         1 
100.40000  98.89474 
> with(gorilla, tapply(C, seen, mean))
       0        1 
73.76667 75.36842 
> with(gorilla, tapply(CW, seen, mean))
       0        1 
46.70000 47.84211

The Stroop scale scores are moderately positively correlated with each other, but none of them appears to be related to the “seen” response variable, at least not to any impressive extent. There doesn’t appear to be much here to look at. Let’s have a go at it anyway.
Since the response is a binomial variable, a logistic regression can be done as follows... 
> glm.out = glm(seen ~ W * C * CW, family=binomial(logit), 
+ data=gorilla)
> summary(glm.out)

Call:
glm(formula = seen ~ W * C * CW, family = binomial(logit), data = gorilla)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8073  -0.9897  -0.5740   1.2368   1.7362  

Coefficients:
              Estimate Std. Error z value Pr(>|z|)  
(Intercept) -1.323e+02  8.037e+01  -1.646   0.0998 .
W            1.316e+00  7.514e-01   1.751   0.0799 .
C            2.129e+00  1.215e+00   1.753   0.0797 .
CW           2.206e+00  1.659e+00   1.329   0.1837  
W:C         -2.128e-02  1.140e-02  -1.866   0.0621 .
W:CW        -2.201e-02  1.530e-02  -1.439   0.1502  
C:CW        -3.582e-02  2.413e-02  -1.485   0.1376  
W:C:CW       3.579e-04  2.225e-04   1.608   0.1078  
---
Signif.codes:0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ‘ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 65.438  on 48  degrees of freedom
Residual deviance: 57.281  on 41  degrees of freedom
AIC: 73.281

Number of Fisher Scoring iterations: 5

> anova(glm.out, test=“Chisq”)
Analysis of Deviance Table

Model: binomial, link: logit

Response: seen

Terms added sequentially (first to last)

       Df Deviance Resid. Df Resid. Dev Pr(>Chi)  
NULL                      48     65.438           
W       1   0.0755        47     65.362  0.78351  
C       1   0.3099        46     65.052  0.57775  
CW      1   0.1061        45     64.946  0.74467  
W:C     1   2.3632        44     62.583  0.12423  
W:CW    1   0.5681        43     62.015  0.45103  
C:CW    1   1.4290        42     60.586  0.23193  
W:C:CW  1   3.3053        41     57.281  0.06906 .
---
Signif.codes:0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ‘ 1

Two different extractor functions have been used to see the results of our analysis. What do they mean?
The first gives us what amount to regression coefficients with standard errors and a z-test, as we saw in the single variable example above. None of the coefficients are significantly different from zero (but a few are close). The deviance was reduced by 8.157 points on 7 degrees of freedom, for a p-value of... 
> 1 - pchisq(8.157, df=7)
[1] 0.3189537

Overall, the model appears to have performed poorly, showing no significant reduction in deviance (no significant difference from the null model).
The second print out shows the same overall reduction in deviance, from 65.438 to 57.281 on 7 degrees of freedom. In this print out, however, the reduction in deviance is shown for each term, added sequentially first to last. Of note is the three-way interaction term, which produced a nearly significant reduction in deviance of 3.305 on 1 degree of freedom (p = 0.069). 
In the event you are encouraged by any of this, the following graph might be revealing... 
> plot(glm.out$fitted)
> abline(v=30.5,col=“red”)
> abline(h=.3,col=“green”)
> abline(h=.5,col=“green”)
> text(15,.9,”seen = 0”)
> text(40,.9,”seen = 1”)
 
Figure 17-32. Plot of the model with fitted values
We leave it up to you to interpret this.
Logistic Regression: Categorical Predictors
Categorical data are commonly encountered in three forms: a frequency table or cross-tabulation, a flat table, or a case-by-case data frame. Let’s begin with the last of these. Copy and paste the following lines ALL AT ONCE into R. That is, highlight these lines with your mouse, hit Ctrl-C on your keyboard, click at a command prompt in R, and hit Ctrl-V on your keyboard, and hit Enter if necessary, i.e., if R hasn’t returned to a command prompt. On the Mac, use Command-C and Command-V. This will execute these lines as a script and create a data frame called “ucb” in your workspace. WARNING: Your workspace will also be cleared, so save anything you don’t want to lose first. 
# Begin copying here.
rm(list=ls())
gender = rep(c(“female”,”male”),c(1835,2691))
admitted = rep(c(“yes”,”no”,”yes”,”no”),c(557,1278,1198,1493))
dept = rep(c(“A”,”B”,”C”,”D”,”E”,”F”,”A”,”B”,”C”,”D”,”E”,”F”),
           c(89,17,202,131,94,24,19,8,391,244,299,317))
dept2 = rep(c(“A”,”B”,”C”,”D”,”E”,”F”,”A”,”B”,”C”,”D”,”E”,”F”),
            c(512,353,120,138,53,22,313,207,205,279,138,351))
department = c(dept,dept2)
ucb = data.frame(gender,admitted,department)
rm(gender,admitted,dept,dept2,department)
ls()
# End copying here.
[1] “ucb”

Data sets that are purely categorical are not economically represented in case-by-case data frames, and so the built-in data sets that are purely categorical come in the form of tables (contingency tables or crosstabulations). We have just taken the data from one of these (the “UCBAdmissions” built-in data set) and turned it into a case-by-case data frame. It’s the classic University of California, Berkeley, admissions data from 1973 describing admissions into six different graduate programs broken down by gender. Let’s examine the “UCBAdmissions” data set.
> ftable(UCBAdmissions, col.vars=“Admit”)
            Admit Admitted Rejected
Gender Dept                        
Male   A               512      313
       B               353      207
       C               120      205
       D               138      279
       E                53      138
       F                22      351
Female A                89       19
       B                17        8
       C               202      391
       D               131      244
       E                94      299
       F                24      317

The data are from 1973 and show admissions by gender to the top six grad programs at the University of California, Berkeley. Looked at as a two-way table, there appears to be a bias against admitting women... 
> dimnames(UCBAdmissions)
$Admit
[1] “Admitted” “Rejected”

$Gender
[1] “Male”   “Female”

$Dept
[1] “A” “B” “C” “D” “E” “F”

> margin.table(UCBAdmissions, c(2,1))
        Admit
Gender   Admitted Rejected
  Male       1198     1493
  Female      557     1278

However, there are also relationships between “Gender” and “Dept” as well as between “Dept” and “Admit”, which means the above relationship may be confounded by “Dept” (or “Dept” might be a lurking variable, in the language of traditional regression analysis). Perhaps a logistic regression with the binomial variable “Admit” as the response can tease these variables apart.
If there is a way to conveniently get that flat table into a data frame (without splitting an infinitive), I do not know it. So I had to do this... 
> ucb.df =
+ data.frame(gender=rep(c(“Male”,”Female”),c(6,6)),
+ dept=rep(LETTERS[1:6],2),
+ yes=c(512,353,120,138,53,22,89,17,202,131,94,24),
+ no=c(313,207,205,279,138,351,19,8,391,244,299,317))
> ucb.df
   gender dept yes  no
1    Male    A 512 313
2    Male    B 353 207
3    Male    C 120 205
4    Male    D 138 279
5    Male    E  53 138
6    Male    F  22 351
7  Female    A  89  19
8  Female    B  17   8
9  Female    C 202 391
10 Female    D 131 244
11 Female    E  94 299
12 Female    F  24 317

Once again, we do not have a binary coded response variable, so the last two columns of this data frame will have to be bound into the columns of a table to serve as the response in the model formula... 
> mod.form = “cbind(yes,no) ~ gender * dept”
> glm.out = glm(mod.form, family=binomial(logit), 
+ data=ucb.df)

We used a trick here of storing the model formula in a data object, and then entering the name of this object into the glm( ) function. That way, if we made a mistake in the model formula (or want to run an alternative model), we have only to edit the “mod.form” object to do it.
Let’s see what we have found... 
> options(show.signif.stars=F)  # turn off significance 
                                # stars (optional)
> anova(glm.out, test=“Chisq”)
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(yes, no)

Terms added sequentially (first to last)


            Df Deviance Resid. Df Resid. Dev  Pr(>Chi)
NULL                           11     877.06          
gender       1    93.45        10     783.61 < 2.2e-16
dept         5   763.40         5      20.20 < 2.2e-16
gender:dept  5    20.20         0       0.00  0.001144

This is a saturated model, meaning we have used up all our degrees of freedom, and there is no residual deviance left over at the end. Saturated models always fit the data perfectly. In this case, it appears the saturated model is required to explain the data adequately. If we leave off the interaction term, for example, we will be left with a residual deviance of 20.2 on 5 degrees of freedom, and the model will be rejected (p=0.001144). It appears all three terms are making a significant contribution to the model.
How they are contributing appears if we use the other extractor... 
> summary(glm.out)

Call:
glm(formula = mod.form, family = binomial(logit), data = ucb.df)

Deviance Residuals: 
 [1]  0  0  0  0  0  0  0  0  0  0  0  0

Coefficients:
                 Estimate Std. Error z value Pr(>|z|)
(Intercept)        1.5442     0.2527   6.110 9.94e-10
genderMale        -1.0521     0.2627  -4.005 6.21e-05
deptB             -0.7904     0.4977  -1.588  0.11224
deptC             -2.2046     0.2672  -8.252  < 2e-16
deptD             -2.1662     0.2750  -7.878 3.32e-15
deptE             -2.7013     0.2790  -9.682  < 2e-16
deptF             -4.1250     0.3297 -12.512  < 2e-16
genderMale:deptB   0.8321     0.5104   1.630  0.10306
genderMale:deptC   1.1770     0.2996   3.929 8.53e-05
genderMale:deptD   0.9701     0.3026   3.206  0.00135
genderMale:deptE   1.2523     0.3303   3.791  0.00015
genderMale:deptF   0.8632     0.4027   2.144  0.03206

(Dispersion parameter for binomial family taken to be 1)

    Null deviance:  8.7706e+02  on 11  degrees of freedom
Residual deviance: -1.6676e-13  on  0  degrees of freedom
AIC: 92.94

Number of Fisher Scoring iterations: 3

These are the regression coefficients for each predictor in the model, with the base level of each factor being suppressed. Remember, we are predicting log odd”... 
> exp(-1.0521)      # antilog of the genderMale coefficient
[1] 0.3492037
> 1/exp(-1.0521)
[1] 2.863658

This shows that men were actually at a significant disadvantage when department and the interaction are controlled. The odds of a male being admitted were only 0.35 times the odds of a female being admitted. The reciprocal of this turns it on its head. All else being equal, the odds of female being admitted were 2.86 times the odds of a male being admitted.
Each coefficient compares the corresponding predictor to the base level. So... 
> exp(-2.2046)
[1] 0.1102946

...the odds of being admitted to department C were only about 1/9th the odds of being admitted to department A, all else being equal. If you want to compare, for example, department C to department D, do this... 
> exp(-2.2046) / exp(-2.1662)        # C:A / D:A leaves C:D
[1] 0.9623279

All else equal, the odds of being admitted to department C were 0.96 times the odds of being admitted to department D. (To be honest, I am not sure I am comfortable with the interaction in this model. You might want to examine the interaction, and if you think it doesn’t merit inclusion, run the model again without it. Statistics are nice, but in the end it’s what makes sense that should rule the day.)
Example using KNIME
Here I use a different dataset because I am using the R interface in KNIME. The dataset contain patients with a factor for rheumatoid arthritis, near and dear to my heart since I have the factor and the disease.
 
Figure 17-33. KNIME model view of the logistics regresion model
file:/C:/Users/Strickland/Documents/10%20Analytics/arthritis.csv 
 
Figure 17-34. KNIME settings window for the data import (File Reader)
 
 Figure 17-35. KNIME settings window for the Column Rename 
Training data
knime.out <- knime.in[1:500,] 
Prediction data
knime.out <- knime.in[501:906,]
Classification Tree
library(stats)
knime.model <- glm(knime.in$"resp" ~ knime.in$"r-factor" + knime.in$"sex" + knime.in$"age" + knime.in$"baseline" + knime.in$"time" + knime.in$"trt", data = knime.in, family=binomial) 
Tree plot
plot(knime.model) 
Classification Tree
knime.out<-cbind(knime.in, predict(knime.model,knime.in,)) 
Cumulative statistics
Some examples for how to generate cumulative metrics for a column of data.
# Reference a column in your table here.
column = knime.in$"age"
#Add the cumulative sum of all values in your column
knime.in$"cumsum" <- cumsum(column)	
#The cumulative product of all values in your column
knime.in$"cumprod" <- cumprod(column)	
# Add the min for all values in your column from the start of the vector up to the current position
knime.in$"cummin" <- cummin(column)
# Add the max for all values in your column from the start of the vector up to the current position
knime.in$"cummax" <- cummax(column)
knime.out <- knime.in 
Matrix plot
R <- knime.in
pairs(R[1:2], labels = c(knime.in$"age", knime.in$"g2"))
pairs(R) 
Scatter Plot
This example relies on ggplot2 and grid, if these packages are not part of your R installation, please add them!
require(ggplot2)
require(grid)
# Insert column references here. 
# Note: variable names are used as plot labels ("x" will be the x axis label)
x = knime.in$"age"#<Choose column from Column List>
y = knime.in$"g2"#<Choose column from Column List>
#Column to color by:
class = knime.in$"ploidy"#<Choose column from Column List>
# Use a flow variable for a title
title = "Stage C Prostate Cancer"#<Choose variable from Flow Variable List or set manually>
# define a plot theme
# http://docs.ggplot2.org/0.9.2.1/theme.html for more options
clean_theme = theme(panel.background = element_blank(),
plot.title = element_text(size=20, face="bold", colour = "black"),
panel.border = element_rect(color = "black", linetype = "solid", fill = "transparent"),
axis.title.x = element_text(size=14, face="italic", colour = "black"),
axis.title = element_text(size=14, face="italic", colour = "black"),
axis.text = element_text(size=12, face="italic", colour = "black"),
legend.text = element_text(size=12, face="italic", colour = "black"),
panel.grid = element_blank()
)
# Generate a plot and apply the theme
qplot('time', 'resp', color = 'blue', main = "title") + clean_theme	 
 
Figure 17-36. KNIME window for Lift Chart Data
A lift chart is used to evaluate a predictive model. The higher the lift (the difference between the "lift" line and the base line), the better performs the predictive model. The lift is the ratio between the results obtained with and without the predictive model. It is calculated as number of positive hits (e .g. responses) divided by the average number of positives without model. The data table must have a column containing probabilities and a nominal column, containing the actual labels. At first, the data is sorted by probability, divided into deciles, then the actual labels are counted and the average rate is calculated.
 
Figure 17-37. KNIME window for Lift Chart Plot 
?
 
Figure 17-38. KNIME window for the Tree Plot 

https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip 
 
Figure 17-39. KNIME window for Test Gains Table Plot 
?
Exercises
	Download the Titanic survivor train and test data from https://www.kaggle.com/c/titanic/data, and use it to construct and test a logistic regression model with survival as the target outcome, where survival = 1 (yes). How does this model compare to your random forest model from Chapter 10?
Contraceptive use data, showing the distribution of 1607 currently married and fecund women interviewed in the Fiji Fertility Survey, according to age, education, desire for more children and current use of contraception. This version has 32 rows corresponding to all possible covariate and response patterns, and includes a weight indicating the frequency of each combination. Download at http://data.princeton.edu/wws509/datasets/cuse.raw. The file has 5 columns with numeric codes:
	age (four groups, 1=<25, 2=25-29, 3=30-39 and 4=40-49),
	education (0=none, 1=some),
	desire for more children (0=more, 1=no more),
	contraceptive use (0=no, 1=yes), and
	frequency (number of cases in this category).
	Fit a GLM to predict contraceptive use based on the remaining four variables
?
?
Analysis of variance
Analysis of variance (ANOVA) is a collection of statistical models used to analyze the differences between group means and their associated procedures (such as “variation” among and between groups), developed by R.A. Fisher. In the ANOVA setting, the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the t-test to more than two groups. As doing multiple two-sample t-tests would result in an increased chance of committing a statistical type I error, ANOVAs are useful in comparing (testing) three or more means (groups or variables) for statistical significance.
Motivating example
The analysis of variance can be used as an exploratory tool to explain observations. A dog show provides an example. A dog show is not a random sampling of the breed: it is typically limited to dogs that are male, adult, pure-bred and exemplary. A histogram of dog weights from a show might plausibly be rather complex, like the yellow-orange distribution shown in the illustrations. Suppose we wanted to predict the weight of a dog based on a certain set of characteristics of each dog. Before we could do that, we would need to explain the distribution of weights by dividing the dog population into groups based on those characteristics. A successful grouping will split dogs such that a) each group has a low variance of dog weights (meaning the group is relatively homogeneous) and b) the mean of each group is distinct (if two groups have the same mean, then it isn’t reasonable to conclude that the groups are, in fact, separate in any meaningful way).
In the Figure 18-1 below, each group is identified as X_1 , X_2 , etc. In the first illustration, we divide the dogs according to the product (interaction) of two binary groupings: young vs old, and short-haired vs long-haired (thus, group 1 is young, short-haired dogs, group 2 is young, long-haired dogs, etc.). Since the distributions of dog weight within each of the groups has a large variance, and since the means are very close across groups, grouping dogs by these characteristics does not produce an effective way to explain the variation in dog weights: knowing which group a dog is in does not allow us to make any reasonable statements as to what that dog’s weight is likely to be. Thus, this grouping fails to fit the distribution we are trying to explain.
 
Figure 18-1. No fit.
An attempt to explain the weight distribution by grouping dogs as (pet vs working breed) and (less athletic vs more athletic) would probably be somewhat more successful (fair fit)as in Figure 18-2. The heaviest show dogs are likely to be big strong working breeds, while breeds kept as pets tend to be smaller and thus lighter. As shown by Figure 18-3, the distributions have variances that are considerably smaller than in the first case, and the means are more reasonably distinguishable.
 
Figure18-2. Fair fit
 
Figure 18-3. Very good fit
However, the significant overlap of distributions, for example, means that we cannot reliably say that X_1 and X_2 are truly distinct (i.e., it is perhaps reasonably likely that splitting dogs according to the flip of a coin—by pure chance—might produce distributions that look similar).
An attempt to explain weight by breed is likely to produce a very good fit. All Chihuahuas are light and all St. Bernards are heavy. The difference in weights between Setters and Pointers does not justify separate breeds. The analysis of variance provides the formal tools to justify these intuitive judgments. A common use of the method is the analysis of experimental data or the development of models. The method has some advantages over correlation: not all of the data must be numeric and one result of the method is a judgment in the confidence in an explanatory relationship.
Background and terminology
ANOVA is a particular form of statistical hypothesis testing heavily used in the analysis of experimental data. A statistical hypothesis test is a method of making decisions using data. A test result (calculated from the null hypothesis and the sample) is called statistically significant if it is deemed unlikely to have occurred by chance, assuming the truth of the null hypothesis. A statistically significant result, when a probability (p-value) is less than a threshold (significance level), justifies the rejection of the null hypothesis, but only if the a priori probability of the null hypothesis is not high.
In the typical application of ANOVA, the null hypothesis is that all groups are simply random samples of the same population. For example, when studying the effect of different treatments on similar samples of patients, the null hypothesis would be that all treatments have the same effect (perhaps none). Rejecting the null hypothesis would imply that different treatments result in altered effects.
By construction, hypothesis testing limits the rate of Type I errors (false positives leading to false scientific claims) to a significance level. Experimenters also wish to limit Type-II errors (false negatives resulting in missed scientific discoveries). The Type II error rate is a function of several things including sample size (positively correlated with experiment cost), significance level (when the standard of proof is high, the chances of overlooking a discovery are also high) and effect size (when the effect is obvious to the casual observer, Type II error rates are low).
The terminology of ANOVA is largely from the statistical design of experiments. The experimenter adjusts factors and measures responses in an attempt to determine an effect. Factors are assigned to experimental units by a combination of randomization and blocking to ensure the validity of the results. Blinding keeps the weighing impartial. Responses show a variability that is partially the result of the effect and is partially random error.
ANOVA is the synthesis of several ideas and it is used for multiple purposes. As a consequence, it is difficult to define concisely or precisely.
Classical ANOVA for balanced data does three things at once:
1. As exploratory data analysis, an ANOVA is an organization of an additive data decomposition, and its sums of squares indicate the variance of each component of the decomposition (or, equivalently, each set of terms of a linear model).
2. Comparisons of mean squares, along with F-tests ... allow testing of a nested sequence of models.
3. Closely related to the ANOVA is a linear model fit with coefficient estimates and standard errors (Gelman, 2005).
In short, ANOVA is a statistical tool used in several ways to develop and confirm an explanation for the observed data.
Additionally:
1. It is computationally elegant and relatively robust against violations of its assumptions.
2. ANOVA provides industrial strength (multiple sample comparison) statistical analysis.
3. It has been adapted to the analysis of a variety of experimental designs.
As a result: ANOVA “has long enjoyed the status of being the most used (some would say abused) statistical technique in psychological research.” (Howell, 2002) ANOVA “is probably the most useful technique in the field of statistical inference.” (Montgomery, 2001)
ANOVA is difficult to teach, particularly for complex experiments, with split-plot designs being notorious (Gelman, 2005). In some cases the proper application of the method is best determined by problem pattern recognition followed by the consultation of a classic authoritative test (Gelman, 2005).
Design-of-experiments terms
(Condensed from the NIST Engineering Statistics handbook: Section 5.7. A Glossary of DOE Terminology.) (NIST, 2012)
Balanced design
An experimental design where all cells (i.e. treatment combinations) have the same number of observations. 
Blocking
A schedule for conducting treatment combinations in an experimental study such that any effects on the experimental results due to a known change in raw materials, operators, machines, etc., become concentrated in the levels of the blocking variable. The reason for blocking is to isolate a systematic effect and prevent it from obscuring the main effects. Blocking is achieved by restricting randomization.
Design
A set of experimental runs which allows the fit of a particular model and the estimate of effects.
DOE
Design of experiments. An approach to problem solving involving collection of data that will support valid, defensible, and supportable conclusions (NIST, 2012).
Effect
How changing the settings of a factor changes the response. The effect of a single factor is also called a main effect.
Error
Unexplained variation in a collection of observations. DOE’s typically require understanding of both random error and lack of fit error.
Experimental unit
The entity to which a specific treatment combination is applied. 
Factors
Process inputs an investigator manipulates to cause a change in the output. 
Lack-of-fit error
Error that occurs when the analysis omits one or more important terms or factors from the process model. Including replication in a DOE allows separation of experimental error into its components: lack of fit and random (pure) error.
Model
Mathematical relationship which relates changes in a given response to changes in one or more factors. 
Random error
Error that occurs due to natural variation in the process. Random error is typically assumed to be normally distributed with zero mean and a constant variance. Random error is also called experimental error.
Randomization
A schedule for allocating treatment material and for conducting treatment combinations in a DOE such that the conditions in one run neither depend on the conditions of the previous run nor predict the conditions in the subsequent runs.
Replication
Performing the same treatment combination more than once. Including replication allows an estimate of the random error independent of any lack of fit error.
Responses
The output(s) of a process. Sometimes called dependent variable(s). 
Treatment
A treatment is a specific combination of factor levels whose effect is to be compared with other treatments.
Classes of models
There are three classes of models used in the analysis of variance, and these are outlined here.
Fixed-effects models
The fixed-effects model of analysis of variance applies to situations in which the experimenter applies one or more treatments to the subjects of the experiment to see if the response variable values change. This allows the experimenter to estimate the ranges of response variable values that the treatment would generate in the population as a whole.
Random-effects models
Random effects models are used when the treatments are not fixed. This occurs when the various factor levels are sampled from a larger population. Because the levels themselves are random variables, some assumptions and the method of contrasting the treatments (a multi-variable generalization of simple differences) differ from the fixed-effects model (Montgomery, 2001).
Mixed-effects models
A mixed-effects model contains experimental factors of both fixed and random-effects types, with appropriately different interpretations and analysis for the two types.
Example: Teaching experiments could be performed by a university department to find a good introductory textbook, with each text considered a treatment. The fixed-effects model would compare a list of candidate texts. The random-effects model would determine whether important differences exist among a list of randomly selected texts. The mixed-effects model would compare the (fixed) incumbent texts to randomly selected alternatives.
Defining fixed and random effects has proven elusive, with competing definitions arguably leading toward a linguistic quagmire (Gelman, 2005).
Assumptions of ANOVA
The analysis of variance has been studied from several approaches, the most common of which uses a linear model that relates the response to the treatments and blocks. Note that the model is linear in parameters but may be nonlinear across factor levels. Interpretation is easy when data is balanced across factors but much deeper understanding is needed for unbalanced data.
Textbook analysis using a normal distribution
The analysis of variance can be presented in terms of a linear model, which makes the following assumptions about the probability distribution of the responses (Snedecor & Cochran, 1989) (Cochran & Cox, 1992):
Independence of observations – this is an assumption of the model that simplifies the statistical analysis.
Normality – the distributions of the residuals are normal.
Equality (or “homogeneity”) of variances, called homoscedasticity — the variance of data in groups should be the same.
The separate assumptions of the textbook model imply that the errors are independently, identically, and normally distributed for fixed effects models, that is, that the errors (?’s) are independent and ?~N(0,?^2 ).
Randomization-based analysis
In a randomized controlled experiment, the treatments are randomly assigned to experimental units, following the experimental protocol. This randomization is objective and declared before the experiment is carried out. The objective random-assignment is used to test the significance of the null hypothesis, following the ideas of C. S. Peirce (Peirce, 1992) and Ronald A. Fisher (Fisher & Prance, 1974). This design-based analysis was discussed and developed by Francis J. Anscombe at Rothamsted Experimental Station and by Oscar Kempthorne (Hinkelmann & Kempthorne, 2008) at Iowa State University (Anscombe, 1948). Kempthorne and his students make an assumption of unit treatment additivity, which is discussed in the books of Kempthorne and David R. Cox (Cox, 1992).
Unit-treatment additivity
In its simplest form, the assumption of unit-treatment additivity[ states that the observed response y_(i,j) from experimental unit i when receiving treatment j can be written as the sum of the unit’s response  and the treatment-effect t_j, that is (Kempthorne, 1979) (Cox, 1992) (Hinkelmann & Kempthorne, 2008)
y_(i,j)=y_i+t_j.
The assumption of unit-treatment additivity implies that, for every treatment j, the jth treatment have exactly the same effect t_j on every experiment unit.
The assumption of unit treatment additivity usually cannot be directly falsified, according to Cox and Kempthorne (Cox, 1992) (Kempthorne, 1979). However, many consequences of treatment-unit additivity can be falsified. For a randomized experiment, the assumption of unit-treatment additivity implies that the variance is constant for all treatments. Therefore, by contraposition, a necessary condition for unit-treatment additivity is that the variance is constant.
The use of unit treatment additivity and randomization is similar to the design-based inference that is standard in finite-population survey sampling.
Derived linear model
Kempthorne uses the randomization-distribution and the assumption of unit-treatment additivity to produce a derived linear model, very similar to the textbook model discussed previously (Hinkelmann & Kempthorne, 2008).] The test statistics of this derived linear model are closely approximated by the test statistics of an appropriate normal linear model, according to approximation theorems and simulation studies (Hinkelmann & Kempthorne, 2008). However, there are differences. For example, the randomization-based analysis results in a small but (strictly) negative correlation between the observations (Bailey, 2008) (Hinkelmann & Kempthorne, 2008). In the randomization-based analysis, there is no assumption of a normal distribution and certainly no assumption of independence. On the contrary, the observations are dependent.
The randomization-based analysis has the disadvantage that its exposition involves tedious algebra and extensive time. Since the randomization-based analysis is complicated and is closely approximated by the approach using a normal linear model, most teachers emphasize the normal linear model approach. Few statisticians object to model-based analysis of balanced randomized experiments.
Statistical models for observational data
However, when applied to data from non-randomized experiments or observational studies, model-based analysis lacks the warrant of randomization (Kempthorne, 1979). For observational data, the derivation of confidence intervals must use subjective models, as emphasized by Ronald A. Fisher and his followers. In practice, the estimates of treatment-effects from observational studies generally are often inconsistent. In practice, “statistical models” and observational data are useful for suggesting hypotheses that should be treated very cautiously by the public (Freedman, 2005).
Summary of assumptions
The normal-model based ANOVA analysis assumes the independence, normality and homogeneity of the variances of the residuals. The randomization-based analysis assumes only the homogeneity of the variances of the residuals (as a consequence of unit-treatment additivity) and uses the randomization procedure of the experiment. Both these analyses require homoscedasticity, as an assumption for the normal-model analysis and as a consequence of randomization and additivity for the randomization-based analysis.
However, studies of processes that change variances rather than means (called dispersion effects) have been successfully conducted using ANOVA (Montgomery, 2001). There are no necessary assumptions for ANOVA in its full generality, but the F-test used for ANOVA hypothesis testing has assumptions and practical limitations which are of continuing interest.
Problems which do not satisfy the assumptions of ANOVA can often be transformed to satisfy the assumptions. The property of unit-treatment additivity is not invariant under a “change of scale”, so statisticians often use transformations to achieve unit-treatment additivity. If the response variable is expected to follow a parametric family of probability distributions, then the statistician may specify (in the protocol for the experiment or observational study) that the responses be transformed to stabilize the variance (Hinkelmann & Kempthorne, 2008). Also, a statistician may specify that logarithmic transforms be applied to the responses, which are believed to follow a multiplicative model (Cox, 1992) (Bailey, 2008). According to Cauchy’s functional equation theorem, the logarithm is the only continuous transformation that transforms real multiplication to addition. 
Characteristics of ANOVA
ANOVA is used in the analysis of comparative experiments, those in which only the difference in outcomes is of interest. The statistical significance of the experiment is determined by a ratio of two variances. This ratio is independent of several possible alterations to the experimental observations: Adding a constant to all observations does not alter significance. Multiplying all observations by a constant does not alter significance. So ANOVA statistical significance results are independent of constant bias and scaling errors as well as the units used in expressing observations. In the era of mechanical calculation it was common to subtract a constant from all observations (when equivalent to dropping leading digits) to simplify data entry (Montgomery, 2001) (Cochran & Cox, 1992). This is an example of data coding.
Logic of ANOVA
The calculations of ANOVA can be characterized as computing a number of means and variances, dividing two variances and comparing the ratio to a handbook value to determine statistical significance. Calculating a treatment effect is then trivial, “the effect of any treatment is estimated by taking the difference between the mean of the observations which receive the treatment and the general mean.” (Cochran & Cox, 1992)
Partitioning of the sum of squares
ANOVA uses traditional standardized terminology. The definitional equation of sample variance is s^2=1/(n-q) ??(y_i-y ? )^2 , where the divisor is called the degrees of freedom (DF), the summation is called the sum of squares (SS), the result is called the mean square (MS) and the squared terms are deviations from the sample mean. ANOVA estimates 3 sample variances: a total variance based on all the observation deviations from the grand mean, an error variance based on all the observation deviations from their appropriate treatment means and a treatment variance. The treatment variance is based on the deviations of treatment means from the grand mean, the result being multiplied by the number of observations in each treatment to account for the difference between the variance of observations and the variance of means.
The fundamental technique is a partitioning of the total sum of squares SS into components related to the effects used in the model. For example, the model for a simplified ANOVA with one type of treatment at different levels:
?SS?_"Total" =?SS?_"Error" +?SS?_"Treatments" .
The number of degrees of freedom DF can be partitioned in a similar way: one of these components (that for error) specifies a chi-squared distribution which describes the associated sum of squares, while the same is true for “treatments” if there is no treatment effect:
?DF?_"Total" =?DF?_"Error" +?DF?_"Treatments" .
The F-test
The F-test is used for comparing the factors of the total deviation. For example, in one-way, or single-factor ANOVA, statistical significance is tested for by comparing the F test statistic
F=variance between treatments/variance within treatments
F=?MS?_"Treatments" /?MS?_"Error"  =(?SS?_"Treatments" ?((I-1) ))/(?SS?_"Error" ?((n_T-I) ))
where MS is mean square, I = number of treatments and n_T = total number of cases to the F-distribution with I-1, n_T-I degrees of freedom. Using the F-distribution is a natural candidate because the test statistic is the ratio of two scaled sums of squares each of which follows a scaled chi-squared distribution.
The expected value of F is (1+?n??_"Treatment" ^2)?(?_"Error" ^2 ) (where n is the treatment sample size) which is 1 for no treatment effect. As values of F increase above 1, the evidence is increasingly inconsistent with the null hypothesis. Two apparent experimental methods of increasing F are increasing the sample size and reducing the error variance by tight experimental controls.
There are two methods of concluding the ANOVA hypothesis test, both of which produce the same result:
The textbook method is to compare the observed value of F with the critical value of F determined from tables. The critical value of F is a function of the degrees of freedom of the numerator and the denominator and the significance level (?). If F?F_"Critical" , the null hypothesis is rejected.
The computer method calculates the probability (p-value) of a value of F greater than or equal to the observed value. The null hypothesis is rejected if this probability is less than or equal to the significance level (?). 
The ANOVA F-test is known to be nearly optimal in the sense of minimizing false negative errors for a fixed rate of false positive errors (i.e. maximizing power for a fixed significance level). For example, to test the hypothesis that various medical treatments have exactly the same effect, the F-test’s p-values closely approximate the permutation test’s p-values: The approximation is particularly close when the design is balanced (Hinkelmann & Kempthorne, 2008). Such permutation tests characterize tests with maximum power against all alternative hypotheses, as observed by Rosenbaum (Rosenbaum, 2002). The ANOVA F–test (of the null-hypothesis that all treatments have exactly the same effect) is recommended as a practical test, because of its robustness against many alternative distributions (Moore & McCabe, 2003).
Extended logic
ANOVA consists of separable parts; partitioning sources of variance and hypothesis testing can be used individually. ANOVA is used to support other statistical tools. Regression is first used to fit more complex models to data, then ANOVA is used to compare models with the objective of selecting simple(r) models that adequately describe the data. “Such models could be fit without any reference to ANOVA, but ANOVA tools could then be used to make some sense of the fitted models, and to test hypotheses about batches of coefficients.” (Gelman, 2005) “[W]e think of the analysis of variance as a way of understanding and structuring multilevel models—not as an alternative to regression but as a tool for summarizing complex high-dimensional inferences ...” (Gelman, 2005)
ANOVA for a single factor
The simplest experiment suitable for ANOVA analysis is the completely randomized experiment with a single factor. More complex experiments with a single factor involve constraints on randomization and include completely randomized blocks and Latin squares (and variants: Graeco-Latin squares, etc.). The more complex experiments share many of the complexities of multiple factors. A relatively complete discussion of the analysis (models, data summaries, ANOVA table) of the completely randomized experiment is available.
ANOVA for multiple factors
ANOVA generalizes to the study of the effects of multiple factors. When the experiment includes observations at all combinations of levels of each factor, it is termed factorial. Factorial experiments are more efficient than a series of single factor experiments and the efficiency grows as the number of factors increases (Montgomery, 2001). Consequently, factorial designs are heavily used.
The use of ANOVA to study the effects of multiple factors has a complication. In a 3-way ANOVA with factors x, y and z, the ANOVA model includes terms for the main effects (x,y,z) and terms for interactions (xy,xz,yz,xyz). All terms require hypothesis tests. The proliferation of interaction terms increases the risk that some hypothesis test will produce a false positive by chance. Fortunately, experience says that high order interactions are rare (Belle, 2008). The ability to detect interactions is a major advantage of multiple factor ANOVA. Testing one factor at a time hides interactions, but produces apparently inconsistent experimental results (Montgomery, 2001).
Caution is advised when encountering interactions; Test interaction terms first and expand the analysis beyond ANOVA if interactions are found. Texts vary in their recommendations regarding the continuation of the ANOVA procedure after encountering an interaction. Interactions complicate the interpretation of experimental data. Neither the calculations of significance nor the estimated treatment effects can be taken at face value. “A significant interaction will often mask the significance of main effects.” (Montgomery, 2001) Graphical methods are recommended to enhance understanding. Regression is often useful. A lengthy discussion of interactions is available in Cox (1958) (Cox, 1992). Some interactions can be removed (by transformations) while others cannot.
A variety of techniques are used with multiple factor ANOVA to reduce expense. One technique used in factorial designs is to minimize replication (possibly no replication with support of analytical trickery) and to combine groups when effects are found to be statistically (or practically) insignificant. An experiment with many insignificant factors may collapse into one with a few factors supported by many replications (Montgomery, 2001).
Worked numeric examples
Several fully worked numerical examples are available in Neter, J., et al, Applied Linear Statistical Models. A simple case uses one-way (a single factor) analysis. A more complex case uses two-way (two-factor) analysis.
Associated analysis
Some analysis is required in support of the design of the experiment while other analysis is performed after changes in the factors are formally found to produce statistically significant changes in the responses. Because experimentation is iterative, the results of one experiment alter plans for following experiments.
Preparatory analysis
The number of experimental units
In the design of an experiment, the number of experimental units is planned to satisfy the goals of the experiment. Experimentation is often sequential.
Early experiments are often designed to provide mean-unbiased estimates of treatment effects and of experimental error. Later experiments are often designed to test a hypothesis that a treatment effect has an important magnitude; in this case, the number of experimental units is chosen so that the experiment is within budget and has adequate power, among other goals.
Reporting sample size analysis is generally required in psychology. “Provide information on sample size and the process that led to sample size decisions.” (Wilkinson, 1999) The analysis, which is written in the experimental protocol before the experiment is conducted, is examined in grant applications and administrative review boards.
Besides the power analysis, there are less formal methods for selecting the number of experimental units. These include graphical methods based on limiting the probability of false negative errors, graphical methods based on an expected variation increase (above the residuals) and methods based on achieving a desired confident interval (Montgomery, 2001).
Power analysis
Power analysis is often applied in the context of ANOVA in order to assess the probability of successfully rejecting the null hypothesis if we assume a certain ANOVA design, effect size in the population, sample size and significance level. Power analysis can assist in study design by determining what sample size would be required in order to have a reasonable chance of rejecting the null hypothesis when the alternative hypothesis is true (Moore & McCabe, 2003) (Howell, 2002).
Effect size
Several standardized measures of effect have been proposed for ANOVA to summarize the strength of the association between a predictor(s) and the dependent variable (e.g., ?^2, ?^2, or ƒ^2) or the overall standardized difference (?) of the complete model. Standardized effect-size estimates facilitate comparison of findings across studies and disciplines. However, while standardized effect sizes are commonly used in much of the professional literature, a non-standardized measure of effect size that has immediately “meaningful” units may be preferable for reporting purposes (Wilkinson, 1999).
Followup analysis
It is always appropriate to carefully consider outliers. They have a disproportionate impact on statistical conclusions and are often the result of errors.
Model confirmation
It is prudent to verify that the assumptions of ANOVA have been met. Residuals are examined or analyzed to confirm homoscedasticity and gross normality (Montgomery, 2001). Residuals should have the appearance of (zero mean normal distribution) noise when plotted as a function of anything including time and modeled data values. Trends hint at interactions among factors or among observations. One rule of thumb: “If the largest standard deviation is less than twice the smallest standard deviation, we can use methods based on the assumption of equal standard deviations and our results will still be approximately correct.” (Moore & McCabe, 2003)
Follow-up tests
A statistically significant effect in ANOVA is often followed up with one or more different follow-up tests. This can be done in order to assess which groups are different from which other groups or to test various other focused hypotheses. Follow-up tests are often distinguished in terms of whether they are planned (a priori) or post hoc. Planned tests are determined before looking at the data and post hoc tests are performed after looking at the data.
Often one of the “treatments” is none, so the treatment group can act as a control. Dunnett’s test (a modification of the t-test) tests whether each of the other treatment groups has the same mean as the control (Montgomery, 2001).
Post hoc tests such as Tukey’s range test most commonly compare every group mean with every other group mean and typically incorporate some method of controlling for Type I errors. Comparisons, which are most commonly planned, can be either simple or compound. Simple comparisons compare one group mean with one other group mean. Compound comparisons typically compare two sets of group means where one set has two or more groups (e.g., compare average group means of group A, B and C with group D). Comparisons can also look at tests of trend, such as linear and quadratic relationships, when the independent variable involves ordered levels.
Following ANOVA with pair-wise multiple-comparison tests has been criticized on several grounds (Hinkelmann & Kempthorne, 2008) (Wilkinson, 1999). There are many such tests (10 in one table) and recommendations regarding their use are vague or conflicting (Howell, 2002) (Montgomery, 2001).
Study designs and ANOVAs
There are several types of ANOVA. Many statisticians base ANOVA on the design of the experiment (Cochran & Cox, 1992), especially on the protocol that specifies the random assignment of treatments to subjects; the protocol’s description of the assignment mechanism should include a specification of the structure of the treatments and of any blocking. It is also common to apply ANOVA to observational data using an appropriate statistical model.
Some popular designs use the following types of ANOVA:
One-way ANOVA is used to test for differences among two or more independent groups (means), e.g., different levels of urea application in a crop. Typically, however, the one-way ANOVA is used to test for differences among at least three groups, since the two-group case can be covered by a t-test. When there are only two means to compare, the t-test and the ANOVA F-test are equivalent; the relation between ANOVA and t is given by F=t^2.
Factorial ANOVA is used when the experimenter wants to study the interaction effects among the treatments.
Repeated measures ANOVA is used when the same subjects are used for each treatment (e.g., in a longitudinal study).
Multivariate analysis of variance (MANOVA) is used when there is more than one response variable.
ANOVA cautions
Balanced experiments (those with an equal sample size for each treatment) are relatively easy to interpret; unbalanced experiments offer more complexity. For single factor (one way) ANOVA, the adjustment for unbalanced data is easy, but the unbalanced analysis lacks both robustness and power (Montgomery, 2001). For more complex designs the lack of balance leads to further complications. “The orthogonality property of main effects and interactions present in balanced data does not carry over to the unbalanced case. This means that the usual analysis of variance techniques do not apply. Consequently, the analysis of unbalanced factorials is much more difficult than that for balanced designs.” (Montgomery, 2001) In the general case, “The analysis of variance can also be applied to unbalanced data, but then the sums of squares, mean squares, and F-ratios will depend on the order in which the sources of variation are considered.” (Gelman, 2005) The simplest techniques for handling unbalanced data restore balance by either throwing out data or by synthesizing missing data. More complex techniques use regression.
ANOVA is (in part) a significance test. The American Psychological Association holds the view that simply reporting significance is insufficient and that reporting confidence bounds is preferred (Wilkinson, 1999).
While ANOVA is conservative (in maintaining a significance level) against multiple comparisons in one dimension, it is not conservative against comparisons in multiple dimensions (Wilkinson, 1999).
Generalizations
ANOVA is considered to be a special case of linear regression (Gelman, 2005) which in turn is a special case of the general linear model (Howell, 2002). All consider the observations to be the sum of a model (fit) and a residual (error) to be minimized. The Kruskal–Wallis test and the Friedman test are nonparametric tests, which do not rely on an assumption of normality (Montgomery, 2001).
History
While the analysis of variance reached fruition in the 20th century, antecedents extend centuries into the past according to Stigler (Stigler, 1986). These include hypothesis testing, the partitioning of sums of squares, experimental techniques and the additive model. Laplace was performing hypothesis testing in the 1770s. The development of least-squares methods by Laplace and Gauss circa 1800 provided an improved method of combining observations (over the existing practices of astronomy and geodesy). It also initiated much study of the contributions to sums of squares. Laplace soon knew how to estimate a variance from a residual (rather than a total) sum of squares. By 1827 Laplace was using least squares methods to address ANOVA problems regarding measurements of atmospheric tides. Before 1800 astronomers had isolated observational errors resulting from reaction times (the “personal equation”) and had developed methods of reducing the errors. The experimental methods used in the study of the personal equation were later accepted by the emerging field of psychology which developed strong (full factorial) experimental methods to which randomization and blinding were soon added. An eloquent non-mathematical explanation of the additive effects model was available in 1885 (Stigler, 1986).
Sir Ronald Fisher introduced the term “variance“ and proposed a formal analysis of variance in a 1918 article The Correlation Between Relatives on the Supposition of Mendelian Inheritance (Fisher R. A., 1918). His first application of the analysis of variance was published in 1921 (Fisher R. A., Probable Error" of a Coefficient of Correlation Deduced from a Small Sample, 1921). Analysis of variance became widely known after being included in Fisher’s 1925 book, Statistical Methods for Research Workers (Edwards, 1925).
Randomization models were developed by several researchers. The first was published in Polish by Neyman in 1923 (Scheffé, 1959).
One of the attributes of ANOVA which ensured its early popularity was computational elegance. The structure of the additive model allows solution for the additive coefficients by simple algebra rather than by matrix calculations. In the era of mechanical calculators this simplicity was critical. The determination of statistical significance also required access to tables of the F function which were supplied by early statistics texts.
Software
All the primary software packages discussed in Chapter 2 have this functionality.	
Example Using R
This example requires the R stats package. There are three groups with seven observations per group. We denote group i values by y_i:
> y1 = c(18.2, 20.1, 17.6, 16.8, 18.8, 19.7, 19.1)
> y2 = c(17.4, 18.7, 19.1, 16.4, 15.9, 18.4, 17.7)
> y3 = c(15.2, 18.8, 17.7, 16.5, 15.9, 17.1, 16.7)
> local({pkg <- select.list(sort(.packages(all.available = 
+ TRUE)), graphics=TRUE)
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})

Now we combine them into one long vector, with a second vector, group, identifying group membership:
> y = c(y1, y2, y3)
> n = rep(7, 3)
> n
[1] 7 7 7
> group = rep(1:3, n)
> group
 [1] 1 1 1 1 1 1 1 2 2 2 2 2 2 2 3 3 3 3 3 3 3

Here are summaries by group and for the combined data. First we show stem-leaf diagrams.
> tmp = tapply(y, group, stem)

  The decimal point is at the |

  16 | 8
  17 | 6
  18 | 28
  19 | 17
  20 | 1


  The decimal point is at the |

  15 | 9
  16 | 4
  17 | 47
  18 | 47
  19 | 1


  The decimal point is at the |

  15 | 29
  16 | 57
  17 | 17
  18 | 8

> stem(y)

  The decimal point is at the |

  15 | 299
  16 | 4578
  17 | 14677
  18 | 24788
  19 | 117
  20 | 1

Now we show summary statistics by group and overall. We locally define a temporary function, tmpfn, to make this easier.
> tmpfn = function(x) c(sum = sum(x), mean = mean(x), 
+	var = var(x),n = length(x))
> tapply(y, group, tmpfn)
$`1`
       sum       mean        var          n 
130.300000  18.614286   1.358095   7.000000 

$`2`
       sum       mean        var          n 
123.600000  17.657143   1.409524   7.000000 

$`3`
       sum       mean        var          n 
117.900000  16.842857   1.392857   7.000000 

> tmpfn(y)
       sum       mean        var          n 
371.800000  17.704762   1.798476  21.000000 

While we could show you how to use R to mimic the computation of SS by hand, it is more natural to go directly to the ANOVA table.
> data = data.frame(y = y, group = factor(group))
> fit = lm(y ~ group, data)
> anova(fit)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value  Pr(>F)  
group      2 11.007  5.5033  3.9683 0.03735 *
Residuals 18 24.963  1.3868                  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ‘ 1

The anova(fit) object can be used for other computations on the handout and in class. For instance, the tabled F values can be found by the following. First we extract the treatment and error degrees of freedom. Then we use qt to get the tabled F values.
> df = anova(fit)[, “Df”]
> names(df) = c(“trt”, “err”)
> df
trt err 
  2  18 
> alpha = c(0.05, 0.01)
> qf(alpha, df[“trt”], df[“err”], lower.tail = FALSE)
[1] 3.554557 6.012905

A confidence interval on the pooled variance can be computed as well using the anova(fit) object. First we get the residual sum of squares, SSTrt, then we divide by the appropriate chi-square tabled values.
> anova(fit)[“Residuals”, “Sum Sq”]
[1] 24.96286
> anova(fit)[“Residuals”, “Sum Sq”]/qchisq(c(0.025, 0.975), 18)
[1] 3.0328790 0.7918086
anova(fit)[“Residuals”, “Sum Sq”]/qchisq(c(0.025, 0.975), 18,lower.tail = FALSE)
[1] 0.7918086 3.0328790

?
Exercises
	Using the PlantGrowth data in R:
	Label the data columns as Control, Treatment1, and Treatement2.
	Make a boxplot of the data.
	Fit a linear model to the data
	Perform an ANOVA
	Calculate a 95 Confidence Interval for the Parameters
	Plot the Residuals
	A drug company tested three formulations of a pain relief medicine for migraine headache sufferers. For the experiment 27 volunteers were selected and 9 were randomly assigned to one of three drug formulations. The subjects were instructed to take the drug during their next migraine headache episode and to report their pain on a scale of 1 to 10 (10 being most pain).
Drug A 4 5 4 3 2 4 3 4 4
Drug B 6 8 4 5 4 6 5 8 6
Drug C 6 7 6 6 7 5 6 5 5
	Make side-by-side boxplots of the variable pain grouped by the variable drug we must first read in the data into the appropriate format.
	Fit an ANOVA model using the R function aov() and report the summary.
	Discuss the F-Statistic and the decision regarding the null hypothesis.
	If the null hypothesis is rejected, perform a pairwise t-test and discuss your findings. Hint: use pairwise.t.test on the response.
	Use the TukeyHDS() function for another comparison and discuss your findings.
?


 
Appendix A. Notation Used
 
Set Theory
Set: { }
A set: A
Element of a set: A a_i for i=1,…,n
Set union: A?B
Set intersection: A?B
Set complement: A^c, ¬A
Element of: a?A
Such that: |
Natural numbers: N
Integers: Z
Real numbers: R
Indexed set: ?{y_i,x_i}?_(i=1)^n
Probability and statistics
Probability function: P(A), Pr(a), Prob(A)
Conditional probability: P(A?B)
Probability of events intersection: P(A?B)
Probability of events union: P(A?B)
probability density function (pdf): f(x)
Cumulative distribution function (cdf): F(x)
Paramete:r y
Parameter estimate: y ?
population mean: ?
expectation value: E(x)
conditional expectation: E(X?Y)
variance: Var(X)
variance: ?^2
standard deviation: ?_X
covariance: cov(X,Y)
correlation: corr(X,Y)
correlation: ?_XY
lower / first quartile: "Q" _"1" 
median / second quartile: "Q" _"2" 
upper / third quartile: "Q" _"3" 
Sample mean: x ?
Sample variance: s^2
Sample standard deviation: s
Error: ?
Regression coefficient: ?_i
Regression coefficient estimate:   ? ?_i
distribution of X: X~
Normal (Gaussian) distribution: N(?,?^2 )
uniform distribution: U(a,b)
exponential distribution: exp(?) or e^?
gamma distribution: gamma(c,?) or ?(?)
chi-square distribution: ?^2
F distribution: F(k_1,k_2 )
Poisson distribution: Poisson(?)	
Bernoulli distribution: Bern(p)
Linear Algebra
Vector x, x ?
Matrix X
Element of a vector x_i for i=1,…,n
Element of a matrix x_mn for m=number of rows, n=number of columns
Matrix size (dimension): m×n
Vector cross product: ×
Vector dot product: ? 
Norm: ?x?
Rank of a matrix: rank(A)
Matrix transpose: A^T or A'
Matrix inverse: A^(-1)
Hermitian matrix: A^*
Matrix dimension: dim(A)
j-th diagonal of a matrix: a^((j))
Algebra and Calculus 
generic argument: (.)
Very small number: ?
Mapping: f?g
Maps over d: ?(??d )
Derivative: dy/dx
Partial derivative: (?f(x,y))/?x
Integral: ???(.)?
Double integral: ???(.)?
closed interval: [a,b]
open interval: (a,b)
maximize: max(a,b)
maximize subject to: ?(min?x@| condition)
minimize: min(a,b)
Argument of the minimum:	 arg min?(a?A)??(.)? (.)
Laplace transform: L
Weierstrass elliptic function: ?
Natural logarithm: ln(.)
Function: f(x)
Composite function: f(g(x)), f?g
Iteration i of F(x): F_i (x)
Combination: (?(n@k))
Factoiral:  !
Distance between x and y: ||x-y||
If and only if: Iff, ?, ?
approximately equal: ?
Proportional to: ?
Plur or minus: ±
Such that: :
Summation: ??(.) 
Double summation: ?????(.)?
Sum from i=1,…,n:   ?_(i=1)^n??(.)?
Sum of all i?f(i):  ?_(i?f(i))??(.)?
Gradient: ?
Not sign: ¬
 
 
Appendix B. Deploying Hadoop
This appendix is a as a hands-on tutorial to get you started on deploying Hortonworks sandbox. You will ne to ensure you complete the prerequisites before proceeding with the rest of the chapter.
The are several ways to deploy an instance of Hortonworks Sandbox. One way is to download and install a virtual machine, like VirtualBoxTM, and install Sandbox on it with Ambari. This method is not optimal and it is the most difficult of deployments, but it is free.
A second method is using Google Web Services (GWS). Although Goggle loads a default set of resources (for example, Hive, Pig, HDFS, and Azeplin Notebook)., it is difficult to determine file directory paths. I would not use this until Google responds to user feedback. There is a free level, but it is difficult to perform many of the Hortonworks tutorials without paying to upgrade.
A third way is to deploy a Hodoop instance using Amazon Web Services (AWS). While this is more straight-forward than the prior methods, I had limited success in running Pig scripts on tutorial data. Like, GWS, there is a free level, but it is difficult to perform many of the Hortonworks tutorials without paying to upgrade.
Microsoft Azure offers the easiest method—and in my opinion, the best. There is a free trial version of this deployment, but you will may want to pay for appropriate upgrades, up to 25 cents per hour. Azure does all of the installation, configuration, and  updating. In this chapter I demonstrate a deployment using Azure.
Before we begin working with Hadoop, we need to get an instance up an running. With Azure, that means we will ne to establish an account (which is free) at https://portal.azure.com/#resource/subscriptions/.
Once we active our account, we’ll perform a search in Azure for “Hortonworks Sandbox,” as shown in Figure A-1.  This will bring up the screen pictured in Figure A-2.
 
Figure A-1. Azure welcome screen
 
Figure A-2. Hortonworks Sandbox Screen
Choosing Hortonworks Sandbox will bring up the screen depicted in Figure A-3. We will click on Create Virtual Machine. 
If you need assistance beyond what I provide here, Hortonworks has a tutorial for deploying Sandbox in Azure, as shown in Figure A-4.

 
Figure A-3. Create a Virtual Machine Screen
 
Figure A-4. Deploying Hortonworks Sandbox
After you click to create a virtual machine, you should see a screen like the one shown in Figure A-5. In the left part f the screen you will see a list of service types with a plus sign at the top. We will select the plus sign to add a new resource. You may need to establish a subscription, which is not the same as having an account. In that event, you will see a screen like the one shown in Figure A-6. If you need a subscription (I already had one), just choose Free Trial.


 
Figure A-5. Obtaining an Azure Subscription
 
Figure A-6. Free Trial Subscription
After you have a subscription (of if you already have one), you should see a screen like the one depicted in Figure A-7. We will click on the Create button. This will result in a setup screen as shown in Figure A-8. The setup includes three steps. The first step is for configuring basic setting. It the settings, under Authentication type, choose Password, unless you are familiar with ssh.
 
Figure A-7. Create a Virtual Machine
 
Figure A-8. Complete Account Information
Once we complete the basic setting, we will need to choose a machine size as shown in Figure A-9. You can choose an A1 or A2 configuration. I chose  A4. Each selection shows a cost per minute and month. Remember that you have a on-month free trial so these costs do not apply.
 
Figure A-9. Choose a Configuration (A1 or A2 recommended)
Figure A-10 depicts the third step. You can accept all the defaults here.
 
Figure A-10. Configure the Settings
After completing step 3, you will see a summary of your selections like the one shown in Figure A-11. Clicking OK will bring up a purchase screen like the one shown in Figure A-12. Making a purchase selection will not result in a charge for the one-month trial period.
 
Figure A-11. Review a Summary of the Machine
 
Figure A-12. Purchase Additional Features (Optional)
After we create a virtual machine with the selections we have made, we can look at the progress of our deployment by slicking on the bell at the top of the screen, as shown in Figure A-13.
 
Figure A-13. Deployment Status Notification
Once the status changes to “Successful”, we can go to the dashboard (left pane) and select Virtual Machines. This will result in a screen like the one shown in Figure A-14, and will indicate that you have successfully deployed and instance of Hadoop.
 
Figure A-14. Azure Portal Personal Dashboard
When we start running the Sandbox instance, we will ne to supply some admin information as shown in Figure A-15.
 
Figure A-15. Sandbox Welcome Screen
When we finish with the admin information, we should see a screen like the one in Figure A-16. This screen displays several features we will use. Fist it links us to a tutorial called Hello HDP. We will cover the training provided by the tutorial but in a different manner. The tutorial is written so that we would use the Ambari views for all requirements. However, if you ever desire to be Hadoop Certified, you will need to enter all commands through the terminal. We will balance Hello World with a little of both.
The screen also provides a link to the Ambari View and a button to look at advanced options. Click on View Advanced Options and a screen like that in Figure A-17 will appear. This screen provides additional links. One in particular is the to the web-based terminal. We will use this to enter terminal-like commands (see Figure A-18) One of the first commands will change the Ambari admin password so that we can access it in the admin mode (see Figure A-19).
 
Figure A-16. Sandbox Navigation Screen 
 
Figure A-17. Sandbox Navigation Screen – View Advanced Options
 
Figure A-18. Web-based Secure Shell Client
As mentioned above, we want to change the Ambari admin password. The command for this is sudo ambari-admin-password-reset (see Figure A-19). The sudo command is used to get root-level access. Without using it, we would get a permission denied message.
 
Figure A-19. Connecting and Changing Admin Password to Ambari Server




?
?
Glossary
A Priori Probability: A priori probability is the probability estimate prior to receiving new information. See also Bayes Theorem and posterior probability.
Attribute: In data analysis or data mining, an attribute is a characteristic or feature that is measured for each observation (record) and can vary from one observation to another. It might be measured in continuous values (e.g. time spent on a web site), or in categorical values (e.g. red, yellow, green). The terms “attribute” and “feature” are used in the machine learning community, “variable” is used in the statistics community. They are synonyms.
Autoregression:  Autoregression refers to a special branch of regression analysis aimed at analysis of time series. It rests on autoregressive models - that is, models where the dependent variable is the current value and the independent variables are N previous values of the time series. The N is called “the order of the autoregression”.
Autoregression and Moving Average (ARMA) Models: The autoregression and moving average (ARMA) models are used in time series analysis to describe stationary time series. These models represent time series that are generated by passing white noise through a recursive and through a nonrecursive linear filter, consecutively. In other words, the ARMA model is a combination of an autoregressive (AR) model and a moving average (MA) model.
Autoregressive (AR) Models: The autoregressive (AR) models are used in time series analysis to describe stationary time series. These models represent time series that are generated by passing the white noise through a recursive linear filter. The output of such a filter at the moment t image is a weighted sum of m image previous values of the filter output. The integer parameter m image is called the order of the AR-model.
Average Deviation: The average deviation or the average absolute deviation is a measure of dispersion. It is the average of absolute deviations of the individual values from the median or from the mean.
Bagging: In predictive modeling, bagging is an ensemble method that uses bootstrap replicates of the original training data to fit predictive models. For each record, the predictions from all available models are then averaged for the final prediction. For a classification problem, a majority vote of the models is used. Bagging is short for “bootstrap aggregating.”
Bayes´ Theorem: Bayes theorem is a formula for revising a priori probabilities after receiving new information. The revised probabilities are called posterior probabilities. For example, consider the probability that you will develop a specific cancer in the next year. An estimate of this probability based on general population data would be a prior estimate; a revised (posterior) estimate would be based on both on the population data and the results of a specific test for cancer.
Boosting: In predictive modeling, boosting is an iterative ensemble method that starts out by applying a classification algorithm and generating classifications. The classifications are then assessed, and a second round of model-fitting occurs in which the records classified incorrectly in the first round are given a higher weight in the second round. This procedure is repeated a number of times, and the final classifier results from a merger of the various iterations, with lesser weights typically accorded to the very last rounds. The idea is to concentrate the iterative learning process on the hard-to-classify cases.
Chi-Square Test: Chi-square test (or ?^2-test) is a statistical test for testing the null hypothesis that the distribution of a discrete random variable coincides with a given distribution. It is one of the most popular goodness-of-fit tests.
Classification and Regression Trees (CART): Classification and regression trees (CART) are a set of techniques for classification and prediction. The technique is aimed at producing rules that predict the value of an outcome (target) variable from known values of predictor (explanatory) variables. The predictor variables may be a mixture of categorical and continuous variables.
Classification Trees: Classification trees are one of the CART techniques. The main distinction from regression trees (another CART technique) is that the dependent variable Y image is categorical.
Coefficient of Determination: In regression analysis, the coefficient of determination is a measure of goodness-of-fit (i.e. how well or tightly the data fit the estimated model). The coefficient is defined as the ratio of two sums of squares: R^2=SSR /SSE , where SSR is the sum of squares due to regression, SST is the total sum of squares. By “sum of squares” we mean the sum of squared deviations between actual values and the mean (SST), or between predicted values and the mean (SSR). The coefficient of determination takes on values between 0 and 1, with values closer to 1 implying a better fit.
Collinearity: In regression analysis, collinearity of two variables means that strong correlation exists between them, making it difficult or impossible to estimate their individual regression coefficients reliably.
Conditional Probability: When probabilities are quoted without specification of the sample space, it could result in ambiguity when the sample space is not self-evident. To avoid this, the sample space can be explicitly made known. The probability of an event A given sample space S, denoted by P(A|S), is nothing but the conditional probability of A given S.
Covariance: covariance is a measure of how much two random variables change together. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the smaller values, i.e., the variables tend to show similar behavior, the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the smaller values of the other, i.e., the variables tend to show opposite behavior, the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables.
Data Mining: Data mining is concerned with finding latent patterns in large data bases. The goal is to discover unsuspected relationships that are of practical importance, e.g., in business.
Degrees of Freedom: For a set of data points in a given situation (e.g. with mean or other parameter specified, or not), degrees of freedom is the minimal number of values which should be specified to determine all the data points.
Dependent variable: Dependent variables are also called response variables, outcome variables, target variables or output variables. The terms “dependent” and “independent” here have no direct relation to the concept of statistical dependence or independence of events.
Deviance: Deviance is a quality of fit statistic for a model that is often used for statistical hypothesis testing. It is a generalization of the idea of using the sum of squares of residuals in ordinary least squares to cases where model-fitting is achieved by maximum likelihood.
Effect: In design of experiments, the effect of a factor is an additive term of the model, reflecting the contribution of the factor to the response.
Estimator: A statistic, measure, or model, applied to a sample, intended to estimate some parameter of the population that the sample came from.
Explanatory Variable: Explanatory variable is a synonym for independent variable.
Factor: In design of experiments, factor is an independent variable manipulated by the experimenter.
Failure rate: The failure rate is defined for non-repairable populations as the (instantaneous) rate of failure for the survivors to time t during the next instant of time. The failure rate (or hazard rate) h(t) and is:
h(t)=f(t)/(1-F(t))=f(t)/R(t)= instantaneous failure rate.
Filter: A filter is an algorithm for processing a time series or random process.
General Linear Model:  General (or generalized) linear models (GLM), in contrast to linear models, allow you to describe both additive and non-additive relationship between a dependent variable and N independent variables. The independent variables in GLM may be continuous as well as discrete.
Hazard function: The hazard function, conventionally denoted ?, is defined as the event rate at time t conditional on survival until time t or later (that is, T?t).
Hazard rate: see failure rate.
Heteroscedasticity: Heteroscedasticity generally means unequal variation of data, e.g. unequal variance.
Heteroscedasticity in regression: In regression analysis, heteroscedasticity means a situation in which the variance of the dependent variable varies across the data. Heteroscedasticity complicates analysis because many methods in regression analysis are based on an assumption of equal variance.
Independent Events: Two events A and B are said to be independent if P(AB)=P(A)P(B). To put it differently, events A and B are independent if the occurrence or non-occurrence of A does not influence the occurrence of non-occurrence of B and vice-versa.
Independent Variables: Statistical models normally specify how one set of variables, called dependent variables, functionally depend on another set of variables, called independent variables. The term “independent” reflects only the functional relationship between variables within a model. Several models based on the same set of variables may differ by how the variables are subdivided into dependent and independent variables. Alternative names for independent variables (especially in data mining and predictive modeling) are input variables, predictors or features.
Inferential Statistics: Inferential statistics is the body of statistical techniques that deal with the question “How reliable is the conclusion or estimate that we derive from a set of data?” The two main techniques are confidence intervals and hypothesis tests.
k-Nearest Neighbors Prediction: The k-nearest neighbors (k-NN) prediction is a method to predict a value of a target variable in a given record, using as a reference point a  training set  of similar objects. The basic idea is to choose k objects from the training set that are closest to the given object in terms of the predictor variables, then to form the weighted average of target variable for those k objects. The weights are usually chosen inversely proportionally to the distances from the target object.
Kurtosis: Kurtosis measures the “heaviness of the tails” of a distribution (in compared to a normal distribution). Kurtosis is positive if the tails are “heavier” then for a normal distribution, and negative if the tails are “lighter” than for a normal distribution. The normal distribution has kurtosis of zero.
Latent Variable: A latent variable describes an unobservable construct and cannot be observed or measured directly.
Least Squares Method: In a narrow sense, the Least Squares Method is a technique for fitting a straight line through a set of points in such a way that the sum of the squared vertical distances from the observed points to the fitted line is minimized. In a wider sense, the Least Squares Method is a general approach to fitting a model of the data-generating mechanism to the observed data.
Likelihood Function:  Likelihood function is a fundamental concept in statistical inference. It indicates how likely a particular population is to produce an observed sample.
Likelihood ratio test: a likelihood ratio test is a statistical test used to compare the fit of two models, one of which (the null model) is a special case of the other (the alternative model). The test is based on the likelihood ratio, which expresses how many times more likely the data are under one model than the other.
Linear regression: In statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables denoted X. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.
Linkage Function: A linkage function is an essential prerequisite for hierarchical cluster analysis. Its value is a measure of the “distance” between two groups of objects (i.e. between two clusters).
Logit: Logit is a nonlinear function of probability. If p is the probability of an event, then the corresponding logit is given by the formula: logit(p)log(p/(1-p)).
Logistic regression: In statistics, logistic regression, or logit regression, is a type of probabilistic statistical classification model. It is also used to predict a binary response from a binary predictor, used for predicting the outcome of a categorical dependent variable (i.e., a class label) based on one or more predictor variables (features). That is, it is used in estimating the parameters of a qualitative response model.
Machine Learning: Analytics in which computers “learn” from data to produce models or rules that apply to those data and to other similar data. Predictive modeling techniques such as neural nets, classification and regression trees (decision trees), naive Bayes, k-nearest neighbor, and support vector machines are generally included. One characteristic of these techniques is that the form of the resulting model is flexible, and adapts to the data.
Markov chain: A Markov chain, named after Andrey Markov, is a mathematical system that undergoes transitions from one state to another on a state space. It is a random process usually characterized as memoryless: the next state depends only on the current state and not on the sequence of events that preceded it. This specific kind of “memorylessness” is called the Markov property.
Mean Squared Error (MLE): The mean squared error is a measure of performance of a point estimator. It measures the average squared difference between the estimator and the parameter. For an unbiased estimator, the mean squared error is equal to the variance of the estimator.
Multicollinearity:  In regression analysis, multicollinearity refers to a situation of collinearity of independent variables, often involving more than two independent variables, or more than one pair of collinear variables. Multicollinearity means redundancy in the set of variables.
Naive Bayes Classification: The Naive Bayes method is a method of classification applicable to categorical data, based on Bayes theorem. For a record to be classified, the categories of the predictor variables are noted and the record is classified according to the most frequent class among the same values of those predictor variables in the training set. A rigorous application of the Bayes theorem would require availability of all possible combinations of the values of the predictor variables. When the number of variables is large enough, this requires a training set of unrealistically large size (and, indeed, even a huge training set is unlikely to cover all possible combinations). The naive Bayes method overcomes this practical limitation of the rigorous Bayes approach to classification.
Neural Network: A neural network (NN) is a network of many simple processors (“units”), each possibly having a small amount of local memory. The units are connected by communication channels (“connections”) which usually carry numeric (as opposed to symbolic) data, encoded by any of various means. The units operate only on their local data and on the inputs they receive via the connections. The restriction to local operations is often relaxed during training.
Null Hypothesis: In hypothesis testing, the null hypothesis is the one you are hoping can be disproven by the observed data. Typically, it asserts that chance variation is responsible for an effect seen in observed data (for example, a difference between treatment and placebo, an apparent correlation between one variable and another, a divergence between a sample measure and some benchmark, etc.)
Odds ratio: In statistics, the odds ratio (usually abbreviated “OR”) is one of three main ways to quantify how strongly the presence or absence of property A is associated with the presence or absence of property B in a given population.
p-value: The p-value is the probability that the null model could, by random chance variation, produce a sample as extreme as the observed sample (as measured by some sample statistic of interest.)
Parameter: A Parameter is a numerical value that describes one of the characteristics of a probability distribution or population. For example, a binomial distribution is completely specified if the number of trials and probability of success are known. Here, the number of trials and the probability of success are two parameters. A normal distribution is completely specified if its mean and standard deviation are known.
Posterior Probability: Posterior probability is a revised probability that takes into account new available information. For example, let there be two urns, urn A having 5 black balls and 10 red balls and urn B having 10 black balls and 5 red balls. Now if an urn is selected at random, the probability that urn A is chosen is 0.5. This is the a priori probability. If we are given an additional piece of information that a ball was drawn at random from the selected urn, and that ball was black, what is the probability that the chosen urn is urn A? Posterior probability takes into account this additional information and revises the probability downward from 0.5 to 0.333 according to Bayes´ theorem, because a black ball is more probable from urn B than urn A.
Probit: Probit is a nonlinear function of probability p: probit(p) = ?^(-1) (p), where ?^(-1) (.)  is the function inverse to the cumulative distribution function ?(.) of the  standard normal distribution. In contrast to the probability p itsef (which takes on values from 0 to 1), the values of the probit of p are from minus- to plus-infinity.
R-squared: See Coefficient of determination.
Random Error: The random error is the fluctuating part of the overall error that varies from measurement to measurement. Normally, the random error is defined as the deviation of the total error from its mean value.
Random variable: a random variable is a variable whose value is subject to variations due to chance (i.e. randomness, in a mathematical sense). A random variable can take on a set of possible different values, each with an associated probability (if discrete) or a probability density function (if continuous), in contrast to other mathematical variables.
Regression Analysis: Regression analysis provides a “best-fit” mathematical equation for the relationship between the dependent variable (response) and independent variable(s) (covariates).
Regression Trees: Regression trees is one of the CART techniques. The main distinction from classification trees (another CART technique) is that the dependent variable Y image is continuous.
Regularization: Regularization refers to a wide variety of techniques used to bring structure to statistical models in the face of data size, complexity and sparseness. Advances in digital processing, storage and retrieval have led to huge and growing data sets (“Big Data”). Regularization is used to allow models to usefully model such data without overfitting. A very simple example is linear regression; other examples are smoothing techniques.
Residuals: Residuals are differences between the observed values and the values predicted by some model. Analysis of residuals allows you to estimate the adequacy of a model for particular data; it is widely used in regression analysis.
Sample: A sample is a portion of the elements of a population. A sample is chosen to make inferences about the population by examining or measuring the elements in the sample.
Sample Space: The set of all possible outcomes of a particular experiment is called the sample space for the experiment. If a coin is tossed twice, the sample space is {HH, HT, TH, TT}, where TH, for example, means getting tails on the first toss and heads on the second toss.
Seasonal Decomposition: The seasonal decomposition is a method used in time series analysis to represent a time series as a sum (or, sometimes, a product) of three components—the linear trend, the periodic (seasonal) component, and random residuals.
Serial Correlation: In analysis of time series, the Nth order serial correlation is the correlation between the current value and the Nth previous value of the same time series. For this reason serial correlation is often called “autocorrelation”.
Simple Linear Regression: The simple linear regression is aimed at finding the “best-fit” values of two parameters - A and B in the following regression equation: Y_i= A X_i+ B + E_i,i=1,p‚R,N, where Y_i, X_i, and E_i are the values of the dependent variable, of the independent variable, and of the random error, respectively. Parameter A is called “the slope of the regression line”, B - “the y-intercept of the regression line”.
Singularity: In regression analysis, singularity is the extreme form of multicollinearity - when a perfect linear relationship exists between variables or, in other terms, when the correlation coefficient is equal to 1.0 or -1.0.
Standard error: The standard error measures the variability of an estimator (or sample statistic) from sample to sample. By formula, STDEER=STDEV/?N.
Stationary process: a stationary process is a stochastic process whose joint probability distribution does not change when shifted in time. Consequently, parameters such as the mean and variance, if they are present, also do not change over time and do not follow any trends.
Statistical inference: statistical inference is the process of drawing conclusions from data that are subject to random variation, for example, observational errors or sampling variation.
Statistical Significance: Outcomes to an experiment or repeated events are statistically significant if they differ from what chance variation might produce.
Stochastic process: a stochastic process, or sometimes random process is a collection of random variables, representing the evolution of some system of random values over time.
t-test: A t-test is a statistical hypothesis test based on a test statistic whose sampling distribution is a t-distribution. Various t-tests, strictly speaking, are aimed at testing hypotheses about populations with normal probability distribution. However, statistical research has shown that t-tests often provide quite adequate results for non-normally distributed populations too.
Time Series: A time series is a sequence of data points, measured typically at successive points in time spaced at uniform time intervals. Examples of time series are the daily closing value of the Dow Jones Industrial Average and the annual flow volume of the Nile River at Aswan.
Training Set: A training set is a portion of a data set used to fit (train) a model for prediction or classification of values that are known in the training set, but unknown in other (future) data. The training set is used in conjunction with validation and/or test sets that are used to evaluate different models.
Type I Error: In a test of significance, Type I error is the error of rejecting the null hypothesis when it is true -- of saying an effect or event is statistically significant when it is not. The projected probability of committing type I error is called the level of significance.
Type II Error: In a test of significance, Type II error is the error of accepting the null hypothesis when it is false -- of failing to declare a real difference as statistically significant. Obviously, the bigger your samples, the more likely your test is to detect any difference that exists. The probability of detecting a real difference of specified size (i.e. of not committing a Type II error) is called the power of the test.
Validation Set: A validation set is a portion of a data set used in data mining to assess the performance of prediction or classification models that have been fit on a separate portion of the same data set (the training set). Typically both the training and validation set are randomly selected, and the validation set is used as a more objective measure of the performance of various models that have been fit to the training data (and whose performance with the training set is therefore not likely to be a good guide to their performance with data that they were not fit to).
Variance: Variance is a measure of dispersion. It is the average squared distance between the mean and each item in the population or in the sample.
